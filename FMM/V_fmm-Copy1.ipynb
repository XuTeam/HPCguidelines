{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ec2b0f-da5a-46da-ae3f-2c979b072875",
   "metadata": {},
   "source": [
    "# Resolution of 144*144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fb23319-6da7-4872-9031-6ca5b6f7c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb85a22f-90fc-4a31-a61a-280d7c93a052",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-33a230a62eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "#     print(x.shape)\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "#         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "        \n",
    "        x = x.repeat(1,1,3).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = x[0], x[1], x[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "    \n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.reconstruction = nn.Linear(2 * dim, 4 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, direction='down', uplevel_result1=None, uplevel_result2=None):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        if direction=='down':\n",
    "            \n",
    "            H, W = self.input_resolution\n",
    "            B, L, C = x.shape\n",
    "#             print(x.shape)\n",
    "#             print(H, W)\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "            assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "            x = x.view(B, H, W, C)\n",
    "\n",
    "            x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "            x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "            x = self.norm(x)\n",
    "            x = self.reduction(x)\n",
    "        else:\n",
    "            H, W = self.input_resolution\n",
    "#             print(x.shape)\n",
    "            B = x.shape[0]\n",
    "            C = x.shape[-1]\n",
    "#             print(x.shape)\n",
    "#             print(H, W)\n",
    "#             assert H_0 == H // 2, \"input feature has wrong size\"\n",
    "            x = self.reconstruction(x)\n",
    "            x = x.view(B, H//2, W//2, 2*C)\n",
    "            New_C = C//2\n",
    "            x_target = torch.zeros(B, H, W, self.dim, device=x.device)\n",
    "\n",
    "            x_target[:, 0::2, 0::2, :] = x[...,:New_C] # B H/2 W/2 C\n",
    "            x_target[:, 1::2, 0::2, :] = x[...,New_C:2*New_C]  # B H/2 W/2 C\n",
    "            x_target[:, 0::2, 1::2, :] = x[...,2*New_C:3*New_C]  # B H/2 W/2 C\n",
    "            x_target[:, 1::2, 1::2, :] = x[...,3*New_C:] # B H/2 W/2 C\n",
    "            x = x_target + uplevel_result1.view(B, H, W, self.dim) \n",
    "           \n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "    \n",
    "class PatchUnMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "#         self.reduction = nn.Linear(dim//4, 1, bias=False)\n",
    "#         self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        New_C = C//4\n",
    "        x_target = torch.zeros(B, H*2, W*2, New_C, device=x.device)\n",
    "\n",
    "        x_target[:, 0::2, 0::2, :] = x[..., :New_C] # B H/2 W/2 C\n",
    "        x_target[:, 1::2, 0::2, :] = x[..., New_C:2*New_C]  # B H/2 W/2 C\n",
    "        x_target[:, 0::2, 1::2, :] = x[..., 2*New_C:3*New_C]  # B H/2 W/2 C\n",
    "        x_target[:, 1::2, 1::2, :] = x[..., 3*New_C:] # B H/2 W/2 C\n",
    "#         x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "#         x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "#         x = self.norm(x)\n",
    "#         x = self.reduction(x)\n",
    "\n",
    "        return x_target\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, stride_size=3):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [(img_size[0]-patch_size[0]+2*6) // stride_size+1, \n",
    "                              (img_size[0]-patch_size[0]+2*6) // stride_size+1]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride_size, padding=6)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "    \n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        B, HW, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=[7, 7, 7, 7], mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size[i_layer],\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "        self.patchUnmerge = PatchUnMerging(dim=int(embed_dim * 2 ** i_layer), input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)))\n",
    "        self.norm = norm_layer(self.num_features//4)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "#         x = self.norm(x)  # B L C\n",
    "#         x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "#         x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "#         x = self.head(x)\n",
    "        x = self.patchUnmerge(x)\n",
    "        x = self.norm(x)  # B L C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n",
    "    \n",
    "class FMMTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=[7, 7, 7, 7], mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.downsamplers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size[i_layer],\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        for i_layer in range(self.num_layers-1):\n",
    "            layer = PatchMerging(dim=int(embed_dim * 2 ** i_layer),\n",
    "                                 input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)))\n",
    "            self.downsamplers.append(layer)\n",
    "            \n",
    "        self.patchUnmerge = PatchUnMerging(dim=int(embed_dim * 2 ** i_layer), input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)))\n",
    "        self.norm = norm_layer(self.num_features//4)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "#         x = self.norm(x)  # B L C\n",
    "#         x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "#         x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        y0 = self.layers[0](x)\n",
    "        x1 = self.downsamplers[0](x)\n",
    "        y1 = self.layers[1](x1)\n",
    "        x2 = self.downsamplers[1](x1)\n",
    "        y2 = self.layers[2](x2)\n",
    "        \n",
    "        y1 = self.downsamplers[1](y2, direction='up', uplevel_result1=y1)\n",
    "        y0 = self.downsamplers[0](y1, direction='up', uplevel_result1=y0)\n",
    "\n",
    "        x = self.norm(y0)  # B L C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2df9ea0-ebd4-4c96-bfa2-6c08c7ea96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Zongyi Li\n",
    "This file is the Fourier Neural Operator for 2D problem such as the Darcy Flow discussed in Section 5.2 in the [paper](https://arxiv.org/pdf/2010.08895.pdf).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "from utilities3 import *\n",
    "# normalization, pointwise gaussian\n",
    "class UnitGaussianNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(UnitGaussianNormalizer, self).__init__()\n",
    "\n",
    "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
    "        self.mean = torch.mean(x, 0)\n",
    "        self.std = torch.std(x, 0)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        if sample_idx is None:\n",
    "            std = self.std + self.eps # n\n",
    "            mean = self.mean\n",
    "        else:\n",
    "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
    "                std = self.std[sample_idx] + self.eps  # batch*n\n",
    "                mean = self.mean[sample_idx]\n",
    "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
    "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
    "                mean = self.mean[:,sample_idx]\n",
    "\n",
    "        # x is in shape of batch*n or T*batch*n\n",
    "        x = (x * std) + mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self, device):\n",
    "        self.mean = self.mean.to(device)\n",
    "        self.std = self.std.to(device)\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31198def-4506-4d73-9679-bbe54284cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "#         self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "#         self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "#         self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(self.width, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         grid = self.get_grid(x.shape, x.device)\n",
    "#         x = torch.cat((x, grid), dim=-1)\n",
    "#         x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "#         x = F.gelu(x)\n",
    "\n",
    "#         x1 = self.conv3(x)\n",
    "#         x2 = self.w3(x)\n",
    "#         x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "#         x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c258c61-d5cf-4e8a-b283-4a636ae5bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# configs\n",
    "################################################################\n",
    "device = torch.device('cuda:1')\n",
    "TRAIN_PATH = 'data/piececonst_r421_N1024_smooth1.mat'\n",
    "TEST_PATH = 'data/piececonst_r421_N1024_smooth2.mat'\n",
    "\n",
    "ntrain = 1000\n",
    "ntest = 100\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 500\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 100\n",
    "\n",
    "r = 3\n",
    "h = int(((421 - 1)/r) + 1)\n",
    "s = h\n",
    "\n",
    "################################################################\n",
    "# load data and data normalization\n",
    "################################################################\n",
    "reader = MatReader(TRAIN_PATH)\n",
    "# x_train[:, np.newaxis, :, :]\n",
    "# x_train = reader.read_field('coeff')[:, np.newaxis, :, :]\n",
    "x_train = reader.read_field('coeff')[:ntrain,...]\n",
    "# x_train = reader.read_field('coeff')[:ntrain,::r,::r][:,:s,:s]\n",
    "y_train = reader.read_field('sol')[:ntrain,::r,::r][:,:s,:s]\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "x_test = reader.read_field('coeff')[:ntest,...]\n",
    "y_test = reader.read_field('sol')[:ntest,::r,::r][:,:s,:s]\n",
    "\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train[:, np.newaxis, ...]\n",
    "x_test = x_test[:, np.newaxis, ...]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train.to(device), y_train.to(device)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test.to(device), y_test.to(device)), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8333e8f9-edbe-45a1-bdb6-9dbfa74722eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2089813\n",
      "0 7.317775344476104 0.12814762115478515 0.11516076326370239\n",
      "1 7.312827855348587 0.10401642668247223 0.10501975178718567\n",
      "2 7.327712694182992 0.09304938268661499 0.08847338676452637\n",
      "3 7.372341316193342 0.07126545286178589 0.06533522963523865\n",
      "4 7.371336420997977 0.055701515555381775 0.0523560094833374\n",
      "5 7.387003723531961 0.0461014114022255 0.04962015748023987\n",
      "6 7.404483247548342 0.040307494580745694 0.04421263933181763\n",
      "7 7.407505387440324 0.036614154994487766 0.04324667811393738\n",
      "8 7.428168129175901 0.03183501636981964 0.037708335518836976\n",
      "9 7.4189173094928265 0.0293955157995224 0.03533747911453247\n",
      "10 7.430736353620887 0.02736859717965126 0.03433479428291321\n",
      "11 7.447097277268767 0.02459780251979828 0.031066465973854065\n",
      "12 7.445274963974953 0.02286002105474472 0.029341243505477906\n",
      "13 7.447421882301569 0.02122162878513336 0.027699029445648192\n",
      "14 7.447461202740669 0.02015801641345024 0.027454513311386108\n",
      "15 7.4498903919011354 0.019192454159259797 0.026447976529598235\n",
      "16 7.4582915641367435 0.018473200410604476 0.025777698159217835\n",
      "17 7.454916786402464 0.018040585905313492 0.024962407350540162\n",
      "18 7.456021731719375 0.01692733547091484 0.025445429384708406\n",
      "19 7.471498101949692 0.01679636150598526 0.023197919130325317\n",
      "20 7.457185842096806 0.016199439197778703 0.02235629588365555\n",
      "21 7.459236476570368 0.0162173992395401 0.022882414758205415\n",
      "22 7.456672890111804 0.015273325502872467 0.023415777385234832\n",
      "23 7.467804692685604 0.01639151152968407 0.02248107224702835\n",
      "24 7.469179350882769 0.015580930083990098 0.022005763351917267\n",
      "25 7.466775128617883 0.015438903629779815 0.021523215174674988\n",
      "26 7.458966974169016 0.014743050307035446 0.02056886076927185\n",
      "27 7.460368312895298 0.013547871068120002 0.02017656296491623\n",
      "28 7.459492960944772 0.013876010805368424 0.0208780562877655\n",
      "29 7.4631133284419775 0.013534512013196946 0.019382879436016083\n",
      "30 7.464437099173665 0.013061607480049134 0.01913849115371704\n",
      "31 7.472173010930419 0.012481547951698303 0.019175628125667574\n",
      "32 7.460275026038289 0.01303704436123371 0.019032607972621917\n",
      "33 7.458817832171917 0.012656139329075814 0.018673723340034486\n",
      "34 7.465890036895871 0.012592626884579659 0.01846741795539856\n",
      "35 7.458986336365342 0.01180905243754387 0.01832735687494278\n",
      "36 7.461402943357825 0.011917049512267113 0.018760674595832826\n",
      "37 7.46301962248981 0.011956992700695992 0.01762012928724289\n",
      "38 7.460323264822364 0.012326801761984824 0.019913673996925355\n",
      "39 7.474984111264348 0.012453873962163926 0.019891911745071413\n",
      "40 7.4637922160327435 0.012587464109063149 0.017926346361637115\n",
      "41 7.4637838043272495 0.011808818310499191 0.017919374108314515\n",
      "42 7.458849851042032 0.012283184573054313 0.018059791624546052\n",
      "43 7.480859361588955 0.01172615846991539 0.0177106648683548\n",
      "44 7.462804526090622 0.011329158917069435 0.017601411044597625\n",
      "45 7.463999275118113 0.01186390233039856 0.0171866700053215\n",
      "46 7.464286498725414 0.01138019920885563 0.01668818384408951\n",
      "47 7.469565261155367 0.011230384856462479 0.017425221502780915\n",
      "48 7.463464714586735 0.011714035496115684 0.01690868943929672\n",
      "49 7.458274206146598 0.011765551030635833 0.016977498829364775\n",
      "50 7.460943361744285 0.011176344975829124 0.01702958792448044\n",
      "51 7.464471850544214 0.010851015836000443 0.01663254588842392\n",
      "52 7.4582222905009985 0.010849270805716514 0.01689262092113495\n",
      "53 7.4577304888516665 0.010566454634070396 0.015904891192913054\n",
      "54 7.472686221823096 0.010505444318056107 0.016299817264080047\n",
      "55 7.458087045699358 0.010542818948626518 0.01603924363851547\n",
      "56 7.46255024895072 0.010323027774691582 0.016253090500831603\n",
      "57 7.4608510080724955 0.01027440993487835 0.01640717387199402\n",
      "58 7.45725679025054 0.010352066084742546 0.016013844162225722\n",
      "59 7.46206315420568 0.01067385409772396 0.016170387864112856\n",
      "60 7.465144036337733 0.01081689317524433 0.01588071882724762\n",
      "61 7.458977239206433 0.010672586217522621 0.016036811769008636\n",
      "62 7.461975179612637 0.010653524935245514 0.01554408371448517\n",
      "63 7.461733989417553 0.010672769919037818 0.015492987036705017\n",
      "64 7.462818393483758 0.010028789713978768 0.015028254240751266\n",
      "65 7.459442337974906 0.010118882149457931 0.015841891169548036\n",
      "66 7.480160864070058 0.010454931050539016 0.01787958264350891\n",
      "67 7.470293611288071 0.01105294331908226 0.016586043536663056\n",
      "68 7.46128430403769 0.010889538660645485 0.015888835489749908\n",
      "69 7.460166642442346 0.010708033561706543 0.015851294994354247\n",
      "70 7.466223135590553 0.010095058500766755 0.015550412237644196\n",
      "71 7.46329771541059 0.010524437859654427 0.01519136741757393\n",
      "72 7.462363099679351 0.00969331680238247 0.014875496029853821\n",
      "73 7.462419388815761 0.010014549925923348 0.015776712894439698\n",
      "74 7.4654337503015995 0.00975504620373249 0.015013922452926636\n",
      "75 7.4712909404188395 0.010121858060359954 0.015089889168739318\n",
      "76 7.461944198235869 0.010034147813916207 0.01470998913049698\n",
      "77 7.4600360710173845 0.009673833459615708 0.014848889410495758\n",
      "78 7.465667931362987 0.00960060253739357 0.014697854220867158\n",
      "79 7.463262943550944 0.010341318577528 0.015241738557815552\n",
      "80 7.46288170479238 0.01039937199652195 0.01495572656393051\n",
      "81 7.463808633387089 0.00993301360309124 0.014805583953857422\n",
      "82 7.469427166506648 0.00957624626159668 0.014428441822528839\n",
      "83 7.46051037684083 0.009624194011092185 0.014871498346328735\n",
      "84 7.465170109644532 0.010027526766061784 0.014311168491840363\n",
      "85 7.458935795351863 0.010079773530364036 0.014876206815242767\n",
      "86 7.475230062380433 0.009852567717432975 0.015496470332145691\n",
      "87 7.459539206698537 0.010319188222289085 0.015360148549079895\n",
      "88 7.463839687407017 0.009882131218910217 0.014341341555118561\n",
      "89 7.459236888214946 0.009601567715406418 0.014628267884254455\n",
      "90 7.468984249979258 0.010215582147240639 0.014072260409593583\n",
      "91 7.4680544026196 0.009894925206899642 0.014271522760391236\n",
      "92 7.462386857718229 0.009327279478311539 0.01447499617934227\n",
      "93 7.4640590492635965 0.0092846157848835 0.014451406002044677\n",
      "94 7.465780338272452 0.009482558891177177 0.014404774010181427\n",
      "95 7.459751268848777 0.009726480230689049 0.014464695304632187\n",
      "96 7.465967521071434 0.010131494745612145 0.0156780144572258\n",
      "97 7.461827198043466 0.010914454489946365 0.014456720650196075\n",
      "98 7.4642922934144735 0.009061909481883049 0.014144462645053863\n",
      "99 7.462135383859277 0.009113279908895492 0.014032677859067916\n",
      "100 7.467056570574641 0.007820206686854363 0.012844530642032623\n",
      "101 7.481901291757822 0.007102768257260322 0.012630464434623718\n",
      "102 7.47737518325448 0.0068833247274160386 0.012589463889598846\n",
      "103 7.470489006489515 0.0068363074660301205 0.012498853504657745\n",
      "104 7.474614975973964 0.006680904880166053 0.01245713084936142\n",
      "105 7.474844520911574 0.006679036669433117 0.01256582409143448\n",
      "106 7.467731714248657 0.006665745958685875 0.012433073818683624\n",
      "107 7.468284714967012 0.006858631558716297 0.012468570470809936\n",
      "108 7.487167906016111 0.007010486885905266 0.012301065772771836\n",
      "109 7.467307703569531 0.006762857802212238 0.012390539646148682\n",
      "110 7.46593002229929 0.006808603622019291 0.012152652591466903\n",
      "111 7.472726486623287 0.006679345451295376 0.01222856193780899\n",
      "112 7.479284631088376 0.0066672156229615216 0.012365273237228393\n",
      "113 7.463222390040755 0.00694505912065506 0.012455923557281494\n",
      "114 7.46020670235157 0.007091605558991432 0.012741095423698424\n",
      "115 7.458811700344086 0.007059783414006233 0.012466631084680557\n",
      "116 7.462935416027904 0.006831281974911689 0.012268263101577758\n",
      "117 7.46106019988656 0.0069466098397970195 0.012552692443132401\n",
      "118 7.460709976032376 0.006975638121366501 0.0127068030834198\n",
      "119 7.469531863927841 0.006895744875073433 0.012371836751699448\n",
      "120 7.4728344436734915 0.006851003214716911 0.012265353947877883\n",
      "121 7.474924053996801 0.007231007970869541 0.013348265141248703\n",
      "122 7.469958474859595 0.0074367871880531315 0.01210214450955391\n",
      "123 7.461683830246329 0.006892396524548531 0.01216238796710968\n",
      "124 7.4773295149207115 0.006863918855786324 0.012222692668437958\n",
      "125 7.460644718259573 0.0067851777076721196 0.012342650294303894\n",
      "126 7.461869798600674 0.0068230943158268925 0.01198059245944023\n",
      "127 7.471680702641606 0.006820042110979557 0.012439016252756119\n",
      "128 7.458372846245766 0.007080555021762848 0.012061185985803604\n",
      "129 7.458546187728643 0.006959244012832642 0.012161423116922379\n",
      "130 7.468744270503521 0.0068405125141143795 0.011930773258209229\n",
      "131 7.472532102838159 0.006702258296310902 0.011790345460176467\n",
      "132 7.4635659996420145 0.006769529469311237 0.012258853614330292\n",
      "133 7.454420980066061 0.006772052057087421 0.011971950083971023\n",
      "134 7.464585974812508 0.006986696138978004 0.01204816773533821\n",
      "135 7.47157795727253 0.006716807395219803 0.012138873785734177\n",
      "136 7.460357101634145 0.006783936902880669 0.012154369056224823\n",
      "137 7.458438619971275 0.006992916144430638 0.012439469397068024\n",
      "138 7.464307127520442 0.00704203288257122 0.012107305228710175\n",
      "139 7.4725792948156595 0.007017010182142258 0.012024313062429428\n",
      "140 7.462249489501119 0.007075143486261368 0.011883098483085632\n",
      "141 7.460674224421382 0.006717108845710754 0.0117860509455204\n",
      "142 7.463171452283859 0.0066685167700052264 0.011981793195009232\n",
      "143 7.460956145077944 0.006816508859395981 0.011844293475151061\n",
      "144 7.466383026912808 0.006791616395115853 0.012448293715715408\n",
      "145 7.468999307602644 0.006889683522284031 0.012022084891796112\n",
      "146 7.460200930014253 0.006857007719576359 0.012155111730098724\n",
      "147 7.463313173502684 0.006731203988194465 0.011975673735141754\n",
      "148 7.463702766224742 0.0067246342152357105 0.01161406397819519\n",
      "149 7.46345205232501 0.00667280301451683 0.011655233800411224\n",
      "150 7.477814009413123 0.00666180644929409 0.011667920053005218\n",
      "151 7.468884913250804 0.006611582584679127 0.011810236126184464\n",
      "152 7.465998709201813 0.006556787520647049 0.01188016876578331\n",
      "153 7.471849426627159 0.006499263353645802 0.011463651359081268\n",
      "154 7.4747685845941305 0.006680284492671489 0.012614964246749879\n",
      "155 7.468997122719884 0.007682949990034103 0.011943272352218627\n",
      "156 7.471811708062887 0.00757187308371067 0.011959909349679948\n",
      "157 7.468690810725093 0.006974899642169475 0.012116094827651977\n",
      "158 7.485065001994371 0.0065734702795743945 0.011681724041700363\n",
      "159 7.470827490091324 0.006573058009147644 0.011746186167001724\n",
      "160 7.469033800065517 0.006707310028374195 0.011825370788574218\n",
      "161 7.461844550445676 0.006647896528244018 0.011603602021932603\n",
      "162 7.47193687222898 0.006693084232509136 0.011554374992847442\n",
      "163 7.46108865737915 0.006418172761797905 0.011574972867965699\n",
      "164 7.460315372794867 0.006571666195988655 0.01164078339934349\n",
      "165 7.456900827586651 0.006682855285704136 0.01148208200931549\n",
      "166 7.465583784505725 0.00647553350776434 0.011481565088033677\n",
      "167 7.460053415969014 0.00657259052991867 0.011831101924180985\n",
      "168 7.462585262954235 0.006706732898950577 0.011730370223522186\n",
      "169 7.459918100386858 0.006684772633016109 0.011776890009641647\n",
      "170 7.47422407194972 0.006722088828682899 0.011683435291051864\n",
      "171 7.459009127691388 0.006858082219958306 0.01152787521481514\n",
      "172 7.465596469119191 0.006736401312053204 0.011397636085748673\n",
      "173 7.463021144270897 0.00648367815464735 0.011690677106380463\n",
      "174 7.4578832518309355 0.0064778348356485365 0.011634029150009155\n",
      "175 7.460205804556608 0.006677443981170654 0.011442631781101227\n",
      "176 7.462333589792252 0.0067773741558194165 0.0118636354804039\n",
      "177 7.465194074437022 0.006834605574607849 0.0114044588804245\n",
      "178 7.460952367633581 0.0065280115306377414 0.011856479346752167\n",
      "179 7.461805889382958 0.006554478898644447 0.011292603909969329\n",
      "180 7.460113223642111 0.0065503762736916545 0.011524897366762162\n",
      "181 7.45827504619956 0.006561256423592568 0.0118568517267704\n",
      "182 7.465410280972719 0.0065550382807850836 0.011308725327253341\n",
      "183 7.46182568743825 0.006580620497465134 0.011403487622737884\n",
      "184 7.4602861274033785 0.0064052511677145955 0.011245534420013428\n",
      "185 7.479373598471284 0.006283677369356156 0.011177299320697784\n",
      "186 7.463659027591348 0.00621366411447525 0.011060677021741867\n",
      "187 7.459875453263521 0.00640631090849638 0.01149221569299698\n",
      "188 7.461995886638761 0.0064292758628726005 0.011174402832984924\n",
      "189 7.465232258662581 0.006348361529409885 0.01120404303073883\n",
      "190 7.456983681768179 0.006803604297339916 0.011696053147315979\n",
      "191 7.467525087296963 0.006690985105931759 0.011120108366012573\n",
      "192 7.460924603044987 0.006386972829699516 0.011077821552753449\n",
      "193 7.479489406570792 0.006208275474607944 0.011101808547973633\n",
      "194 7.469547659158707 0.006860969476401806 0.011675529032945633\n",
      "195 7.46548855304718 0.006456104941666126 0.011276247948408126\n",
      "196 7.460021765902638 0.0066108939051628115 0.011143644452095031\n",
      "197 7.487226404249668 0.006465270332992077 0.011210302263498307\n",
      "198 7.459414279088378 0.006410616479814052 0.011466718167066575\n",
      "199 7.461989415809512 0.006960691630840302 0.011271773874759673\n",
      "200 7.456721313297749 0.005768130041658878 0.010670739859342575\n",
      "201 7.458741400390863 0.00539542705565691 0.010600100606679916\n",
      "202 7.461691113188863 0.005271165050566197 0.01055659979581833\n",
      "203 7.462141633033752 0.005230449005961418 0.010520024448633194\n",
      "204 7.460252018645406 0.005203193075954914 0.01055393248796463\n",
      "205 7.468818889930844 0.005187705837190151 0.010475296825170517\n",
      "206 7.46400180272758 0.005168247371912003 0.01047903910279274\n",
      "207 7.4646743927150965 0.005186701767146588 0.010505973398685454\n",
      "208 7.463309874758124 0.005223328791558742 0.010478227585554122\n",
      "209 7.480298105627298 0.005186014547944069 0.010617110878229141\n",
      "210 7.4752819407731295 0.005237679474055767 0.010523976534605026\n",
      "211 7.4774817284196615 0.005220750026404858 0.01059327945113182\n",
      "212 7.470722109079361 0.005176824301481247 0.010443767756223679\n",
      "213 7.464035492390394 0.005154120802879333 0.01041453868150711\n",
      "214 7.482370199635625 0.00524176898598671 0.010593976080417632\n",
      "215 7.4585769437253475 0.005197618916630745 0.010408477932214737\n",
      "216 7.4589417316019535 0.005180766001343727 0.010443501919507981\n",
      "217 7.476645505055785 0.00529044222086668 0.010562333166599274\n",
      "218 7.467031430453062 0.005335710018873215 0.010391328185796737\n",
      "219 7.456282349303365 0.005241169929504395 0.01044864758849144\n",
      "220 7.457553645595908 0.005200347505509854 0.01041617751121521\n",
      "221 7.457420017570257 0.005221836373209953 0.010408671796321869\n",
      "222 7.463020754978061 0.0051900938600301745 0.010456530451774597\n",
      "223 7.461728725582361 0.005187313809990883 0.010335125774145127\n",
      "224 7.4709278997033834 0.005212849818170071 0.010413471460342407\n",
      "225 7.462887812405825 0.005290253803133965 0.010549599528312683\n",
      "226 7.459062563255429 0.005306038543581962 0.010381888747215271\n",
      "227 7.458379283547401 0.005580631628632545 0.010626935362815858\n",
      "228 7.465817388147116 0.005574963629245758 0.010643054693937302\n",
      "229 7.479927008971572 0.005529691927134991 0.010545534342527389\n",
      "230 7.4633839428424835 0.005456604450941086 0.010481758117675781\n",
      "231 7.459404917433858 0.005453387945890426 0.010440188944339752\n",
      "232 7.45940175652504 0.005398469626903534 0.010374614298343658\n",
      "233 7.46856851503253 0.00530456542223692 0.010360689163208007\n",
      "234 7.459722438827157 0.005344695344567299 0.01036916121840477\n",
      "235 7.466669388115406 0.00532901418954134 0.010431520342826843\n",
      "236 7.469276502728462 0.005498690158128738 0.010337702184915542\n",
      "237 7.461679581552744 0.005277781791985035 0.010122022628784179\n",
      "238 7.464340647682548 0.005170350275933743 0.010209866911172866\n",
      "239 7.4602766670286655 0.00515453651547432 0.010292655378580094\n",
      "240 7.475748259574175 0.005165419198572636 0.010247449278831483\n",
      "241 7.462859753519297 0.005103499077260494 0.010220735222101212\n",
      "242 7.462340589612722 0.005060540072619915 0.01022013783454895\n",
      "243 7.464058758690953 0.005272095791995526 0.010426752269268036\n",
      "244 7.484306855127215 0.005465111941099167 0.01018778458237648\n",
      "245 7.4593947641551495 0.005257132798433304 0.01027144655585289\n",
      "246 7.460947159677744 0.005258879758417607 0.010210030227899552\n",
      "247 7.461862927302718 0.005250437833368779 0.01038798525929451\n",
      "248 7.480148626491427 0.005269333072006702 0.01026075765490532\n",
      "249 7.468853065744042 0.005323369853198528 0.010161852091550827\n",
      "250 7.466438041999936 0.005117817923426628 0.010150984674692155\n",
      "251 7.475696459412575 0.005083905644714832 0.01007665753364563\n",
      "252 7.491525052115321 0.005103205859661102 0.010223748683929444\n",
      "253 7.472106399014592 0.0051968184411525726 0.010284760445356369\n",
      "254 7.4658057279884815 0.00515017169713974 0.010094482153654098\n",
      "255 7.475096674636006 0.005171603612601757 0.010187637060880661\n",
      "256 7.480844961479306 0.005302244946360588 0.010369753539562225\n",
      "257 7.465872408822179 0.0051176131144165995 0.01005713403224945\n",
      "258 7.458301603794098 0.005039567947387695 0.010107812881469726\n",
      "259 7.463609641417861 0.004999101661145687 0.010048574805259704\n",
      "260 7.461452296003699 0.005020808294415474 0.010030374079942703\n",
      "261 7.459022523835301 0.005066076919436455 0.010064621567726134\n",
      "262 7.457495843991637 0.005174418918788433 0.010047159790992737\n",
      "263 7.461809609085321 0.0052701660245656964 0.01030253767967224\n",
      "264 7.459987850859761 0.005685027934610844 0.010378928333520889\n",
      "265 7.458314737305045 0.005754309162497521 0.010269216895103454\n",
      "266 7.462941270321608 0.005404356226325035 0.010232348740100861\n",
      "267 7.458907255902886 0.005185008056461811 0.010105903893709182\n",
      "268 7.462820012122393 0.005172132164239883 0.010249720513820648\n",
      "269 7.460722152143717 0.005182775735855102 0.010174870789051056\n",
      "270 7.461225310340524 0.005191281475126743 0.010104702860116959\n",
      "271 7.467447938397527 0.005159008197486401 0.009991338104009628\n",
      "272 7.46272992901504 0.0051800949797034266 0.010178539752960205\n",
      "273 7.462057007476687 0.005170845940709114 0.009996892958879471\n",
      "274 7.460269985720515 0.005147319041192531 0.010080078691244125\n",
      "275 7.47431761957705 0.005095978587865829 0.01001055970788002\n",
      "276 7.4557310957461596 0.0051936421170830725 0.009959358721971512\n",
      "277 7.462390990927815 0.0051587789803743365 0.009888888001441956\n",
      "278 7.457924742251635 0.00509271041303873 0.009913734048604966\n",
      "279 7.466819353401661 0.005276521191000938 0.010144301056861878\n",
      "280 7.457722159102559 0.005194317325949669 0.00992207258939743\n",
      "281 7.458700429648161 0.005175587899982929 0.010029556155204773\n",
      "282 7.456271149218082 0.005090951584279537 0.010061539709568024\n",
      "283 7.480218578130007 0.005017576381564141 0.00986383244395256\n",
      "284 7.461331691592932 0.005005986846983433 0.009929058700799942\n",
      "285 7.460528610274196 0.005078173391520977 0.009820574671030044\n",
      "286 7.461074883118272 0.0049714984148740765 0.010085404813289643\n",
      "287 7.488908026367426 0.005206190474331379 0.010214300155639648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3e7b7a3b4124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Xinliang/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Xinliang/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "model = FMMTransformer(img_size=421, patch_size=4, in_chans=1, num_classes=2,\n",
    "                 embed_dim=32, depths=[1, 2, 1], num_heads=[1, 1, 1],\n",
    "                 window_size=[9, 4, 4], mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=None,\n",
    "                 use_checkpoint=False).to(device)\n",
    "FNOdecoder = FNO2d(12,12,32).to(device)\n",
    "print(count_params(model) + count_params(FNOdecoder))\n",
    "\n",
    "optimizer = torch.optim.Adam(set(model.parameters()) | set(FNOdecoder.parameters()), lr=1e-3, weight_decay=1e-4) # \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda(device)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        out = (out + FNOdecoder(out))[:, :s, :s, 0]     \n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            out = (out + FNOdecoder(out))[:, :s, :s, 0] \n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_l2, test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1957cd-13da-4340-99aa-700d06d1fd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.490366337820888 0.0043686659932136536 0.00829645574092865\n",
      "1 7.498552220873535 0.004052912712097168 0.008289273381233215\n",
      "2 7.495867848396301 0.004067386768758297 0.008287052214145661\n",
      "3 7.497307310812175 0.004059566080570221 0.008312220871448516\n",
      "4 7.4968070378527045 0.004073470391333103 0.008307057023048401\n",
      "5 7.495195134542882 0.004093295447528362 0.008291041404008865\n",
      "6 7.4958120891824365 0.004096587091684341 0.008277386873960495\n",
      "7 7.496122116222978 0.004115093439817428 0.008296602815389633\n",
      "8 7.48842166736722 0.004109777763485908 0.00831193447113037\n",
      "9 7.496706223115325 0.004173107944428921 0.008312439322471618\n",
      "10 7.499921839684248 0.004180296018719673 0.008362342566251756\n",
      "11 7.497675865888596 0.004117960192263127 0.008282361626625061\n",
      "12 7.494259113445878 0.004153036989271641 0.008312531262636185\n",
      "13 7.493751191534102 0.0041223356202244755 0.00837217628955841\n",
      "14 7.499295277521014 0.0041865741685032844 0.008332816511392593\n",
      "15 7.4923681346699595 0.004182357355952263 0.008286456465721131\n",
      "16 7.489707793109119 0.004125468403100968 0.008316939175128936\n",
      "17 7.492943428456783 0.004110099405050278 0.008299362063407898\n",
      "18 7.505737004801631 0.004128806568682194 0.008258216232061386\n",
      "19 7.5000636000186205 0.004132882714271546 0.008310731053352356\n",
      "20 7.494973867200315 0.004132735207676887 0.00825358122587204\n",
      "21 7.495831829495728 0.00412677039206028 0.00831199273467064\n",
      "22 7.488320994190872 0.004181099988520146 0.008319516628980637\n",
      "23 7.495713875629008 0.004164497517049313 0.008345243781805038\n",
      "24 7.499327890574932 0.004129695288836956 0.008342906236648559\n",
      "25 7.507049065083265 0.00415350492298603 0.008332363069057465\n",
      "26 7.498856241814792 0.004154295116662979 0.008254089653491973\n",
      "27 7.496067735366523 0.004111729308962822 0.008297736793756484\n",
      "28 7.510414133779705 0.00413751070946455 0.008247157037258148\n",
      "29 7.501852582208812 0.004133963547646999 0.008240074366331101\n",
      "30 7.5021103816106915 0.004144834212958813 0.008306392729282379\n",
      "31 7.504221429117024 0.004151768773794174 0.008315371721982956\n",
      "32 7.4984251242130995 0.004173533901572228 0.008312689512968064\n",
      "33 7.499014558270574 0.0041605387330055235 0.008328215330839156\n",
      "34 7.494927533902228 0.004197295986115932 0.008304201662540436\n",
      "35 7.49911121185869 0.004180179491639137 0.008264734745025634\n",
      "36 7.497833377681673 0.004145045392215252 0.00824227973818779\n",
      "37 7.503654790110886 0.004123487696051598 0.008284011781215668\n",
      "38 7.498463003896177 0.004143493205308914 0.008255999833345413\n",
      "39 7.504597887396812 0.004195697754621506 0.008296760618686677\n",
      "40 7.498004675842822 0.004180443905293941 0.008297287076711655\n",
      "41 7.506219091825187 0.004147209368646145 0.008237836509943008\n",
      "42 7.512239942327142 0.0041496059969067575 0.008307853788137436\n",
      "43 7.507313949987292 0.004149319544434547 0.008260498493909836\n",
      "44 7.500782035291195 0.004148438297212124 0.008247332721948624\n",
      "45 7.49482530914247 0.004141989015042782 0.008259553462266922\n",
      "46 7.501271617598832 0.004116143375635147 0.008269814848899841\n",
      "47 7.508795379661024 0.004120307363569736 0.00827280804514885\n",
      "48 7.5026492746546865 0.004164021499454975 0.008277290612459182\n",
      "49 7.500417722389102 0.004188745066523552 0.008284858018159867\n",
      "50 7.50455282535404 0.004171748459339142 0.008322300910949708\n",
      "51 7.5012417724356055 0.0041313814222812655 0.008256247639656067\n",
      "52 7.505288121290505 0.004113564893603325 0.008265524804592132\n",
      "53 7.501534932292998 0.004126134805381298 0.008332494646310806\n",
      "54 7.5014647813513875 0.004141568817198277 0.008265830129384994\n",
      "55 7.5013875560835 0.004156807288527489 0.00824114218354225\n",
      "56 7.5007358668372035 0.0041421581879258155 0.008281230479478836\n",
      "57 7.5038971127942204 0.0041556148082017895 0.008290358930826186\n",
      "58 7.496660053730011 0.004173910193145275 0.008324853032827378\n",
      "59 7.504964524880052 0.0041381996646523474 0.008237006962299348\n",
      "60 7.507766172289848 0.00410251858830452 0.00831343799829483\n",
      "61 7.50641153100878 0.0041066276580095295 0.008202079385519027\n",
      "62 7.500123321078718 0.004121413379907608 0.008278581500053405\n",
      "63 7.501713823527098 0.004148968577384949 0.00822642371058464\n",
      "64 7.50159564986825 0.004127344816923142 0.008214946538209915\n",
      "65 7.513247479684651 0.004124728210270405 0.008220860958099365\n",
      "66 7.500899816863239 0.004097444012761116 0.008197753131389618\n",
      "67 7.503456709906459 0.004111506573855877 0.00829087793827057\n",
      "68 7.500907869078219 0.004153723962605 0.008238587379455566\n",
      "69 7.498841803520918 0.004160046175122261 0.008268051445484162\n",
      "70 7.504928722977638 0.0041293825209140775 0.008181972950696946\n",
      "71 7.502293976955116 0.004108795046806336 0.00821348488330841\n",
      "72 7.5030452301725745 0.00414529974013567 0.008353209495544434\n",
      "73 7.504420439712703 0.004289712280035019 0.008210497796535493\n",
      "74 7.5050463220104575 0.004145543552935124 0.008207546323537826\n",
      "75 7.501805145293474 0.004111094333231449 0.00817311242222786\n",
      "76 7.500629737041891 0.0041035027652978895 0.008189678192138672\n",
      "77 7.501341300085187 0.004146416902542114 0.008197687119245529\n",
      "78 7.502365029416978 0.004163552716374397 0.008174265921115875\n",
      "79 7.503495251759887 0.004123995773494244 0.008266793936491013\n",
      "80 7.50710185430944 0.0041990790963172915 0.008191084563732147\n",
      "81 7.503481070511043 0.004173015862703323 0.008160562962293625\n",
      "82 7.503506460227072 0.004128366596996784 0.0082094606757164\n",
      "83 7.505534052848816 0.004125698506832123 0.008192750364542008\n",
      "84 7.502346924506128 0.004110999643802643 0.00820004239678383\n",
      "85 7.505798788741231 0.004146832585334778 0.008235456198453903\n",
      "86 7.503165720030665 0.004139826767146588 0.008186942636966706\n",
      "87 7.5113611333072186 0.004089806541800499 0.00818210631608963\n",
      "88 7.505146701820195 0.004134435780346394 0.008162802010774612\n",
      "89 7.5033302288502455 0.004178666800260544 0.00819042608141899\n",
      "90 7.505602287128568 0.0041805351749062535 0.008223365694284439\n",
      "91 7.500440499745309 0.00414760223031044 0.008148067444562913\n",
      "92 7.509958613663912 0.004119653649628162 0.008170875012874603\n",
      "93 7.499084978364408 0.004094086632132531 0.008148541301488876\n",
      "94 7.506347087211907 0.004092065081000328 0.008170591741800308\n",
      "95 7.499367781914771 0.0041249582022428516 0.008165062814950943\n",
      "96 7.502445778809488 0.004095157541334629 0.008260253369808197\n",
      "97 7.513460323214531 0.004136757872998715 0.008238687962293625\n",
      "98 7.496918837539852 0.004104019932448864 0.008172605484724045\n",
      "99 7.505899767391384 0.00421188648045063 0.008231418430805207\n",
      "100 7.5005271742120385 0.004026704967021942 0.008122392445802689\n",
      "101 7.517339773476124 0.0039713262245059016 0.008120385408401489\n",
      "102 7.521290344186127 0.003970907554030418 0.008105446845293044\n",
      "103 7.506582813337445 0.003955872587859631 0.008124435991048813\n",
      "104 7.506274165585637 0.003953732885420323 0.008079010844230652\n",
      "105 7.512868176214397 0.003951821453869343 0.008093539029359817\n",
      "106 7.499810403212905 0.003951610125601292 0.0080922931432724\n",
      "107 7.509910860098898 0.003947486452758312 0.008100646138191224\n",
      "108 7.503624122589827 0.0039470806047320365 0.008099181652069092\n",
      "109 7.5012678522616625 0.003961620837450028 0.00807064563035965\n",
      "110 7.504131713882089 0.003955009184777736 0.008075153827667237\n",
      "111 7.497870400547981 0.003960783876478672 0.008111020177602768\n",
      "112 7.5034454725682735 0.003963114485144615 0.008067763894796371\n",
      "113 7.500750742852688 0.003953331105411052 0.008087717443704606\n",
      "114 7.503385786898434 0.003952018260955811 0.008091865330934525\n",
      "115 7.49845842178911 0.003956927768886089 0.008104791641235351\n",
      "116 7.503423441201448 0.003952631518244743 0.008098607212305068\n",
      "117 7.50180028937757 0.003950584694743156 0.008072368949651718\n",
      "118 7.5031937919557095 0.003947980508208275 0.008092607706785201\n",
      "119 7.499007914215326 0.003948892027139664 0.008072700947523118\n",
      "120 7.507846781052649 0.003956324145197868 0.00809482902288437\n",
      "121 7.513879877515137 0.003960010834038258 0.00810657873749733\n",
      "122 7.5116583136841655 0.003954991087317466 0.008094906657934189\n",
      "123 7.501474612392485 0.003956232503056526 0.008113325834274291\n",
      "124 7.504582089371979 0.003959368266165256 0.00806719571352005\n",
      "125 7.5021026926115155 0.003951668851077557 0.008063238114118576\n",
      "126 7.502769926562905 0.003953674659132957 0.008078506737947464\n",
      "127 7.503016155213118 0.003949601374566555 0.008078455179929733\n",
      "128 7.507297357544303 0.003944138519465923 0.00806444615125656\n",
      "129 7.5096311047673225 0.003951593331992626 0.008067108988761902\n",
      "130 7.505922421813011 0.003952947400510311 0.008091499656438827\n",
      "131 7.50894661527127 0.003958451256155968 0.008058481961488723\n",
      "132 7.505021140910685 0.003953335009515286 0.008083269149065018\n",
      "133 7.5041537610813975 0.003953501202166081 0.008077976107597352\n",
      "134 7.502858225256205 0.00396421941369772 0.008082008063793182\n",
      "135 7.5075938776135445 0.0039519650489091875 0.008076535314321518\n",
      "136 7.502453510649502 0.003945992089807987 0.008074372559785842\n",
      "137 7.501281537115574 0.003956895291805268 0.008083713799715042\n",
      "138 7.5146118411794305 0.0039694544300436976 0.008086823970079421\n",
      "139 7.505945059470832 0.003964136198163033 0.008122758716344833\n",
      "140 7.526404984295368 0.003982583880424499 0.008053122460842133\n",
      "141 7.516082834452391 0.003958104968070984 0.00810059428215027\n",
      "142 7.515396233648062 0.004005807302892208 0.008078675270080566\n",
      "143 7.505113312974572 0.0039636693000793454 0.008070673644542694\n",
      "144 7.496313073672354 0.003969611100852489 0.0080684033036232\n",
      "145 7.506147865206003 0.003949624300003052 0.008061532974243163\n",
      "146 7.498541588895023 0.003939851723611355 0.008063168823719024\n",
      "147 7.503244768828154 0.003938806854188443 0.008056067526340485\n",
      "148 7.507240829989314 0.003940773293375969 0.00805229291319847\n",
      "149 7.504724000580609 0.003938920363783837 0.008064633458852768\n",
      "150 7.499512926675379 0.003932341441512108 0.008044158369302749\n",
      "151 7.4987705098465085 0.0039424881339073185 0.008077037781476974\n",
      "152 7.501686613075435 0.003942795805633068 0.0080316162109375\n",
      "153 7.504312160424888 0.003942777492105961 0.008047606498003006\n",
      "154 7.4997890796512365 0.003939740672707558 0.008031972348690034\n",
      "155 7.504562992602587 0.003953901156783104 0.008064118474721908\n",
      "156 7.4949433682486415 0.003954664073884487 0.008101998269557953\n",
      "157 7.506346130743623 0.003976816907525062 0.008035089522600174\n",
      "158 7.4989259680733085 0.003959194779396057 0.00805405616760254\n",
      "159 7.496533422730863 0.003948092967271805 0.008069678992033005\n",
      "160 7.506435398943722 0.003963924847543239 0.008016568422317506\n",
      "161 7.501239218749106 0.003949169076979161 0.008085620254278184\n",
      "162 7.503924809396267 0.003956999875605107 0.008120235204696655\n",
      "163 7.501886000856757 0.003949860535562039 0.008035593032836914\n",
      "164 7.506162301637232 0.003935642302036285 0.008038382530212402\n",
      "165 7.505646609701216 0.003936291135847568 0.008023565411567688\n",
      "166 7.505843055434525 0.003931956939399243 0.008026534467935562\n",
      "167 7.503036142326891 0.003930618979036808 0.0080661341547966\n",
      "168 7.502319338731468 0.003944269344210625 0.008045746982097625\n",
      "169 7.5032953806221485 0.003965277791023254 0.008043288588523864\n",
      "170 7.502014375291765 0.003975198946893215 0.008088518977165221\n",
      "171 7.5004756804555655 0.0039564107581973075 0.008032677620649338\n",
      "172 7.504494947381318 0.003953306436538696 0.008049409240484237\n",
      "173 7.498253794386983 0.003938102722167968 0.008046279996633529\n",
      "174 7.503693115897477 0.003935130469501018 0.008042460680007935\n",
      "175 7.50649718940258 0.003932328902184964 0.008016695380210876\n",
      "176 7.500780097208917 0.003960080556571484 0.008079375773668289\n",
      "177 7.509728950448334 0.003941685788333416 0.008074774593114852\n",
      "178 7.503621597774327 0.003939231596887111 0.008041097223758698\n",
      "179 7.502936711534858 0.003941988065838814 0.008054996132850647\n",
      "180 7.500591149553657 0.00394588378071785 0.008009921014308929\n",
      "181 7.506008446216583 0.003938261412084102 0.007994845658540726\n",
      "182 7.502996012568474 0.003931140385568142 0.008026599884033203\n",
      "183 7.501980869099498 0.003929195702075958 0.008017611652612687\n",
      "184 7.502089467830956 0.003927218869328499 0.00800847664475441\n",
      "185 7.5017893724143505 0.003935509115457534 0.008022325932979585\n",
      "186 7.496159956790507 0.003936224363744259 0.008005057722330093\n",
      "187 7.498478366062045 0.0039346802011132245 0.00800800770521164\n",
      "188 7.501324772834778 0.003933485001325608 0.0080291248857975\n",
      "189 7.50292569026351 0.003929608277976513 0.00800217553973198\n",
      "190 7.493071489036083 0.0039269070699810985 0.008026436865329743\n",
      "191 7.5001195557415485 0.003932370260357857 0.008016524165868759\n",
      "192 7.502445489168167 0.003935057193040848 0.00801506295800209\n",
      "193 7.499300121329725 0.0039337860196828845 0.008020729571580888\n",
      "194 7.503683567047119 0.00392330613732338 0.00798551768064499\n",
      "195 7.497512051835656 0.003926027029752731 0.007998088151216507\n",
      "196 7.496553361415863 0.003931148834526539 0.008045006096363067\n",
      "197 7.492269397713244 0.00391923052072525 0.008007189929485322\n",
      "198 7.501225047744811 0.003913840062916279 0.007966731190681458\n",
      "199 7.501896284520626 0.0039224779754877094 0.007991670817136764\n",
      "200 7.495802631601691 0.003872465670108795 0.007960453033447265\n",
      "201 7.493410809896886 0.0038577315211296083 0.007978409379720688\n",
      "202 7.495114772580564 0.0038588967695832254 0.007975686341524124\n",
      "203 7.504506166093051 0.0038616808503866198 0.007962912023067474\n",
      "204 7.4938581408932805 0.003854849375784397 0.007982887625694275\n",
      "205 7.496044953353703 0.0038565115109086036 0.007970887869596481\n",
      "206 7.488709888420999 0.0038542063385248186 0.007992297410964966\n",
      "207 7.4927721712738276 0.0038532397970557214 0.007971435487270355\n",
      "208 7.4935522535815835 0.003853267803788185 0.00796815350651741\n",
      "209 7.493460585363209 0.0038516987189650535 0.007978151589632033\n",
      "210 7.493124596774578 0.0038589202016592024 0.007979055494070053\n",
      "211 7.493917043320835 0.0038536315336823463 0.007969938069581985\n",
      "212 7.493308709934354 0.003852045476436615 0.007973306328058243\n",
      "213 7.500690724700689 0.0038567768037319184 0.007947867810726166\n",
      "214 7.492871186695993 0.0038531652763485907 0.007945206016302109\n",
      "215 7.4926080564036965 0.0038555921092629434 0.007974161207675934\n",
      "216 7.4997746190056205 0.0038584062382578848 0.007963087409734726\n",
      "217 7.494208034127951 0.0038519949764013292 0.0079616878926754\n",
      "218 7.494933633133769 0.003856154091656208 0.00797303855419159\n",
      "219 7.500278106890619 0.003856672517955303 0.007975927144289017\n",
      "220 7.4907746482640505 0.0038501077368855476 0.007975314408540726\n",
      "221 7.493265617638826 0.0038469628021121026 0.007957779616117478\n",
      "222 7.496255385689437 0.003851628616452217 0.007976593226194382\n",
      "223 7.492792058736086 0.0038496771082282065 0.007951174080371857\n",
      "224 7.495516517199576 0.003854000933468342 0.007966163605451583\n",
      "225 7.49149271287024 0.003856209434568882 0.00797064796090126\n",
      "226 7.501473307609558 0.0038573302701115607 0.00795666292309761\n",
      "227 7.5003906805068254 0.00385353684425354 0.007936341911554337\n",
      "228 7.4957225527614355 0.003852110840380192 0.007951933592557907\n",
      "229 7.490304931998253 0.0038547788336873055 0.00795228123664856\n",
      "230 7.498209702782333 0.0038511455282568934 0.007961770594120026\n",
      "231 7.495331837795675 0.003858540654182434 0.007960847020149231\n",
      "232 7.487420370802283 0.0038460297882556916 0.007960050553083419\n",
      "233 7.4945741621777415 0.0038485852852463724 0.007952230721712113\n",
      "234 7.501010678708553 0.00384612637758255 0.007940995544195174\n",
      "235 7.488482250832021 0.0038505041673779486 0.007963460236787796\n",
      "236 7.493031150661409 0.0038441320285201074 0.00796820506453514\n",
      "237 7.494856391102076 0.0038483528792858123 0.007944349348545075\n",
      "238 7.496040033176541 0.003849913224577904 0.00794237583875656\n",
      "239 7.490824022330344 0.0038496054783463476 0.007963824421167373\n",
      "240 7.490913656540215 0.0038523024916648865 0.00793710857629776\n",
      "241 7.487691483460367 0.003849172666668892 0.007946901917457581\n",
      "242 7.485870116390288 0.003845739409327507 0.007950016558170318\n",
      "243 7.484602967277169 0.003845705606043339 0.007941757589578628\n",
      "244 7.486891714856029 0.0038423227071762087 0.00795770451426506\n",
      "245 7.48539321590215 0.003842629462480545 0.00794527843594551\n",
      "246 7.485350121743977 0.0038418906480073927 0.00792644664645195\n",
      "247 7.484955738298595 0.003840687297284603 0.007960181683301926\n",
      "248 7.4817288890480995 0.003846975862979889 0.007935588359832763\n",
      "249 7.47910653334111 0.0038429541662335395 0.007934422194957734\n",
      "250 7.4839261174201965 0.0038385389670729637 0.007941639572381974\n",
      "251 7.483061706647277 0.0038445014730095865 0.007953020334243775\n",
      "252 7.482946622185409 0.003839400388300419 0.007945328503847122\n",
      "253 7.480398449115455 0.0038388799503445625 0.007915785610675812\n",
      "254 7.4811063297092915 0.0038430580869317056 0.007958098649978637\n",
      "255 7.48781456425786 0.003844707332551479 0.0079218527674675\n",
      "256 7.476575621403754 0.003845891058444977 0.007927156537771225\n",
      "257 7.483831803314388 0.0038397098258137704 0.007923847287893295\n",
      "258 7.478454961441457 0.003835722662508488 0.007948286831378937\n",
      "259 7.479806483723223 0.003841012142598629 0.007941286414861679\n",
      "260 7.480968645773828 0.003838485203683376 0.007926258146762849\n",
      "261 7.4785374673083425 0.003839673861861229 0.007935954183340073\n",
      "262 7.48132804967463 0.00383896803855896 0.007922244817018509\n",
      "263 7.483870941214263 0.0038340463787317276 0.00792712539434433\n",
      "264 7.480759512633085 0.0038361945375800133 0.007936230450868607\n",
      "265 7.477757140994072 0.0038382441177964213 0.00794587180018425\n",
      "266 7.482931535691023 0.0038371237814426424 0.007920414060354233\n",
      "267 7.4790561478585005 0.0038361074104905127 0.007908296585083009\n",
      "268 7.478609072044492 0.0038364582508802416 0.007921462059020995\n",
      "269 7.479005738161504 0.0038378604054450987 0.007929192632436752\n",
      "270 7.475013834424317 0.003836451031267643 0.00793727844953537\n",
      "271 7.479463851079345 0.0038323412612080573 0.007913067489862442\n",
      "272 7.483497199602425 0.003835004039108753 0.007908361107110977\n",
      "273 7.488900133408606 0.003831606648862362 0.007940091937780381\n",
      "274 7.478927417658269 0.0038360582254827022 0.007917673140764237\n",
      "275 7.476642678491771 0.0038353055939078333 0.007949014902114868\n",
      "276 7.486163238063455 0.0038356877341866494 0.007935579866170883\n",
      "277 7.479801928624511 0.0038441112115979196 0.007933453917503356\n",
      "278 7.477633279748261 0.003830353781580925 0.007938373386859893\n",
      "279 7.477724411524832 0.0038354287892580032 0.007962294816970826\n",
      "280 7.4789048144593835 0.0038427230790257453 0.007929278761148453\n",
      "281 7.480239783413708 0.0038344546109437944 0.007909038215875625\n",
      "282 7.476075955666602 0.003828719839453697 0.0079148168861866\n",
      "283 7.485390623100102 0.003830655388534069 0.0079402594268322\n",
      "284 7.481607116758823 0.0038270896226167677 0.007906284928321839\n",
      "285 7.4805114809423685 0.003829208478331566 0.007935279458761215\n",
      "286 7.478179031051695 0.0038321150466799738 0.007906879633665084\n",
      "287 7.474057746119797 0.0038269004374742507 0.007914348095655442\n",
      "288 7.478973878547549 0.0038253649547696114 0.00792204648256302\n",
      "289 7.474315283820033 0.0038270108476281166 0.007908741384744645\n",
      "290 7.476834933273494 0.0038225468397140505 0.007911226600408555\n",
      "291 7.476427586749196 0.0038245524242520334 0.007908340096473693\n",
      "292 7.482089647091925 0.003838406518101692 0.007913977652788163\n",
      "293 7.479196200147271 0.0038308518826961515 0.007898109704256058\n",
      "294 7.484559236094356 0.0038289224207401274 0.007906930893659592\n",
      "295 7.4764386005699635 0.0038225091621279716 0.0079050312936306\n",
      "296 7.474904200062156 0.003826584130525589 0.007916927337646484\n",
      "297 7.478288880549371 0.0038222784027457236 0.007902741581201553\n",
      "298 7.481658431701362 0.0038264127746224404 0.007907273769378663\n",
      "299 7.476522938348353 0.003829388953745365 0.007911487966775894\n",
      "300 7.473871850408614 0.00380014318972826 0.007904469817876816\n",
      "301 7.472609715536237 0.003795138843357563 0.00790060669183731\n",
      "302 7.474731650203466 0.0037941192016005517 0.00788856327533722\n",
      "303 7.482877752743661 0.0037939967438578607 0.007891932129859924\n",
      "304 7.476703749038279 0.0037924278005957604 0.00789372444152832\n",
      "305 7.4770879270508885 0.0037947928160429 0.007887615561485291\n",
      "306 7.476713040843606 0.0037945738434791566 0.007888978123664856\n",
      "307 7.482186919078231 0.0037934263572096823 0.007882230877876283\n",
      "308 7.481592560186982 0.0037935965433716773 0.007888959050178527\n",
      "309 7.475022278726101 0.00379478769749403 0.007891378700733184\n",
      "310 7.473826106637716 0.0037978467792272566 0.007886589765548705\n",
      "311 7.4839125499129295 0.003794438198208809 0.00789679154753685\n",
      "312 7.477017225697637 0.0037938704416155813 0.007879857271909714\n",
      "313 7.480144498869777 0.0037922741249203683 0.007885746657848358\n",
      "314 7.469581893645227 0.003795402832329273 0.007901963889598846\n",
      "315 7.480688624083996 0.0037947502732276917 0.007895054519176483\n",
      "316 7.481572485528886 0.0037940108850598336 0.007890473157167434\n",
      "317 7.477360560558736 0.003792284056544304 0.007889128774404525\n",
      "318 7.478011850267649 0.003794010542333126 0.007889065593481064\n",
      "319 7.480302858166397 0.00379311665892601 0.0078839111328125\n",
      "320 7.480788081884384 0.0037927100732922554 0.007887104004621505\n",
      "321 7.476957878097892 0.003793551690876484 0.0078777277469635\n",
      "322 7.474214905872941 0.0037958883568644524 0.007886025309562682\n",
      "323 7.477977288886905 0.003791165113449097 0.00788852944970131\n",
      "324 7.4739194847643375 0.003793056108057499 0.007877968102693558\n",
      "325 7.476728824898601 0.0037899725139141083 0.007879382967948913\n",
      "326 7.484532907605171 0.0037916641309857367 0.007892693728208541\n",
      "327 7.490817326121032 0.0037906885370612145 0.007897062003612518\n",
      "328 7.475769861601293 0.0037942163571715354 0.007879214435815811\n",
      "329 7.478124490007758 0.003790913172066212 0.007883003354072571\n",
      "330 7.483581533655524 0.0037906889989972113 0.007888549268245696\n",
      "331 7.48184419889003 0.0037889112383127214 0.007901316732168198\n",
      "332 7.474872675724328 0.0037894254550337792 0.00788657009601593\n",
      "333 7.486983771435916 0.003790856659412384 0.007884891331195831\n",
      "334 7.491694514639676 0.0037894724905490874 0.007880866825580597\n",
      "335 7.486103484407067 0.0037883056104183197 0.007884526699781418\n",
      "336 7.481880837120116 0.0037896801978349686 0.007879457026720047\n",
      "337 7.476630334742367 0.003790712170302868 0.007883365154266358\n",
      "338 7.47918301448226 0.003790311239659786 0.007882166653871536\n",
      "339 7.479393944144249 0.003787844143807888 0.007881442457437516\n",
      "340 7.471436727792025 0.0037891087606549264 0.007881829589605331\n",
      "341 7.476926336996257 0.0037887234538793563 0.00787587344646454\n",
      "342 7.477392610162497 0.0037863236963748934 0.00787354752421379\n",
      "343 7.478065884672105 0.003790117993950844 0.007884211391210556\n",
      "344 7.479468620382249 0.0037888974025845528 0.007870607227087021\n",
      "345 7.479476351290941 0.0037883764132857324 0.007882431149482727\n",
      "346 7.47263406123966 0.0037874309718608857 0.007877597957849503\n",
      "347 7.476539580151439 0.003786007486283779 0.007883079797029496\n",
      "348 7.472449157387018 0.003786973915994167 0.007893754988908768\n",
      "349 7.478260817937553 0.003786751002073288 0.007863731682300567\n",
      "350 7.478029791265726 0.003786448523402214 0.007884577214717865\n",
      "351 7.47181442938745 0.003785997949540615 0.007879109680652618\n",
      "352 7.4707901971414685 0.003785599060356617 0.007873532474040984\n",
      "353 7.468005470000207 0.0037848008796572685 0.007877124845981598\n",
      "354 7.471996915526688 0.003784844771027565 0.007875535637140274\n",
      "355 7.4722225442528725 0.0037863361835479737 0.00786456823348999\n",
      "356 7.467713517136872 0.00378470341861248 0.007869035303592682\n",
      "357 7.46931161172688 0.003784990683197975 0.007879773080348968\n",
      "358 7.4673533253371716 0.003784649409353733 0.007865131050348282\n",
      "359 7.469728680327535 0.0037863340750336648 0.007873980402946472\n",
      "360 7.464589765295386 0.0037902080938220025 0.007884097695350646\n",
      "361 7.472889695316553 0.0037830407917499543 0.007869510948657989\n",
      "362 7.459461584687233 0.003783917710185051 0.007877113968133927\n",
      "363 7.464840408414602 0.0037838736400008202 0.007875836491584777\n",
      "364 7.4693694319576025 0.003786104992032051 0.00787060558795929\n",
      "365 7.466259425505996 0.0037862716913223265 0.00787909835577011\n",
      "366 7.45763582456857 0.0037859710529446603 0.007868556678295136\n",
      "367 7.466387533582747 0.0037836993634700776 0.007866015583276749\n",
      "368 7.47603618633002 0.0037815892174839975 0.007877483516931533\n",
      "369 7.467481999658048 0.0037825191617012026 0.007873622328042984\n",
      "370 7.465783158317208 0.0037822681888937952 0.007869998514652253\n",
      "371 7.484097664244473 0.0037809571251273153 0.007870717346668244\n",
      "372 7.478649208322167 0.0037796456813812256 0.00786920726299286\n",
      "373 7.474914021790028 0.0037801299542188646 0.007882408946752548\n",
      "374 7.477949984371662 0.003783945567905903 0.007874845266342164\n",
      "375 7.469194906763732 0.00378328625112772 0.007865181267261505\n",
      "376 7.465589321218431 0.0037819360122084617 0.007863551676273346\n",
      "377 7.462077801115811 0.0037801966741681098 0.00788051262497902\n",
      "378 7.46414399612695 0.003780295416712761 0.007872914969921112\n",
      "379 7.467226400971413 0.0037805824726819992 0.007873532176017762\n",
      "380 7.468233457766473 0.003780767723917961 0.007866376340389251\n",
      "381 7.468360584229231 0.003781111128628254 0.007856850177049637\n",
      "382 7.469973053783178 0.0037803395837545397 0.007868822813034058\n",
      "383 7.472897984087467 0.0037818683087825775 0.007853521257638931\n",
      "384 7.466713992878795 0.003777082182466984 0.007861869186162948\n",
      "385 7.475461293011904 0.0037822517156600954 0.007867639362812042\n",
      "386 7.461677727289498 0.0037791393622756005 0.007856097966432572\n",
      "387 7.4692318495363 0.003778007626533508 0.007860578447580337\n",
      "388 7.467198823578656 0.0037789368107914926 0.007859210222959519\n",
      "389 7.471170987933874 0.003777706615626812 0.007862357348203659\n",
      "390 7.4729332216084 0.0037776710987091062 0.007864087373018264\n",
      "391 7.467728394083679 0.003778297148644924 0.007870292365550995\n",
      "392 7.466218553483486 0.003781317628920078 0.007844711244106293\n",
      "393 7.471361949108541 0.00377700188010931 0.007867154777050018\n",
      "394 7.473035017028451 0.0037816539481282233 0.007864369750022889\n",
      "395 7.470574853010476 0.003777035355567932 0.007857713401317596\n",
      "396 7.468957031145692 0.0037786990851163865 0.007861114889383316\n",
      "397 7.468562899157405 0.0037774312049150467 0.00785349577665329\n",
      "398 7.458589876070619 0.0037768832072615625 0.007852151095867156\n",
      "399 7.4701488157734275 0.0037743900790810584 0.007862305790185929\n",
      "400 7.462909260764718 0.003761813260614872 0.007853716164827346\n",
      "401 7.4700893852859735 0.0037612749487161636 0.00785217121243477\n",
      "402 7.464302325621247 0.003761455841362476 0.0078459894657135\n",
      "403 7.470099465921521 0.0037607187032699585 0.007846403270959854\n",
      "404 7.467563364654779 0.003760436974465847 0.007837959229946137\n",
      "405 7.471274553798139 0.0037606209069490433 0.00784650281071663\n",
      "406 7.468768232502043 0.0037610853761434553 0.00784654676914215\n",
      "407 7.470271559432149 0.0037604797780513765 0.007849810421466827\n",
      "408 7.4726643757894635 0.0037597782239317894 0.007841581553220749\n",
      "409 7.47367175295949 0.0037605601027607917 0.007844341844320297\n",
      "410 7.465319507755339 0.0037604037001729013 0.007842641174793243\n",
      "411 7.472561439499259 0.0037598374411463736 0.007857267111539841\n",
      "412 7.472119112499058 0.0037603097185492517 0.0078492371737957\n",
      "413 7.468546036630869 0.00375992052257061 0.007847766131162644\n",
      "414 7.47024801466614 0.0037587736025452615 0.007846700549125672\n",
      "415 7.464550407603383 0.003758985370397568 0.007845698595046998\n",
      "416 7.476658857427537 0.003758880153298378 0.007843768745660782\n",
      "417 7.465442165732384 0.003759616255760193 0.007845210731029511\n",
      "418 7.4675421910360456 0.003760098651051521 0.007842297405004502\n",
      "419 7.466897442936897 0.0037594427689909937 0.00784678801894188\n",
      "420 7.46725245565176 0.0037592770159244537 0.00784820556640625\n",
      "421 7.471443441696465 0.0037581565603613852 0.007844097465276718\n",
      "422 7.472152890637517 0.003758388839662075 0.007843343019485473\n",
      "423 7.464073698967695 0.003759969003498554 0.007847979664802551\n",
      "424 7.468538678251207 0.00375897154211998 0.007837980538606643\n",
      "425 7.475608873181045 0.003757514141499996 0.007843300402164459\n",
      "426 7.477563451975584 0.0037577665373682975 0.007839063555002213\n",
      "427 7.46967554744333 0.0037582772225141525 0.007840996235609054\n",
      "428 7.473857249133289 0.003756876416504383 0.007836587876081467\n",
      "429 7.470861857756972 0.003758639819920063 0.007844652831554413\n",
      "430 7.467774114571512 0.003758973442018032 0.007840747237205506\n",
      "431 7.468633816577494 0.0037582316398620604 0.007850181609392166\n",
      "432 7.473647219128907 0.0037574549987912176 0.007843744605779648\n",
      "433 7.474760372191668 0.003757253035902977 0.007849039733409882\n",
      "434 7.475200334563851 0.003758294776082039 0.007840425372123719\n",
      "435 7.483061516657472 0.003757583610713482 0.007828972339630126\n",
      "436 7.468662992119789 0.0037572380527853967 0.007849081158638001\n",
      "437 7.479168405756354 0.0037571728006005286 0.007844797223806382\n",
      "438 7.477872368879616 0.0037579946666955947 0.007838627099990845\n",
      "439 7.473189189098775 0.0037559107691049576 0.007835402488708496\n",
      "440 7.478502877987921 0.00375555669516325 0.007848158180713653\n",
      "441 7.4693714482709765 0.0037561401203274726 0.007840728163719177\n",
      "442 7.475405608303845 0.003756409652531147 0.007837916016578675\n",
      "443 7.477780936285853 0.0037571173384785654 0.007850122153759003\n",
      "444 7.4705057963728905 0.003756055511534214 0.007833109349012374\n",
      "445 7.477371945977211 0.003756426014006138 0.007837539911270142\n",
      "446 7.471797294914722 0.00375719203799963 0.007841809540987016\n",
      "447 7.480533042922616 0.0037549201771616934 0.007831271141767501\n",
      "448 7.474512232467532 0.0037563568353652954 0.007829177975654602\n",
      "449 7.4758284986019135 0.0037579026445746424 0.007840086966753005\n",
      "450 7.474113329313695 0.0037554544284939765 0.007830930054187774\n",
      "451 7.480381382629275 0.0037557170316576956 0.007844482958316802\n",
      "452 7.474925134330988 0.003754750795662403 0.007828958481550217\n",
      "453 7.468897861428559 0.0037566842213273047 0.007831666618585587\n",
      "454 7.478595672175288 0.0037556029334664346 0.007830320447683335\n",
      "455 7.477708000689745 0.0037551249116659162 0.007840692549943924\n",
      "456 7.47161687258631 0.0037541295662522314 0.007837368249893189\n",
      "457 7.479503883980215 0.003755292750895023 0.00784371629357338\n",
      "458 7.485584306530654 0.0037548817843198776 0.007846799045801163\n",
      "459 7.4762779120355844 0.0037543525695800783 0.007829234451055527\n",
      "460 7.483539687469602 0.0037543266490101816 0.007831721156835557\n",
      "461 7.478655549697578 0.0037536191195249556 0.00783197045326233\n",
      "462 7.489388128742576 0.0037536450624465942 0.007835787683725358\n",
      "463 7.4750790083780885 0.003755006954073906 0.007838727980852127\n",
      "464 7.479171588085592 0.0037555167749524117 0.007847205698490144\n",
      "465 7.475870788097382 0.003755436323583126 0.007827961593866348\n",
      "466 7.479009014554322 0.0037526565194129944 0.007841029018163682\n",
      "467 7.472126469016075 0.003754002422094345 0.00783921092748642\n",
      "468 7.476668280549347 0.0037530575320124625 0.00783919870853424\n",
      "469 7.477248042821884 0.003753414750099182 0.007837683856487275\n",
      "470 7.478846953250468 0.0037533746883273125 0.007837377786636353\n",
      "471 7.470278648659587 0.0037522602155804635 0.007830653488636017\n",
      "472 7.485917875543237 0.0037526332959532736 0.007833455353975297\n",
      "473 7.47890655323863 0.0037542015239596365 0.007842121422290802\n",
      "474 7.4767148569226265 0.003752770334482193 0.00784204363822937\n",
      "475 7.482922807335854 0.003752854958176613 0.0078352190554142\n",
      "476 7.48473556432873 0.0037523623779416085 0.007831040024757385\n",
      "477 7.485574736259878 0.0037518684938549996 0.007833742052316666\n",
      "478 7.479715935885906 0.003752499386668205 0.007832460105419159\n",
      "479 7.483262595720589 0.003752527393400669 0.007841816991567612\n",
      "480 7.479595262557268 0.003752601645886898 0.007828591614961624\n",
      "481 7.477892983704805 0.0037517460584640503 0.007829810827970504\n",
      "482 7.478127089329064 0.0037521058320999145 0.007819094210863114\n",
      "483 7.480955007486045 0.003752086013555527 0.007822472751140595\n",
      "484 7.482272479683161 0.0037520373389124872 0.007847620099782944\n",
      "485 7.476458635181189 0.003751318022608757 0.007830283939838409\n",
      "486 7.4833039827644825 0.0037528037577867508 0.007824848294258118\n",
      "487 7.4799228655174375 0.0037511866241693496 0.00782577246427536\n",
      "488 7.475335814058781 0.003750766910612583 0.007837776988744737\n",
      "489 7.475034453906119 0.003750229209661484 0.007823163419961929\n",
      "490 7.485165264457464 0.0037506695687770843 0.007823615223169328\n",
      "491 7.474850478582084 0.0037507744804024697 0.007835671603679657\n",
      "492 7.4770311545580626 0.003752597890794277 0.00782455861568451\n",
      "493 7.473847441375256 0.003748815931379795 0.00781827762722969\n",
      "494 7.470348872244358 0.0037490095719695093 0.007824681401252746\n",
      "495 7.477021814323962 0.0037498229369521143 0.0078266142308712\n",
      "496 7.477174920029938 0.0037490481287240983 0.00782361090183258\n",
      "497 7.469980170018971 0.003748948127031326 0.007822327464818955\n",
      "498 7.479210674762726 0.0037513291239738462 0.007813258171081543\n",
      "499 7.479967066086829 0.0037509532645344733 0.007819491028785706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "# model = FMMTransformer(img_size=421, patch_size=4, in_chans=1, num_classes=2,\n",
    "#                  embed_dim=16, depths=[2, 2, 1], num_heads=[2, 2, 2],\n",
    "#                  window_size=[9, 4, 4], mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "#                  norm_layer=nn.LayerNorm, ape=False, patch_norm=None,\n",
    "#                  use_checkpoint=False).to(device)\n",
    "# FNOdecoder = FNO2d(12,12,32).to(device)\n",
    "# print(count_params(model) + count_params(FNOdecoder))\n",
    "\n",
    "optimizer = torch.optim.Adam(set(model.parameters()) | set(FNOdecoder.parameters()), lr=1e-4, weight_decay=1e-4) # \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda(device)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        out = (out + FNOdecoder(out))[:, :s, :s, 0]     \n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            out = (out + FNOdecoder(out))[:, :s, :s, 0] \n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_l2, test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a3b1ffc-5ebe-44d8-83af-8461d24005e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7499147\n",
      "0 9.298418718390167 0.12282604479789734 0.10627585649490356\n",
      "1 9.307202514261007 0.08977596843242645 0.07908506035804748\n",
      "2 9.307995489798486 0.06418806409835816 0.05867569804191589\n",
      "3 9.311009164899588 0.04774973094463349 0.046282379031181334\n",
      "4 9.306773624382913 0.03737650257349014 0.039768205881118776\n",
      "5 9.32938418444246 0.029774819016456604 0.032284698486328124\n",
      "6 9.317904616706073 0.02701832675933838 0.031955252885818484\n",
      "7 9.312410549260676 0.02615096911787987 0.031826282143592836\n",
      "8 9.304820353165269 0.023692251443862916 0.02796761006116867\n",
      "9 9.306414652615786 0.021398834973573685 0.026586894094944\n",
      "10 9.30862864293158 0.020439614951610566 0.0250232270359993\n",
      "11 9.30777289532125 0.019035546451807023 0.022972458600997926\n",
      "12 9.316112362779677 0.017021965116262437 0.021865414083004\n",
      "13 9.307303570210934 0.017554054081439972 0.023135725557804108\n",
      "14 9.305648006498814 0.01728671142458916 0.021465469598770142\n",
      "15 9.316480587236583 0.01662946906685829 0.023075749576091768\n",
      "16 9.311526419594884 0.01611530789732933 0.02011726975440979\n",
      "17 9.309615425765514 0.015375121772289276 0.019729156494140625\n",
      "18 9.319442456588149 0.015052234917879105 0.01917750060558319\n",
      "19 9.307558350265026 0.014969997107982635 0.020241341292858123\n",
      "20 9.308688883669674 0.013936063915491103 0.01852549970149994\n",
      "21 9.325210838578641 0.01375446817278862 0.01961161196231842\n",
      "22 9.305318471975625 0.015229523122310639 0.01877956509590149\n",
      "23 9.307755004614592 0.014275338932871818 0.01868988573551178\n",
      "24 9.316652432084084 0.01468115819990635 0.019154343903064728\n",
      "25 9.305809556506574 0.013376927420496941 0.01826460540294647\n",
      "26 9.305562662892044 0.012904270648956299 0.017387935221195222\n",
      "27 9.307302992790937 0.012625041261315346 0.017273313999176025\n",
      "28 9.32131049502641 0.012242395743727683 0.016914099752902985\n",
      "29 9.31194104719907 0.012318866342306138 0.017149868011474608\n",
      "30 9.32135125156492 0.01353421936929226 0.01731872022151947\n",
      "31 9.314212627708912 0.012631230011582374 0.01600460559129715\n",
      "32 9.315741503611207 0.011945354849100113 0.015989295244216918\n",
      "33 9.313359417952597 0.012362161308526992 0.017788001000881196\n",
      "34 9.333357956260443 0.012847280517220496 0.016009703874588013\n",
      "35 9.309504658915102 0.012092569202184678 0.015574736595153809\n",
      "36 9.31078633107245 0.01193639200925827 0.01668771505355835\n",
      "37 9.338626702316105 0.012075698181986809 0.015486318171024322\n",
      "38 9.315146490931511 0.012020223706960678 0.015425624251365662\n",
      "39 9.31470521632582 0.01207170994579792 0.01561063051223755\n",
      "40 9.329286944121122 0.012342270493507385 0.017687335312366485\n",
      "41 9.314651457592845 0.012320915162563324 0.015784864723682405\n",
      "42 9.313483295030892 0.011355760216712951 0.015223307609558106\n",
      "43 9.333918361924589 0.012420446753501893 0.016760566532611848\n",
      "44 9.319729570299387 0.011517805352807045 0.016356096863746644\n",
      "45 9.329727873206139 0.011544872313737868 0.01600691705942154\n",
      "46 9.324707627296448 0.012074067324399949 0.016158927977085114\n",
      "47 9.334479083307087 0.012325287610292435 0.018457360565662384\n",
      "48 9.332587322220206 0.013225189715623855 0.016910222172737122\n",
      "49 9.338162717409432 0.01127160294353962 0.015638168454170227\n",
      "50 9.358174728229642 0.010955276533961295 0.015212185084819793\n",
      "51 9.32265970017761 0.011264371901750565 0.014852562248706817\n",
      "52 9.325195388868451 0.011012677058577537 0.014638806879520416\n",
      "53 9.328159470111132 0.011422692462801934 0.015026150047779083\n",
      "54 9.328864863142371 0.010780822575092316 0.014494897574186325\n",
      "55 9.317678053863347 0.010552612707018853 0.015478430688381195\n",
      "56 9.316909860819578 0.010379021316766738 0.014074563980102539\n",
      "57 9.319124481640756 0.0108965961933136 0.01419957548379898\n",
      "58 9.322597320191562 0.010649108365178107 0.015082134604454041\n",
      "59 9.330685562454164 0.010634842082858085 0.014371701776981353\n",
      "60 9.311670589260757 0.010693288177251816 0.014322307109832764\n",
      "61 9.30261883419007 0.01100219041109085 0.016984883546829223\n",
      "62 9.327442402020097 0.011024735435843468 0.01405564248561859\n",
      "63 9.3054373934865 0.011226674422621727 0.01467648446559906\n",
      "64 9.311298715882003 0.010716489881277085 0.014028726667165757\n",
      "65 9.311238264665008 0.010726768031716346 0.014085631072521209\n",
      "66 9.304754458367825 0.011103298485279084 0.013877407759428024\n",
      "67 9.306702624075115 0.011322560533881188 0.014375725090503692\n",
      "68 9.323857448995113 0.01093551829457283 0.014908197820186615\n",
      "69 9.310205021873116 0.010578966125845909 0.0141018508374691\n",
      "70 9.311869622208178 0.010282226294279098 0.013983398675918579\n",
      "71 9.321075302548707 0.010469354450702668 0.014231548607349397\n",
      "72 9.314819518476725 0.01155044275522232 0.014701606929302216\n",
      "73 9.307966968975961 0.010947812244296074 0.013962751775979996\n",
      "74 9.305909853428602 0.010350064724683762 0.013463398814201355\n",
      "75 9.319117210805416 0.01033259990811348 0.013932515382766724\n",
      "76 9.309727704152465 0.010608393773436546 0.015098574459552766\n",
      "77 9.306464059278369 0.010454697653651238 0.013350592851638794\n",
      "78 9.319568836130202 0.009810304656624793 0.013319856375455857\n",
      "79 9.311110403388739 0.010155312985181809 0.013952042162418365\n",
      "80 9.31473696231842 0.010167885974049568 0.0138580122590065\n",
      "81 9.30830594804138 0.010036075040698051 0.013523752689361573\n",
      "82 9.306552504189312 0.010415511667728423 0.013573122322559356\n",
      "83 9.306803055107594 0.01015639065206051 0.013618553876876832\n",
      "84 9.310531938448548 0.009410368591547012 0.014151656031608582\n",
      "85 9.302100844681263 0.0095813400298357 0.013523524552583694\n",
      "86 9.301977987401187 0.010515032783150673 0.01390698716044426\n",
      "87 9.317381263710558 0.010329585775732994 0.014020003378391266\n",
      "88 9.304259356111288 0.009958708867430686 0.013590680211782455\n",
      "89 9.30172370467335 0.010559757605195046 0.013380838185548782\n",
      "90 9.33576244302094 0.010930872142314911 0.014648535549640655\n",
      "91 9.307846116833389 0.010564076662063599 0.014262928366661072\n",
      "92 9.312686700373888 0.009784957632422446 0.012971923351287842\n",
      "93 9.327925214543939 0.009485447108745574 0.012782443761825562\n",
      "94 9.318420090712607 0.009667685210704803 0.01314627856016159\n",
      "95 9.314180901274085 0.009380603224039078 0.012975733578205109\n",
      "96 9.331368931569159 0.009402952671051025 0.013012088388204574\n",
      "97 9.331760379485786 0.009338891521096229 0.013629978895187378\n",
      "98 9.319378113374114 0.009655063405632974 0.01328605145215988\n",
      "99 9.331653658300638 0.010260941028594971 0.013323706090450287\n",
      "100 9.320788692682981 0.008447044566273688 0.011980883628129959\n",
      "101 9.316073214635253 0.0074530430436134335 0.012027620375156402\n",
      "102 9.32485603261739 0.007427332580089569 0.011493350118398667\n",
      "103 9.328269302845001 0.0069175949692726136 0.011338825225830079\n",
      "104 9.308496161364019 0.006848590321838856 0.01139314666390419\n",
      "105 9.299906566739082 0.007049094818532467 0.011538146287202835\n",
      "106 9.316788057796657 0.00666921266913414 0.011383033841848373\n",
      "107 9.304792470298707 0.006707118913531303 0.011218417286872864\n",
      "108 9.307484713383019 0.006725903198122978 0.011244721263647079\n",
      "109 9.315665262751281 0.0066873815283179285 0.011223553121089936\n",
      "110 9.306606143712997 0.006989141151309013 0.011257872581481934\n",
      "111 9.305080308578908 0.006765840463340282 0.01122497856616974\n",
      "112 9.324069358408451 0.006782219804823399 0.011055202037096024\n",
      "113 9.303430447354913 0.006770072653889656 0.011336548030376434\n",
      "114 9.30199954006821 0.006897804252803326 0.011023554503917694\n",
      "115 9.308359948918223 0.006852901197969914 0.011416217535734177\n",
      "116 9.30755521543324 0.0067897820696234705 0.011026449799537659\n",
      "117 9.306586691178381 0.007239138811826706 0.01136277750134468\n",
      "118 9.30622714292258 0.007300100579857826 0.011327481120824814\n",
      "119 9.312498674727976 0.006762146338820457 0.010981454998254776\n",
      "120 9.307577177882195 0.006667511396110058 0.011274523437023162\n",
      "121 9.31215445511043 0.007046261601150036 0.01211662322282791\n",
      "122 9.332688719034195 0.0075573275238275525 0.011410755962133408\n",
      "123 9.30963915400207 0.006766645528376102 0.01139143705368042\n",
      "124 9.312773254700005 0.006946341261267662 0.011262020468711853\n",
      "125 9.34503251966089 0.006906422346830368 0.011286450028419494\n",
      "126 9.31320643890649 0.0069466433301568035 0.011435140669345856\n",
      "127 9.310678387060761 0.007152010753750801 0.011317286193370819\n",
      "128 9.335239700041711 0.006945545896887779 0.011057107150554657\n",
      "129 9.308899961411953 0.007077275685966015 0.011273491978645325\n",
      "130 9.310531857423484 0.007239879295229912 0.010999000966548919\n",
      "131 9.319471844471991 0.0070147341191768645 0.011095323711633681\n",
      "132 9.336390798911452 0.006774059273302555 0.010823415666818619\n",
      "133 9.309294587001204 0.006797527685761451 0.01095997467637062\n",
      "134 9.316348111256957 0.0066908783987164495 0.011071642041206359\n",
      "135 9.321743956767023 0.006945989824831486 0.010939445048570633\n",
      "136 9.316762005910277 0.00674004103988409 0.010951715856790542\n",
      "137 9.309688362292945 0.0066847198978066446 0.01074966162443161\n",
      "138 9.326014370657504 0.00715129454433918 0.010971229076385498\n",
      "139 9.309782580472529 0.0071273882612586026 0.011327454894781113\n",
      "140 9.306884608231485 0.007116826720535755 0.010999989807605743\n",
      "141 9.338337210938334 0.007058971531689167 0.011046104282140732\n",
      "142 9.303174117580056 0.006764744706451893 0.010692316889762878\n",
      "143 9.308880938217044 0.006640514671802521 0.01067687287926674\n",
      "144 9.336065102368593 0.006759439311921597 0.010459163039922715\n",
      "145 9.30829203221947 0.006772074148058891 0.011223358362913131\n",
      "146 9.311215641908348 0.006854016557335854 0.011178100854158402\n",
      "147 9.344909261912107 0.006734789654612541 0.01062626600265503\n",
      "148 9.311883934773505 0.006629496917128563 0.010636169612407684\n",
      "149 9.306220324710011 0.006531766816973686 0.010751234889030457\n",
      "150 9.310329489409924 0.006679212257266045 0.010938815176486968\n",
      "151 9.311314425431192 0.006678764566779137 0.01055722400546074\n",
      "152 9.31460274849087 0.00653588393330574 0.01062067985534668\n",
      "153 9.329909583553672 0.006581327781081199 0.01054816946387291\n",
      "154 9.326925144530833 0.006568588025867939 0.010859732776880264\n",
      "155 9.305454834364355 0.0071894082874059675 0.010641656070947646\n",
      "156 9.314572447910905 0.006701497264206409 0.010877204984426498\n",
      "157 9.326873973943293 0.006651673808693886 0.010581938922405243\n",
      "158 9.305900855921209 0.006697809293866158 0.010414982587099076\n",
      "159 9.320870702154934 0.006871654734015465 0.010457387268543243\n",
      "160 9.31517006456852 0.006756366647779941 0.01049555316567421\n",
      "161 9.317254862748086 0.006919521443545819 0.01038094237446785\n",
      "162 9.321165909059346 0.00654528845846653 0.010653029680252075\n",
      "163 9.321781016886234 0.006592299319803715 0.010766548812389374\n",
      "164 9.31602211855352 0.006733563020825386 0.010432404130697251\n",
      "165 9.319479851052165 0.0066868232488632206 0.010665462464094161\n",
      "166 9.363733259961009 0.006678007252514362 0.01068247452378273\n",
      "167 9.318943136371672 0.006922807104885578 0.01130358412861824\n",
      "168 9.31936963275075 0.006703579187393188 0.01050424724817276\n",
      "169 9.312244288623333 0.006483755223453045 0.010179642885923386\n",
      "170 9.320752687752247 0.006495927914977074 0.010720590204000473\n",
      "171 9.322463381104171 0.006421326927840709 0.010433342903852463\n",
      "172 9.34922685008496 0.0064212649464607235 0.010389604568481446\n",
      "173 9.319406463764608 0.00673372196406126 0.010725487172603607\n",
      "174 9.319156136363745 0.006447078250348568 0.01068558618426323\n",
      "175 9.319091796875 0.006428792178630829 0.010541036128997802\n",
      "176 9.321126028895378 0.006748953729867935 0.010466172993183136\n",
      "177 9.320467744022608 0.006478835798799992 0.01086549699306488\n",
      "178 9.31555275246501 0.006558480277657509 0.010724662840366363\n",
      "179 9.326061585918069 0.006913003519177437 0.010572607666254043\n",
      "180 9.321020605042577 0.00715557038038969 0.010704301595687866\n",
      "181 9.336141380481422 0.006623603470623493 0.010514322817325592\n",
      "182 9.336682523600757 0.006331743314862252 0.010617125928401947\n",
      "183 9.332132990472019 0.006673970870673657 0.010772775411605835\n",
      "184 9.319252292625606 0.00673574859648943 0.010253322273492814\n",
      "185 9.363975810818374 0.0066412751972675325 0.010271129310131072\n",
      "186 9.322933742776513 0.006568726159632206 0.01035502627491951\n",
      "187 9.322861026972532 0.006381723187863827 0.010654167532920837\n",
      "188 9.366395963355899 0.006848969280719757 0.010616011321544647\n",
      "189 9.321243276819587 0.006526683785021305 0.010718287378549575\n",
      "190 9.324665741063654 0.006585411056876183 0.010371434092521668\n",
      "191 9.333027984015644 0.006467702858150006 0.010428250133991242\n",
      "192 9.32457214128226 0.006568944595754147 0.010295883268117905\n",
      "193 9.32095610909164 0.0065707026645541195 0.010745084583759308\n",
      "194 9.320908474735916 0.006485735662281513 0.010465100109577179\n",
      "195 9.317339436151087 0.006551336623728275 0.010578262060880661\n",
      "196 9.330031096935272 0.00654960411041975 0.010153049081563949\n",
      "197 9.31982294935733 0.00635062749683857 0.010203576385974885\n",
      "198 9.331051391549408 0.006341692589223385 0.010148925334215164\n",
      "199 9.30491840466857 0.006264985769987106 0.010242310464382171\n",
      "200 9.326415358111262 0.005625845156610012 0.009957315027713775\n",
      "201 9.363398179411888 0.005323877848684788 0.00971683233976364\n",
      "202 9.306520694866776 0.005277124151587486 0.009754348248243332\n",
      "203 9.310137992724776 0.005203757233917713 0.009703156054019928\n",
      "204 9.335683268494904 0.005197521708905697 0.009829345196485519\n",
      "205 9.327126378193498 0.005138594381511211 0.00970635488629341\n",
      "206 9.317068534903228 0.005068353869020939 0.009624754041433335\n",
      "207 9.316325532272458 0.005000379964709282 0.009551483094692231\n",
      "208 9.312877443619072 0.005036121740937233 0.009644192457199097\n",
      "209 9.323460371233523 0.005060081325471401 0.009701742678880692\n",
      "210 9.32445721514523 0.005057920299470425 0.00960270807147026\n",
      "211 9.308767735026777 0.005144476570189 0.00965696170926094\n",
      "212 9.311243794858456 0.005129809208214283 0.00977102741599083\n",
      "213 9.344628299586475 0.0053250033929944035 0.009948330372571946\n",
      "214 9.315543726086617 0.005184683158993721 0.00962766408920288\n",
      "215 9.316330192610621 0.005146402880549431 0.00975446805357933\n",
      "216 9.344901568256319 0.005206939429044723 0.009715289324522019\n",
      "217 9.30660790298134 0.005157091066241264 0.009782063961029052\n",
      "218 9.318419385701418 0.005228382043540478 0.009677410125732422\n",
      "219 9.345795514062047 0.0052464289292693134 0.009557021707296371\n",
      "220 9.328155216760933 0.00524848297983408 0.009573435634374619\n",
      "221 9.317302484996617 0.005200388155877591 0.009706869721412659\n",
      "222 9.311375855468214 0.0051794910803437235 0.009542034417390823\n",
      "223 9.31458156183362 0.005150640457868576 0.009502810835838317\n",
      "224 9.31772269681096 0.005196724966168404 0.009599266052246093\n",
      "225 9.31525824777782 0.00518278231471777 0.009611114710569382\n",
      "226 9.329299956560135 0.005294875934720039 0.009879275113344193\n",
      "227 9.32563788164407 0.005437650635838508 0.009834936261177063\n",
      "228 9.331692779436707 0.005420084446668625 0.009746586233377456\n",
      "229 9.344586847350001 0.005341206751763821 0.009764089286327361\n",
      "230 9.326932445168495 0.0053238974660635 0.009703450500965119\n",
      "231 9.327083471231163 0.005425736896693707 0.009849668443202973\n",
      "232 9.337799118831754 0.005278632782399655 0.00957058608531952\n",
      "233 9.332351346500218 0.005203568182885647 0.009572444558143616\n",
      "234 9.321661431342363 0.005245035618543625 0.009578558504581452\n",
      "235 9.336146374233067 0.005316593334078789 0.009494434297084808\n",
      "236 9.316106190904975 0.00536309639364481 0.009714668542146683\n",
      "237 9.324224226176739 0.0053492882177233695 0.009686635136604308\n",
      "238 9.332249364815652 0.005226309180259705 0.009555552899837495\n",
      "239 9.317071696743369 0.005182415597140789 0.009532370865345\n",
      "240 9.313230685889721 0.005212404489517212 0.009764660596847535\n",
      "241 9.340523460879922 0.0050780920684337615 0.009490702450275421\n",
      "242 9.318869626149535 0.005095587246119976 0.009447883665561676\n",
      "243 9.32197562418878 0.005302784591913223 0.009606997221708298\n",
      "244 9.318742972798645 0.005384084790945053 0.00968808025121689\n",
      "245 9.310251460410655 0.00518756066262722 0.009406645596027375\n",
      "246 9.3293132269755 0.005153283581137657 0.009333078116178513\n",
      "247 9.345131351612508 0.005077384561300278 0.009759842604398727\n",
      "248 9.328611036762595 0.005230175405740738 0.00949773907661438\n",
      "249 9.309214014559984 0.005092610895633697 0.009507897496223449\n",
      "250 9.310078757815063 0.005352425359189511 0.009642300754785537\n",
      "251 9.323611727915704 0.005481388792395592 0.00958240494132042\n",
      "252 9.310382707975805 0.0053432839885354045 0.009586822539567948\n",
      "253 9.311553148552775 0.005161859974265099 0.00940810352563858\n",
      "254 9.349132088944316 0.005234946139156819 0.00949563443660736\n",
      "255 9.30954031739384 0.00519041807204485 0.009486153572797775\n",
      "256 9.321914439089596 0.00502295459061861 0.009402275681495667\n",
      "257 9.3100447608158 0.00495900634676218 0.009339551627635955\n",
      "258 9.309259708970785 0.004974308803677559 0.009397501051425934\n",
      "259 9.305941012687981 0.00498915097117424 0.009381941109895707\n",
      "260 9.323714018799365 0.005033615954220295 0.009447388499975205\n",
      "261 9.30087845865637 0.005356655232608318 0.00950855404138565\n",
      "262 9.308849830180407 0.005147015623748303 0.009396242499351502\n",
      "263 9.327345114201307 0.005112268894910813 0.009447127878665924\n",
      "264 9.310330023057759 0.005135457679629326 0.009308244735002517\n",
      "265 9.314417466521263 0.00499055303633213 0.009421610981225967\n",
      "266 9.328091007657349 0.004978183530271053 0.009366975724697113\n",
      "267 9.32030520029366 0.005061873018741608 0.009366117268800735\n",
      "268 9.303202003240585 0.0050327244102954865 0.00940235584974289\n",
      "269 9.31621553748846 0.005049479506909847 0.009329971671104432\n",
      "270 9.320599378086627 0.004970970913767815 0.009539607763290405\n",
      "271 9.315536587499082 0.00502340055257082 0.009494522660970688\n",
      "272 9.32214548252523 0.005206576071679592 0.009848313182592392\n",
      "273 9.340174289420247 0.00523572451621294 0.009462555795907974\n",
      "274 9.319150815717876 0.005093439593911171 0.009322568476200103\n",
      "275 9.319378218613565 0.005089603036642075 0.009621863067150117\n",
      "276 9.345581158064306 0.005419461570680142 0.009464685767889024\n",
      "277 9.31255076546222 0.005447280041873455 0.009731984287500382\n",
      "278 9.320130639709532 0.005796833604574204 0.010152087956666946\n",
      "279 9.346050504595041 0.006050949402153492 0.010162862837314606\n",
      "280 9.316596634685993 0.005530976891517639 0.009392152279615403\n",
      "281 9.316638745367527 0.005075966462492943 0.009314658343791962\n",
      "282 9.33308544009924 0.005334974355995655 0.0096189147233963\n",
      "283 9.316075827926397 0.005368818216025829 0.00933867648243904\n",
      "284 9.312857948243618 0.005084806017577648 0.009337081611156463\n",
      "285 9.322263111360371 0.0050876749157905575 0.009492568969726563\n",
      "286 9.304963604547083 0.00501631673425436 0.00925460398197174\n",
      "287 9.302922601811588 0.004882492326200008 0.009290382415056229\n",
      "288 9.32528506219387 0.0048797097504138945 0.009308739751577377\n",
      "289 9.301165172830224 0.004933580413460732 0.009252720177173615\n",
      "290 9.30484305229038 0.004978829264640808 0.00940904587507248\n",
      "291 9.310387155972421 0.005085085533559322 0.009357185810804367\n",
      "292 9.301681940443814 0.004955787159502506 0.0092860209941864\n",
      "293 9.303379352204502 0.005114819049835205 0.00944934755563736\n",
      "294 9.29969073459506 0.005119514726102352 0.00936470553278923\n",
      "295 9.31932839564979 0.005504160195589066 0.00959059715270996\n",
      "296 9.30827670916915 0.005358029514551162 0.009364530593156815\n",
      "297 9.329908076673746 0.005223020732402802 0.009323321133852005\n",
      "298 9.320671412162483 0.005060852013528347 0.009279447495937348\n",
      "299 9.308862649835646 0.005234293542802334 0.009492528587579728\n",
      "300 9.304081048816442 0.00480313041061163 0.009166298061609268\n",
      "301 9.330832180567086 0.004600895695388317 0.009150548428297043\n",
      "302 9.306331612169743 0.004558090202510357 0.009053379893302918\n",
      "303 9.30739732645452 0.00454701341688633 0.009100668132305145\n",
      "304 9.349329799413681 0.004533732444047928 0.009091425091028213\n",
      "305 9.315019719302654 0.004520595163106918 0.009063440412282943\n",
      "306 9.312135329470038 0.004476337723433971 0.00907215714454651\n",
      "307 9.330261653289199 0.004454481407999992 0.009017590284347534\n",
      "308 9.317958387546241 0.004460582681000233 0.008986321985721588\n",
      "309 9.315281134098768 0.004446005858480931 0.009009525030851364\n",
      "310 9.315944314934313 0.00446179486066103 0.009033748060464858\n",
      "311 9.310674698092043 0.004461984373629093 0.009045310765504837\n",
      "312 9.303930228576064 0.004442116290330887 0.009008048623800278\n",
      "313 9.319251455366611 0.004456443876028061 0.00905813604593277\n",
      "314 9.304681886918843 0.0044583045467734336 0.009057754278182983\n",
      "315 9.310532961040735 0.004462875120341778 0.008969896882772445\n",
      "316 9.310290777124465 0.00443326410651207 0.009021518528461456\n",
      "317 9.336635205894709 0.004458634920418263 0.009067422747611999\n",
      "318 9.315823616459966 0.004472503177821636 0.009087281376123428\n",
      "319 9.30986036453396 0.004518097922205925 0.009026077091693879\n",
      "320 9.341363319195807 0.004491794407367706 0.008984821140766144\n",
      "321 9.311330817639828 0.004482049688696861 0.00903783455491066\n",
      "322 9.31807612720877 0.0044956251382827755 0.009032229781150819\n",
      "323 9.321441090665758 0.0045442115440964695 0.008980896174907684\n",
      "324 9.311942775733769 0.004465457819402218 0.00900681808590889\n",
      "325 9.309750937856734 0.0044879125729203224 0.009026658833026885\n",
      "326 9.318339527584612 0.004425628431141376 0.008988644033670425\n",
      "327 9.311238456517458 0.004472430236637593 0.009107586592435837\n",
      "328 9.29521385487169 0.004511842079460621 0.009055771231651306\n",
      "329 9.317825674079359 0.004518341556191444 0.008969926238059998\n",
      "330 9.301613544113934 0.004467733927071094 0.00898116171360016\n",
      "331 9.298297020606697 0.004472056858241558 0.008973491936922073\n",
      "332 9.319852428510785 0.004436291821300983 0.009125829339027405\n",
      "333 9.318949510343373 0.004420757085084915 0.008967281877994537\n",
      "334 9.295420764014125 0.004410874858498573 0.008962703198194503\n",
      "335 9.310600779950619 0.004385260216891765 0.00892838478088379\n",
      "336 9.314148818142712 0.004379708789288997 0.00898091822862625\n",
      "337 9.299315856769681 0.004420023843646049 0.009049951285123824\n",
      "338 9.29460112284869 0.0044230945333838464 0.00889502003788948\n",
      "339 9.32443382870406 0.004479413516819477 0.00902446448802948\n",
      "340 9.297647750005126 0.004469785906374454 0.008985934853553772\n",
      "341 9.302248809486628 0.00451681662350893 0.00893548384308815\n",
      "342 9.309077949263155 0.004471591278910637 0.008931556940078736\n",
      "343 9.298631712794304 0.004444060668349266 0.008951919078826904\n",
      "344 9.303858121857047 0.004492785803973675 0.00900383323431015\n",
      "345 9.318536689504981 0.004433563075959683 0.008948072791099548\n",
      "346 9.302041688933969 0.004397774249315262 0.008920592814683914\n",
      "347 9.304264380596578 0.0044429701119661335 0.00895500361919403\n",
      "348 9.333731278777122 0.004487400308251381 0.00896559789776802\n",
      "349 9.310469220392406 0.004493417128920555 0.00898431271314621\n",
      "350 9.315613507293165 0.00443755292147398 0.008965902626514435\n",
      "351 9.334490150213242 0.004424570851027965 0.008868991583585738\n",
      "352 9.318513084203005 0.004440573945641518 0.00899862214922905\n",
      "353 9.311688231304288 0.004431832760572433 0.009008626639842986\n",
      "354 9.331981601193547 0.00439739116281271 0.008940095901489259\n",
      "355 9.315046689473093 0.004408855222165584 0.008899944722652436\n",
      "356 9.31398210953921 0.004400267563760281 0.008832340091466905\n",
      "357 9.34118235297501 0.004405489891767502 0.008911847621202468\n",
      "358 9.3271474679932 0.004434951305389404 0.00891891822218895\n",
      "359 9.326900920830667 0.004524905636906624 0.0089879110455513\n",
      "360 9.337204914540052 0.004427557870745659 0.008891201168298722\n",
      "361 9.330213597975671 0.00441660114377737 0.008874209821224213\n",
      "362 9.334796275012195 0.0043566603809595105 0.008861705660820007\n",
      "363 9.323235724121332 0.0043262486457824705 0.008862122893333435\n",
      "364 9.328247718513012 0.004429222121834755 0.008999075144529342\n",
      "365 9.336888925172389 0.004454665541648865 0.008892169147729874\n",
      "366 9.331763530150056 0.004415643148124218 0.008963703960180282\n",
      "367 9.320495819672942 0.004440201930701732 0.008877334296703338\n",
      "368 9.329039797186852 0.004465695843100548 0.00885506808757782\n",
      "369 9.327070043422282 0.0044344888478517535 0.009069808572530747\n",
      "370 9.346044096164405 0.004445956028997898 0.008946753740310669\n",
      "371 9.337369360029697 0.004447366409003735 0.008973287045955658\n",
      "372 9.32999344728887 0.004438616871833802 0.008864537477493287\n",
      "373 9.34330466389656 0.004378028839826584 0.008846335262060166\n",
      "374 9.317686473019421 0.004383897222578525 0.008914188742637634\n",
      "375 9.325752586126328 0.0044012261778116225 0.008858909904956818\n",
      "376 9.347314774990082 0.0043897048979997634 0.008871145397424698\n",
      "377 9.322320836596191 0.004354881882667541 0.008891551196575165\n",
      "378 9.32017420604825 0.004436842545866966 0.008898652642965316\n",
      "379 9.345879944041371 0.004440336495637894 0.00880035474896431\n",
      "380 9.323760077357292 0.004444844171404839 0.008888355642557143\n",
      "381 9.327175538055599 0.004449404917657375 0.008775074183940888\n",
      "382 9.324826469644904 0.00441032911837101 0.008824459910392761\n",
      "383 9.335860720835626 0.004431466065347195 0.00886500060558319\n",
      "384 9.33322312682867 0.004472969099879265 0.008894221931695938\n",
      "385 9.321541304700077 0.004609893575310707 0.008954243212938308\n",
      "386 9.356531031429768 0.004612370558083057 0.008946133702993393\n",
      "387 9.328283444046974 0.004483603492379189 0.0088903446495533\n",
      "388 9.32522514835 0.004437523864209652 0.008859058022499084\n",
      "389 9.362566146999598 0.004392934188246727 0.008792281448841095\n",
      "390 9.330569527111948 0.004316621147096157 0.008841939568519593\n",
      "391 9.323531991802156 0.00436437488347292 0.008786991685628891\n",
      "392 9.331329189240932 0.004443788953125477 0.00874250590801239\n",
      "393 9.317141792736948 0.004360423393547535 0.008770785480737685\n",
      "394 9.32441287767142 0.0043529244288802145 0.008760795295238495\n",
      "395 9.343115938827395 0.004351969212293625 0.008903987258672714\n",
      "396 9.328491049818695 0.00437543998658657 0.008765307068824769\n",
      "397 9.322086040861905 0.004398269213736057 0.008784237653017044\n",
      "398 9.323948754929006 0.0043673750087618824 0.008749997764825821\n",
      "399 9.32580762449652 0.004361159585416317 0.008814292252063751\n",
      "400 9.321398194879293 0.004192441374063492 0.008710878044366837\n",
      "401 9.335044987499714 0.004166454456746578 0.008724439889192581\n",
      "402 9.32056033052504 0.004137128032743931 0.00868486449122429\n",
      "403 9.319088737480342 0.004131446205079555 0.008699311465024949\n",
      "404 9.347108270041645 0.004133577771484852 0.008679265379905701\n",
      "405 9.347298327833414 0.004119364127516747 0.008647864311933517\n",
      "406 9.318873788230121 0.004121014103293419 0.008683598190546036\n",
      "407 9.328530747443438 0.004116539806127548 0.008686587810516358\n",
      "408 9.344832890667021 0.004130188941955566 0.008679587244987488\n",
      "409 9.331991333514452 0.004140423007309437 0.008708741813898087\n",
      "410 9.3463389929384 0.004124759912490845 0.008698405921459198\n",
      "411 9.323541299439967 0.00413469947129488 0.008683832585811615\n",
      "412 9.329509290866554 0.004148000776767731 0.008675891160964965\n",
      "413 9.337061465717852 0.004148095585405826 0.00871430829167366\n",
      "414 9.35941431671381 0.004134005554020405 0.008689223378896714\n",
      "415 9.328490772284567 0.004145330622792244 0.008657786846160889\n",
      "416 9.320990310050547 0.0041239337474107745 0.008645286411046981\n",
      "417 9.348370488733053 0.004139320626854896 0.008704993575811386\n",
      "418 9.333910063840449 0.004132461793720722 0.008688073009252548\n",
      "419 9.319996746256948 0.0041405028179287914 0.008676971942186355\n",
      "420 9.351358344778419 0.004121625654399395 0.008656006753444672\n",
      "421 9.329180762171745 0.004183119662106037 0.008689360916614533\n",
      "422 9.328199531883001 0.004137047469615936 0.008700049221515656\n",
      "423 9.338018869049847 0.004128463812172413 0.008659098148345947\n",
      "424 9.335460343398154 0.004113705940544605 0.008668938726186752\n",
      "425 9.318859796971083 0.004115174613893032 0.008675995767116546\n",
      "426 9.350766429677606 0.004148632012307644 0.008698785156011581\n",
      "427 9.324055003933609 0.004119846157729625 0.008692112714052201\n",
      "428 9.323266281746328 0.004136905081570149 0.008676103800535202\n",
      "429 9.361553854309022 0.004106344282627105 0.008628255277872086\n",
      "430 9.331013520248234 0.004103700160980225 0.00864868625998497\n",
      "431 9.333659712225199 0.004132898136973381 0.008625143766403198\n",
      "432 9.3241605469957 0.004122081592679024 0.008664053827524186\n",
      "433 9.324683710932732 0.004113840393722057 0.008625832945108413\n",
      "434 9.321134364232421 0.004112572111189365 0.008661782592535019\n",
      "435 9.33477686997503 0.00410832267254591 0.008641636669635773\n",
      "436 9.340229039080441 0.004096448294818401 0.008640780448913574\n",
      "437 9.326900749467313 0.004140753045678139 0.008639038056135178\n",
      "438 9.334476050920784 0.004137261748313904 0.008633059412240983\n",
      "439 9.346036654897034 0.004135753631591797 0.00862515702843666\n",
      "440 9.320821777917445 0.004104692481458187 0.008588262349367142\n",
      "441 9.31699323747307 0.004107874177396297 0.008620397746562957\n",
      "442 9.359479904174805 0.004123032622039318 0.008576718866825104\n",
      "443 9.324019904248416 0.0041100699976086615 0.008684054166078568\n",
      "444 9.31905620265752 0.004183194532990456 0.008597757667303085\n",
      "445 9.347889349795878 0.004105033531785011 0.0085788294672966\n",
      "446 9.325435342267156 0.0041237731128931044 0.008600774407386779\n",
      "447 9.310360346920788 0.004104561433196068 0.00860357478260994\n",
      "448 9.333227404393256 0.004102733872830867 0.008650134056806565\n",
      "449 9.331867540255189 0.0041008365377783774 0.008617510348558425\n",
      "450 9.316578375175595 0.0041560474932193755 0.008636260777711869\n",
      "451 9.32389095146209 0.004098024047911167 0.008620807528495788\n",
      "452 9.329871053807437 0.004096820019185543 0.008620876222848892\n",
      "453 9.317474038340151 0.004116774313151836 0.008632624298334121\n",
      "454 9.313450363464653 0.00412959161400795 0.008625242412090301\n",
      "455 9.33516996446997 0.004131513498723507 0.008630820661783218\n",
      "456 9.318695318885148 0.0041272308677434925 0.008624504804611206\n",
      "457 9.327655083499849 0.004071498721837997 0.008587804138660431\n",
      "458 9.332963740453124 0.004089070565998555 0.008593326658010483\n",
      "459 9.318829556927085 0.004080689072608948 0.008624147474765777\n",
      "460 9.324451137334108 0.004099024169147015 0.008617011904716492\n",
      "461 9.349904475733638 0.004085986100137233 0.008562027961015701\n",
      "462 9.323465400375426 0.0040832143127918245 0.008579072207212449\n",
      "463 9.321855005808175 0.0041752903833985324 0.0086082124710083\n",
      "464 9.37324750609696 0.0042024004459381105 0.008661770522594451\n",
      "465 9.322680626064539 0.004254594512283802 0.008696751594543457\n",
      "466 9.332612474448979 0.004166052825748921 0.00867071196436882\n",
      "467 9.322826837189496 0.004114850588142872 0.00860940396785736\n",
      "468 9.322650708258152 0.004090907543897629 0.008584350496530533\n",
      "469 9.313812971115112 0.00410509742051363 0.008608967810869218\n",
      "470 9.32597209699452 0.004073184736073017 0.00856917455792427\n",
      "471 9.3217055182904 0.00406015482544899 0.008575819581747055\n",
      "472 9.314859167672694 0.00408283980935812 0.00860596239566803\n",
      "473 9.31903120316565 0.004071908473968506 0.008541607260704041\n",
      "474 9.341102242469788 0.004084421403706074 0.00858427882194519\n",
      "475 9.312905976548791 0.004079878389835358 0.008595383763313293\n",
      "476 9.31761225964874 0.004083172552287579 0.008641155809164048\n",
      "477 9.348538738675416 0.004089355580508709 0.008550561517477035\n",
      "478 9.316432323306799 0.004044967152178287 0.008568504303693771\n",
      "479 9.32037878409028 0.004056029379367828 0.008552423268556595\n",
      "480 9.321454823948443 0.004085924856364727 0.00863409325480461\n",
      "481 9.329694920219481 0.004061603881418705 0.008564516007900237\n",
      "482 9.314109514467418 0.004054506286978721 0.008538711369037628\n",
      "483 9.319592370651662 0.0040627911612391475 0.00853272557258606\n",
      "484 9.33657980710268 0.004099787287414074 0.008534193187952042\n",
      "485 9.313089037314057 0.004138242408633232 0.008654037415981293\n",
      "486 9.317388779483736 0.004129919901490212 0.008537525832653046\n",
      "487 9.335354392416775 0.004100122630596161 0.00851809248328209\n",
      "488 9.319841822609305 0.004093145422637463 0.008559622317552567\n",
      "489 9.313513895496726 0.0040826346129179 0.008545785397291183\n",
      "490 9.329428813420236 0.004062203235924244 0.008537603914737702\n",
      "491 9.322812507860363 0.004070516347885132 0.008540124595165254\n",
      "492 9.313669674098492 0.00405452173948288 0.008520158529281617\n",
      "493 9.330415616743267 0.004088936805725097 0.00852236270904541\n",
      "494 9.324381946586072 0.004045884981751442 0.008542262315750122\n",
      "495 9.307119701988995 0.004068668991327286 0.008546914905309677\n",
      "496 9.306992579251528 0.004069550700485707 0.00851086050271988\n",
      "497 9.331671878695488 0.004054222069680691 0.008539550602436066\n",
      "498 9.309142354875803 0.004072534166276455 0.008484503030776977\n",
      "499 9.3287846846506 0.004058828264474869 0.008546691983938218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "model = FMMTransformer(img_size=421, patch_size=4, in_chans=1, num_classes=2,\n",
    "                 embed_dim=32, depths=[2, 2, 1], num_heads=[2, 2, 2],\n",
    "                 window_size=[9, 4, 4], mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=None,\n",
    "                 use_checkpoint=False).to(device)\n",
    "FNOdecoder = FNO2d(12,12,64).to(device)\n",
    "print(count_params(model) + count_params(FNOdecoder))\n",
    "\n",
    "optimizer = torch.optim.Adam(set(model.parameters()) | set(FNOdecoder.parameters()), lr=1e-3, weight_decay=1e-4) # \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda(device)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        out = (out + FNOdecoder(out))[:, :s, :s, 0]     \n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            out = (out + FNOdecoder(out))[:, :s, :s, 0] \n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_l2, test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3414a0e-6ba2-48d9-b0b5-d9a3ccbdfa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNO2dRegressor(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2dRegressor, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "#         self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "#         self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "#         self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "#         self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "#         self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "#         self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "#         self.fc1 = nn.Linear(self.width, 128)\n",
    "#         self.fc2 = nn.Linear(self.width, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         grid = self.get_grid(x.shape, x.device)\n",
    "#         x = torch.cat((x, grid), dim=-1)\n",
    "#         x = self.fc0(x)\n",
    "        B, L, C = x.shape\n",
    "        x = x.view(B, int(L**(1/2)), int(L**(1/2)), C)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6006a9b-6f81-4308-8c47-f589a11016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "#     print(x.shape)\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.reconstruction = nn.Linear(2 * dim, 4 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x, direction='down', uplevel_result1=None, uplevel_result2=None):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        if direction=='down':\n",
    "            \n",
    "            H, W = self.input_resolution\n",
    "            B, L, C = x.shape\n",
    "#             print(x.shape)\n",
    "#             print(H, W)\n",
    "            assert L == H * W, \"input feature has wrong size\"\n",
    "            assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "            x = x.view(B, H, W, C)\n",
    "\n",
    "            x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "            x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "            x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "            x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "            x = self.norm(x)\n",
    "            x = self.reduction(x)\n",
    "        else:\n",
    "            H, W = self.input_resolution\n",
    "#             print(x.shape)\n",
    "            B = x.shape[0]\n",
    "            C = x.shape[-1]\n",
    "#             print(x.shape)\n",
    "#             print(H, W)\n",
    "#             assert H_0 == H // 2, \"input feature has wrong size\"\n",
    "            x = self.reconstruction(x)\n",
    "            x = x.view(B, H//2, W//2, 2*C)\n",
    "            New_C = C//2\n",
    "            x_target = torch.zeros(B, H, W, self.dim, device=x.device)\n",
    "\n",
    "            x_target[:, 0::2, 0::2, :] = x[...,:New_C] # B H/2 W/2 C\n",
    "            x_target[:, 1::2, 0::2, :] = x[...,New_C:2*New_C]  # B H/2 W/2 C\n",
    "            x_target[:, 0::2, 1::2, :] = x[...,2*New_C:3*New_C]  # B H/2 W/2 C\n",
    "            x_target[:, 1::2, 1::2, :] = x[...,3*New_C:] # B H/2 W/2 C\n",
    "            x = self.norm2(x_target) + uplevel_result1.view(B, H, W, self.dim) \n",
    "           \n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "    \n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None, stride_size=3):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [(img_size[0]-patch_size[0]+2*6) // stride_size+1, \n",
    "                              (img_size[0]-patch_size[0]+2*6) // stride_size+1]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride_size, padding=6)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "    \n",
    "class PatchUnEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        B, HW, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        return flops\n",
    "\n",
    "    \n",
    "class FMMTransformer(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=[7, 7, 7, 7], mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.downsamplers = nn.ModuleList()\n",
    "        self.regressors = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size[i_layer],\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=None,\n",
    "                               use_checkpoint=use_checkpoint)\n",
    "            regressor = FNO2dRegressor(int(12 * 2 ** -i_layer), int(12 * 2 ** -i_layer), int(embed_dim * 2 ** i_layer))\n",
    "            self.layers.append(layer)\n",
    "            self.regressors.append(regressor)\n",
    "            \n",
    "        for i_layer in range(self.num_layers-1):\n",
    "            layer = PatchMerging(dim=int(embed_dim * 2 ** i_layer),\n",
    "                                 input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)))\n",
    "            self.downsamplers.append(layer)\n",
    "            \n",
    "        self.norm = norm_layer(self.num_features//4)\n",
    "        self.reduction = nn.Linear(self.num_features//4, 1)\n",
    "#         self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "#         self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "#         x = self.norm(x)  # B L C\n",
    "#         x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "#         x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        y0 = self.layers[0](x)\n",
    "        r0 = self.regressors[0](y0)\n",
    "        x1 = self.downsamplers[0](y0)\n",
    "        y1 = self.layers[1](x1)\n",
    "        r1 = self.regressors[1](y1)\n",
    "        x2 = self.downsamplers[1](y1)\n",
    "        y2 = self.layers[2](x2)\n",
    "        r2 = self.regressors[2](y2)\n",
    "        r1 = self.downsamplers[1](r2, direction='up', uplevel_result1=r1)\n",
    "        r0 = self.downsamplers[0](r1, direction='up', uplevel_result1=r0)\n",
    "\n",
    "        x = self.reduction(F.gelu(self.norm(r0)))  # B L C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64e64d8e-4166-4a24-88e5-415dfad5058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.405306573957205 0.006953861683607101 0.01272925227880478\n",
      "1 7.390439540147781 0.006884744755923748 0.012661205679178238\n",
      "2 7.397443727590144 0.006818985320627689 0.012778270244598388\n",
      "3 7.398211655206978 0.006779484823346138 0.012690533995628356\n",
      "4 7.397454226389527 0.006612830005586147 0.012617146968841553\n",
      "5 7.406585471704602 0.006571423970162868 0.01256306529045105\n",
      "6 7.4224291779100895 0.0066012715473771095 0.01251039355993271\n",
      "7 7.419624709524214 0.006453196309506893 0.01252471774816513\n",
      "8 7.378728949464858 0.0064497913867235185 0.012400962859392167\n",
      "9 7.376713789068162 0.006347043395042419 0.012293878644704818\n",
      "10 7.390589775517583 0.006317442491650582 0.012379762977361678\n",
      "11 7.37775644287467 0.006333934806287289 0.012241893708705901\n",
      "12 7.376395315863192 0.0063008169531822205 0.01221461445093155\n",
      "13 7.3772459495812654 0.0062144075110554696 0.012240093797445298\n",
      "14 7.377151044085622 0.006151109658181667 0.012214636951684952\n",
      "15 7.383190956898034 0.006127494417130947 0.012224032133817673\n",
      "16 7.379520380869508 0.006059697031974793 0.012262357175350189\n",
      "17 7.379115182906389 0.006012702487409115 0.012156872749328614\n",
      "18 7.387865759432316 0.006091010630130768 0.012065102308988572\n",
      "19 7.386191642843187 0.005955654919147491 0.012079958468675614\n",
      "20 7.380279417149723 0.0059022422954440115 0.01204809233546257\n",
      "21 7.380040200427175 0.0059338794276118274 0.011947189718484878\n",
      "22 7.3934708293527365 0.00577419974654913 0.011928083449602127\n",
      "23 7.390470704995096 0.005875830218195916 0.01196449190378189\n",
      "24 7.379913441836834 0.005856279507279396 0.012057721614837646\n",
      "25 7.380892583169043 0.005708244532346725 0.011976321339607238\n",
      "26 7.381320360116661 0.0057841637507081035 0.01185121864080429\n",
      "27 7.392292526550591 0.0056842296198010445 0.011894716322422028\n",
      "28 7.38089020177722 0.0057228805497288706 0.011845159381628036\n",
      "29 7.38133545499295 0.00569433768093586 0.01185382142663002\n",
      "30 7.392791520804167 0.005598854713141918 0.011851661950349807\n",
      "31 7.388975686393678 0.0056141728311777115 0.01186175063252449\n",
      "32 7.382086144760251 0.005552418708801269 0.011859259903430938\n",
      "33 7.383584018796682 0.005551206573843956 0.01175983354449272\n",
      "34 7.380207791924477 0.00559182059019804 0.01176858440041542\n",
      "35 7.409425534307957 0.005509659260511398 0.01185682013630867\n",
      "36 7.384279211983085 0.005513993844389916 0.011752287447452546\n",
      "37 7.38283381704241 0.005401255540549755 0.01175605371594429\n",
      "38 7.379963465966284 0.005471323259174824 0.011744941920042037\n",
      "39 7.387056011706591 0.005351019479334355 0.011747353523969651\n",
      "40 7.37997318431735 0.005425758652389049 0.011749178171157837\n",
      "41 7.38131461571902 0.00554533376544714 0.011693563014268875\n",
      "42 7.3847610633820295 0.0052837016433477406 0.011702109575271607\n",
      "43 7.379568995907903 0.005240540117025376 0.011640699654817581\n",
      "44 7.38125167414546 0.005140938684344292 0.011674993634223939\n",
      "45 7.383391842246056 0.005283022284507752 0.011646237522363663\n",
      "46 7.380857285112143 0.005184839807450771 0.011644736230373383\n",
      "47 7.40198131557554 0.005206824362277985 0.011674050390720368\n",
      "48 7.38146114256233 0.005163307182490826 0.01156953901052475\n",
      "49 7.380125630646944 0.005119412086904049 0.011585396677255631\n",
      "50 7.38183899782598 0.00511075896024704 0.011563782393932343\n",
      "51 7.381357706151903 0.005097547955811024 0.011580996364355087\n",
      "52 7.381268417462707 0.005016530267894268 0.011615723073482513\n",
      "53 7.3820946756750345 0.004971441492438317 0.01148031622171402\n",
      "54 7.382463742047548 0.005017517693340778 0.011566881239414215\n",
      "55 7.381626003421843 0.00494294486939907 0.011549042761325836\n",
      "56 7.382052905857563 0.004959006473422051 0.011510820984840393\n",
      "57 7.38087669108063 0.0049156089797616 0.011400131285190582\n",
      "58 7.380854541435838 0.004903560422360897 0.011423955112695694\n",
      "59 7.378910242579877 0.0050059004649519925 0.011366510093212127\n",
      "60 7.380170437507331 0.004882969252765179 0.011403012424707412\n",
      "61 7.385360945016146 0.004811788812279701 0.011444045454263687\n",
      "62 7.381211922504008 0.004803771547973156 0.011418070048093795\n",
      "63 7.378982553258538 0.004957886517047882 0.01143923044204712\n",
      "64 7.380414984188974 0.004880684867501259 0.011460144817829133\n",
      "65 7.381666034460068 0.004851648472249508 0.011408177465200424\n",
      "66 7.381864360533655 0.004764352716505527 0.011420180052518845\n",
      "67 7.38041250128299 0.0048033203706145285 0.011393837332725525\n",
      "68 7.395855585113168 0.004822655439376831 0.01157580316066742\n",
      "69 7.381505978293717 0.004890287801623344 0.011436169147491454\n",
      "70 7.379673887975514 0.004791415482759476 0.01141413539648056\n",
      "71 7.379283678717911 0.004848224230110645 0.011358877271413803\n",
      "72 7.393987365998328 0.004672940090298653 0.011337227076292039\n",
      "73 7.3803814155980945 0.004663301706314087 0.0113161301612854\n",
      "74 7.37893804628402 0.004691696770489216 0.01130612477660179\n",
      "75 7.379671299830079 0.00466506128013134 0.011301160603761674\n",
      "76 7.3896488789469 0.004729179501533508 0.011328285485506058\n",
      "77 7.381298752501607 0.004673836007714272 0.011398977041244507\n",
      "78 7.379813849925995 0.004669445686042308 0.011321522742509842\n",
      "79 7.378038965165615 0.0045963160395622255 0.011326949596405029\n",
      "80 7.380704614333808 0.004609131999313831 0.011317793130874634\n",
      "81 7.389505006372929 0.004607489541172982 0.011312758326530456\n",
      "82 7.381815624423325 0.00453744289278984 0.01130628153681755\n",
      "83 7.379651526920497 0.004566993489861488 0.011280642747879028\n",
      "84 7.39031776227057 0.004620048135519027 0.011304548531770707\n",
      "85 7.390631088986993 0.0045899940431118015 0.011263194680213928\n",
      "86 7.380604554899037 0.004439274750649929 0.011234687119722366\n",
      "87 7.3783883368596435 0.004496464505791664 0.011224538683891297\n",
      "88 7.384735461324453 0.004471369825303555 0.011298785209655762\n",
      "89 7.387720885686576 0.0044938366040587424 0.011364095509052277\n",
      "90 7.379953988827765 0.004574737958610058 0.011428071707487107\n",
      "91 7.379193074069917 0.0044960141256451605 0.011229993999004364\n",
      "92 7.397297441959381 0.0044606695771217345 0.011232966482639313\n",
      "93 7.380440988577902 0.004479613438248634 0.011228396445512772\n",
      "94 7.380772966891527 0.0044838861450552945 0.011218730956315995\n",
      "95 7.37977687548846 0.0045426363945007325 0.011268360018730163\n",
      "96 7.379721033386886 0.00443228417634964 0.01123366504907608\n",
      "97 7.379160194657743 0.004402361996471882 0.01130988821387291\n",
      "98 7.380496148020029 0.004439318478107452 0.011243481487035752\n",
      "99 7.3808377999812365 0.0044376013800501825 0.011217425614595412\n",
      "100 7.379585498943925 0.004325890526175499 0.011173904985189437\n",
      "101 7.392003471031785 0.004136236853897571 0.011127007603645324\n",
      "102 7.38028384372592 0.00418340690433979 0.01114219069480896\n",
      "103 7.380404292605817 0.004194470159709454 0.011135582774877549\n",
      "104 7.379640644416213 0.004219331122934818 0.01114992767572403\n",
      "105 7.388476509600878 0.004163037560880184 0.011141819357872009\n",
      "106 7.381682416424155 0.0041303631737828255 0.011126948595046997\n",
      "107 7.381103254854679 0.004198878265917301 0.011132122129201888\n",
      "108 7.380421034060419 0.004121155373752117 0.011106453537940979\n",
      "109 7.3916332283988595 0.004115488030016422 0.01109513059258461\n",
      "110 7.381033812649548 0.00414832978695631 0.011117032021284104\n",
      "111 7.381898563355207 0.00412559612840414 0.011116075068712235\n",
      "112 7.380315420217812 0.00407715904712677 0.011114810407161713\n",
      "113 7.397067881189287 0.004118482962250709 0.011097737699747085\n",
      "114 7.378627358004451 0.004078451871871948 0.011089634895324708\n",
      "115 7.3807438230142 0.004096147380769253 0.011097092777490616\n",
      "116 7.379157828167081 0.004141251407563686 0.011068680733442306\n",
      "117 7.378050579689443 0.004137851409614086 0.011077956408262252\n",
      "118 7.398704204708338 0.004123150147497654 0.011093437075614929\n",
      "119 7.381448166444898 0.004121256045997143 0.011105506122112275\n",
      "120 7.379522570408881 0.004053385362029076 0.011104094088077545\n",
      "121 7.3801797879859805 0.004119217909872532 0.011078114360570908\n",
      "122 7.38062555808574 0.0040732759162783625 0.011081006824970246\n",
      "123 7.382508925162256 0.004084271438419819 0.011050988733768464\n",
      "124 7.379912330769002 0.004124720223248005 0.011069842129945755\n",
      "125 7.379505911841989 0.00406123261153698 0.011060987114906311\n",
      "126 7.38844009488821 0.004054312102496624 0.01106516808271408\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-bbbe0412a657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Xinliang/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Xinliang/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# # training and evaluation\n",
    "# ################################################################\n",
    "# model = FMMTransformer(img_size=421, patch_size=4, in_chans=1, num_classes=2,\n",
    "#                  embed_dim=32, depths=[1, 2, 1], num_heads=[1, 1, 1],\n",
    "#                  window_size=[9, 4, 4], mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.01,\n",
    "#                  norm_layer=nn.LayerNorm, ape=False, patch_norm=None,\n",
    "#                  use_checkpoint=False).to(device)\n",
    "# print(count_params(model))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # , weight_decay=1e-4\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "y_normalizer.cuda(device)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "    t1 = default_timer()\n",
    "    train_l2 = 0\n",
    "    for x, y in train_loader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)[:, :s, :s, 0]\n",
    "        out = y_normalizer.decode(out)\n",
    "        y = y_normalizer.decode(y)\n",
    "\n",
    "        loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_l2 += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "            out = model(x)[:, :s, :s, 0]\n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "    train_l2/= ntrain\n",
    "    test_l2 /= ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_l2, test_l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
