{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64941a82-b026-4218-9dc0-ca21c5ce7b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Zongyi Li\n",
    "This file is the Fourier Neural Operator for 2D problem such as the Darcy Flow discussed in Section 5.2 in the [paper](https://arxiv.org/pdf/2010.08895.pdf).\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# fourier layer\n",
    "################################################################\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width, ln=True):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(3, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "        \n",
    "#         if ln:\n",
    "#             self.bn0 = torch.nn.LayerNorm([width, s+self.padding, s+self.padding])\n",
    "#             self.bn1 = torch.nn.LayerNorm([width, s+self.padding, s+self.padding])\n",
    "#             self.bn2 = torch.nn.LayerNorm([width, s+self.padding, s+self.padding])\n",
    "#             self.bn3 = torch.nn.LayerNorm([width, s+self.padding, s+self.padding])\n",
    "            \n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "#         x = self.bn0(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "#         x = self.bn1(x)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "#         x = self.bn2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "#         x = self.bn3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "    \n",
    "    \n",
    "class UnitGaussianNormalizer(object):\n",
    "    def __init__(self, x, eps=0.00001):\n",
    "        super(UnitGaussianNormalizer, self).__init__()\n",
    "\n",
    "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
    "        self.mean = torch.mean(x, 0)\n",
    "        self.std = torch.std(x, 0)\n",
    "        self.eps = eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = (x - self.mean) / (self.std + self.eps)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, sample_idx=None):\n",
    "        if sample_idx is None:\n",
    "            std = self.std + self.eps # n\n",
    "            mean = self.mean\n",
    "        else:\n",
    "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
    "                std = self.std[sample_idx] + self.eps  # batch*n\n",
    "                mean = self.mean[sample_idx]\n",
    "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
    "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
    "                mean = self.mean[:,sample_idx]\n",
    "\n",
    "        # x is in shape of batch*n or T*batch*n\n",
    "        x = (x * std) + mean\n",
    "        return x\n",
    "\n",
    "    def cuda(self, device):\n",
    "        self.mean = self.mean.to(device)\n",
    "        self.std = self.std.to(device)\n",
    "\n",
    "    def cpu(self):\n",
    "        self.mean = self.mean.cpu()\n",
    "        self.std = self.std.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "063e4dd4-af22-486e-9bef-30e3291c67a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# configs\n",
    "################################################################\n",
    "device = torch.device('cuda:0')\n",
    "TRAIN_PATH = 'data/piececonst_r421_N1024_smooth1.mat'\n",
    "TEST_PATH = 'data/piececonst_r421_N1024_smooth2.mat'\n",
    "\n",
    "ntrain = 1000\n",
    "ntest = 100\n",
    "\n",
    "batch_size = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 300\n",
    "step_size = 100\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 32\n",
    "\n",
    "r = 3\n",
    "h = int(((421 - 1)/r) + 1)\n",
    "s = h\n",
    "\n",
    "################################################################\n",
    "# load data and data normalization\n",
    "################################################################\n",
    "reader = MatReader(TRAIN_PATH)\n",
    "x_train = reader.read_field('coeff')[:ntrain,::r,::r][:,:s,:s]\n",
    "y_train = reader.read_field('sol')[:ntrain,::r,::r][:,:s,:s]\n",
    "\n",
    "reader.load_file(TEST_PATH)\n",
    "x_test = reader.read_field('coeff')[:ntest,::r,::r][:,:s,:s]\n",
    "y_test = reader.read_field('sol')[:ntest,::r,::r][:,:s,:s]\n",
    "\n",
    "x_normalizer = UnitGaussianNormalizer(x_train)\n",
    "x_train = x_normalizer.encode(x_train)\n",
    "x_test = x_normalizer.encode(x_test)\n",
    "\n",
    "\n",
    "# y_train = y_normalizer.encode(y_train)\n",
    "\n",
    "x_train = x_train.reshape(ntrain,s,s,1)\n",
    "x_test = x_test.reshape(ntest,s,s,1)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train.to(device), y_train.to(device)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test.to(device), y_test.to(device)), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "919a02a8-4c4d-42ea-96c2-ad0bb0b68c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "\n",
    "def objective(modes, width, learning_rate):\n",
    "    \n",
    "    model = FNO2d(modes, modes, width).to(device)\n",
    "    print(count_params(model))\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "    #                        div_factor=1e4, \n",
    "    #                        final_div_factor=1e4,\n",
    "    #                        pct_start=0.3,\n",
    "    #                        steps_per_epoch=len(train_loader)*5, \n",
    "    #                        epochs=100)\n",
    "\n",
    "\n",
    "    myloss = LpLoss(size_average=False)\n",
    "\n",
    "    y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "    y_normalizer.cuda(device)\n",
    "\n",
    "    train_l2_rec, test_l2_rec = [], []\n",
    "    with tqdm(total=epochs) as pbar_ep:\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "    #         t1 = default_timer()\n",
    "            train_l2 = 0\n",
    "            for x, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                out = model(x).reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "        #         y = y_normalizer.decode(y)\n",
    "\n",
    "                loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                train_l2 += loss.item()\n",
    "\n",
    "            ############################\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            model.eval()\n",
    "            test_l2 = 0.0\n",
    "            with torch.no_grad():\n",
    "                for x, y in test_loader:\n",
    "                    out = model(x).reshape(batch_size, s, s)\n",
    "                    out = y_normalizer.decode(out)\n",
    "\n",
    "                    test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "            train_l2/= ntrain\n",
    "            test_l2 /= ntest\n",
    "            train_l2_rec.append(train_l2); test_l2_rec.append(test_l2)\n",
    "\n",
    "    #         t2 = default_timer()\n",
    "    #         print(epoch, t2-t1, train_l2, test_l2)\n",
    "            desc = f\"epoch: [{epoch+1}/{epochs}]\"\n",
    "            desc += f\" | current lr: {lr:.3e}\"\n",
    "            desc += f\"| train loss: {train_l2:.3e} \"\n",
    "            desc += f\"| val loss: {test_l2:.3e} \"\n",
    "            pbar_ep.set_description(desc)\n",
    "            pbar_ep.update()\n",
    "            \n",
    "    return test_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a0544d6-0c78-4f8c-9b21-3786ce09eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368001\n"
     ]
    }
   ],
   "source": [
    "test_l2 = objective(modes=12, width=32, learning_rate=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a27539b2-1d8d-4689-9cc5-ecdec372888a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012335291653871537"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08a56cc6-c561-42cf-b814-7624e31bcf67",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "objective() missing 1 required positional argument: 'weight_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-549897f7ef15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: objective() missing 1 required positional argument: 'weight_decay'"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "test_l2 = objective(modes=12, width=32, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f910a981-ff40-42ff-ab19-caf5624be3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "\n",
    "def objective(modes, width, learning_rate, weight_decay):\n",
    "    \n",
    "    model = FNO2d(modes, modes, width).to(device)\n",
    "    print(count_params(model))\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "    #                        div_factor=1e4, \n",
    "    #                        final_div_factor=1e4,\n",
    "    #                        pct_start=0.3,\n",
    "    #                        steps_per_epoch=len(train_loader)*5, \n",
    "    #                        epochs=100)\n",
    "\n",
    "\n",
    "    myloss = LpLoss(size_average=False)\n",
    "\n",
    "    y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "    y_normalizer.cuda(device)\n",
    "\n",
    "    train_l2_rec, test_l2_rec = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_l2 = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "    #         y = y_normalizer.decode(y)\n",
    "\n",
    "            loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_l2 += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                out = model(x).reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "\n",
    "                test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "        train_l2/= ntrain\n",
    "        test_l2 /= ntest\n",
    "        train_l2_rec.append(train_l2); test_l2_rec.append(test_l2)\n",
    "            \n",
    "    return test_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc5617a8-d914-4f8e-8b46-157191a4dbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7c05e85401df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncHyperBandScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHyperOptSearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from filelock import FileLock\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from hyperopt import hp\n",
    "import time\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "#import scdiag_model as scm\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "def easy_objective(config, reporter):\n",
    "    device = torch.device(\"cuda\")\n",
    "    learning_rate = config['learning_rate']\n",
    "    weight_decay = config['weight_decay']\n",
    "    modes = config['modes']\n",
    "    width = config['width']\n",
    "\n",
    "\n",
    "    loss = objective(modes=modes, width=width, \n",
    "                                         learning_rate=learning_rate,\n",
    "                                         weight_decay=weight_decay)\n",
    "    print(\"The val loss is \", loss)\n",
    "    reporter(loss=loss)\n",
    "\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "jobid = str(time.strftime('%m%d-%H%M%S', time.localtime(time.time())))\n",
    "\n",
    "ray.init(num_cpus=60, num_gpus=2, memory=100*1024*1024*1024, object_store_memory=4*1024*1024*1024) #memory=28*1024*1024*1024,object_store_memory=2*1024*1024*1024, redis_max_memory=2*1024*1024*1024\n",
    "\n",
    "\n",
    "space = {\n",
    "    \"learning_rate\": hp.choice(\"learning_rate\", [1e-4, 5e-4, 1e-3]),\n",
    "    \"weight_decay\": hp.choice(\"weight_decay\", [1e-4, 5e-4, 1e-3]),\n",
    "    \"pooling_rate\": hp.choice(\"pooling_rate\", [0.5, 0.65, 0.75]),\n",
    "    \"width\": hp.choice([32, 16, 64]),\n",
    "    \"nodes\": hp.choice([10, 12, 14]),\n",
    "\n",
    "}\n",
    "\n",
    "current_best_params = [\n",
    "    {\n",
    "        \"batch_size\": 256,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"nhid\": 256,\n",
    "        \"pooling_rate\": 0.75,\n",
    "    }\n",
    "]\n",
    "\n",
    "config = {\n",
    "    \"stop\": {\n",
    "        \"loss\": 1e-16,\n",
    "    }\n",
    "}\n",
    "\n",
    "algo = HyperOptSearch(\n",
    "    space,\n",
    "    max_concurrent=12,\n",
    "    metric='loss',\n",
    "    mode=\"min\",)\n",
    "\n",
    "scheduler = AsyncHyperBandScheduler(metric='loss', mode=\"min\")\n",
    "\n",
    "analysis = tune.run(easy_objective, search_alg=algo, scheduler=scheduler,\n",
    "                    num_samples = 24, resources_per_trial={\"cpu\": 1, \"gpu\": 0.5}, **config)\n",
    "\n",
    "print(\"Best config is:\", analysis.get_best_config(metric=\"loss\", mode='min'))\n",
    "text_file = open(\"best_config.txt\", \"w\")\n",
    "text_file.write(\"Best config is: %s\" % analysis.get_best_config(metric=\"loss\", mode='min'))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3100979-5f24-430d-b134-67dc36f5f1f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (900942764 vs. 536870912)\"\n\tdebug_error_string = \"{\"created\":\"@1642165956.689269854\",\"description\":\"Sent message larger than max (900942764 vs. 536870912)\",\"file\":\"src/core/ext/filters/message_size/message_size_filter.cc\",\"file_line\":268,\"grpc_status\":8}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-fbcc4b5d625e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#memory=28*1024*1024*1024,object_store_memory=2*1024*1024*1024, redis_max_memory=2*1024*1024*1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, _enable_object_reconstruction, _redis_max_memory, _plasma_directory, _node_ip_address, _driver_object_store_memory, _memory, _redis_password, _temp_dir, _metrics_export_port, _system_config, _tracing_startup_hook, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_post_init_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0mnode_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_node_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mflush_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             _internal_kv_put(\n\u001b[0m\u001b[1;32m    175\u001b[0m                 _make_key(self._prefix, category, key), value, overwrite=True)\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/experimental/internal_kv.py\u001b[0m in \u001b[0;36m_internal_kv_put\u001b[0;34m(key, value, overwrite, namespace)\u001b[0m\n\u001b[1;32m     76\u001b[0m     assert isinstance(key, bytes) and isinstance(value, bytes) and isinstance(\n\u001b[1;32m     77\u001b[0m         overwrite, bool)\n\u001b[0;32m---> 78\u001b[0;31m     return global_gcs_client.internal_kv_put(key, value, overwrite,\n\u001b[0m\u001b[1;32m     79\u001b[0m                                              namespace) == 0\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/gcs_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremaining_retry\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/gcs_utils.py\u001b[0m in \u001b[0;36minternal_kv_put\u001b[0;34m(self, key, value, overwrite, namespace)\u001b[0m\n\u001b[1;32m    247\u001b[0m         req = gcs_service_pb2.InternalKVPutRequest(\n\u001b[1;32m    248\u001b[0m             key=key, value=value, overwrite=overwrite)\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kv_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalKVPut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGcsCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    944\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     def with_call(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (900942764 vs. 536870912)\"\n\tdebug_error_string = \"{\"created\":\"@1642165956.689269854\",\"description\":\"Sent message larger than max (900942764 vs. 536870912)\",\"file\":\"src/core/ext/filters/message_size/message_size_filter.cc\",\"file_line\":268,\"grpc_status\":8}\"\n>"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_cpus=20, num_gpus=1) #memory=28*1024*1024*1024,object_store_memory=2*1024*1024*1024, redis_max_memory=2*1024*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "795df1c7-2689-4813-86e0-5d5e18da4ed9",
   "metadata": {},
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (900942764 vs. 536870912)\"\n\tdebug_error_string = \"{\"created\":\"@1642164711.905453333\",\"description\":\"Sent message larger than max (900942764 vs. 536870912)\",\"file\":\"src/core/ext/filters/message_size/message_size_filter.cc\",\"file_line\":268,\"grpc_status\":8}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b228478ce3e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ray.init(num_cpus=60, num_gpus=2) #memory=28*1024*1024*1024,object_store_memory=2*1024*1024*1024, redis_max_memory=2*1024*1024*1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m analysis = tune.run(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtraining_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     config={\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             experiments[i] = Experiment(\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                 \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, sync_config, trial_name_creator, trial_dirname_creator, log_to_file, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, export_formats, max_failures, restore)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;34m\"checkpointable function. You can specify checkpoints \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \"within your trainable function.\")\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/experiment.py\u001b[0m in \u001b[0;36mregister_if_needed\u001b[0;34m(cls, run_object)\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \"No name detected on trainable. Using {}.\".format(name))\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0mregister_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPicklingError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 extra_msg = (\"Other options: \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister_trainable\u001b[0;34m(name, trainable, warn)\u001b[0m\n\u001b[1;32m     74\u001b[0m         raise TypeError(\"Second argument must be convertable to Trainable\",\n\u001b[1;32m     75\u001b[0m                         trainable)\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0m_global_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINABLE_CLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, category, key, value)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_internal_kv_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mflush_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             _internal_kv_put(\n\u001b[0m\u001b[1;32m    175\u001b[0m                 _make_key(self._prefix, category, key), value, overwrite=True)\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/experimental/internal_kv.py\u001b[0m in \u001b[0;36m_internal_kv_put\u001b[0;34m(key, value, overwrite, namespace)\u001b[0m\n\u001b[1;32m     76\u001b[0m     assert isinstance(key, bytes) and isinstance(value, bytes) and isinstance(\n\u001b[1;32m     77\u001b[0m         overwrite, bool)\n\u001b[0;32m---> 78\u001b[0;31m     return global_gcs_client.internal_kv_put(key, value, overwrite,\n\u001b[0m\u001b[1;32m     79\u001b[0m                                              namespace) == 0\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/gcs_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremaining_retry\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/gcs_utils.py\u001b[0m in \u001b[0;36minternal_kv_put\u001b[0;34m(self, key, value, overwrite, namespace)\u001b[0m\n\u001b[1;32m    247\u001b[0m         req = gcs_service_pb2.InternalKVPutRequest(\n\u001b[1;32m    248\u001b[0m             key=key, value=value, overwrite=overwrite)\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kv_stub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalKVPut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mGcsCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    944\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     def with_call(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Sent message larger than max (900942764 vs. 536870912)\"\n\tdebug_error_string = \"{\"created\":\"@1642164711.905453333\",\"description\":\"Sent message larger than max (900942764 vs. 536870912)\",\"file\":\"src/core/ext/filters/message_size/message_size_filter.cc\",\"file_line\":268,\"grpc_status\":8}\"\n>"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "def training_function(config, checkpoint_dir=None):\n",
    "    # Hyperparameters\n",
    "    modes, width, learning_rate, weight_decay = config[\"modes\"], \n",
    "    config[\"width\"], config[\"learning_rate\"], config['weight_decay']\n",
    "    for step in range(10):\n",
    "        # Iterative training function - can be any arbitrary training procedure.\n",
    "        intermediate_score = objective(modes, width, learning_rate)\n",
    "        # Feed the score back back to Tune.\n",
    "        tune.report(mean_loss=intermediate_score)\n",
    "\n",
    "# ray.init(num_cpus=60, num_gpus=2) #memory=28*1024*1024*1024,object_store_memory=2*1024*1024*1024, redis_max_memory=2*1024*1024*1024\n",
    "\n",
    "analysis = tune.run(\n",
    "    training_function,\n",
    "    config={\n",
    "        \"modes\": tune.choice([12, 10, 14]),\n",
    "        \"width\": tune.choice([32, 16, 64]),\n",
    "        \"learning_rate\": tune.choice([0.007, 0.001, 0.002]),\n",
    "        \"weight_decay\": tune.choice([1e-4, 1e-3])\n",
    "    })\n",
    "\n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "    metric=\"mean_loss\", mode=\"min\"))\n",
    "\n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d56f352f-fae6-498d-8ebf-829a98da99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 19:34:34,002\tWARNING tune.py:570 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-14 19:34:34 (running for 00:00:00.64)<br>Memory usage on this node: 47.2/376.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/96 CPUs, 0/2 GPUs, 0.0/224.09 GiB heap, 0.0/100.03 GiB objects (0.0/1.0 accelerator_type:V100S)<br>Result logdir: /home/liuxinliang/ray_results/training_function_2022-01-14_19-34-33<br>Number of trials: 3/3 (2 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  alpha</th><th style=\"text-align: right;\">  beta</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_f23b7_00000</td><td>RUNNING </td><td>192.168.1.138:38543</td><td style=\"text-align: right;\">  0.001</td><td style=\"text-align: right;\">     1</td></tr>\n",
       "<tr><td>training_function_f23b7_00001</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">  0.01 </td><td style=\"text-align: right;\">     2</td></tr>\n",
       "<tr><td>training_function_f23b7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">  0.1  </td><td style=\"text-align: right;\">     1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for training_function_f23b7_00000:\n",
      "  date: 2022-01-14_19-34-34\n",
      "  done: false\n",
      "  experiment_id: 24814074aaef443f836e896277e671b3\n",
      "  hostname: cluster2021-5\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 192.168.1.138\n",
      "  pid: 38543\n",
      "  time_since_restore: 0.0001049041748046875\n",
      "  time_this_iter_s: 0.0001049041748046875\n",
      "  time_total_s: 0.0001049041748046875\n",
      "  timestamp: 1642160074\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f23b7_00000\n",
      "  \n",
      "Result for training_function_f23b7_00000:\n",
      "  date: 2022-01-14_19-34-34\n",
      "  done: true\n",
      "  experiment_id: 24814074aaef443f836e896277e671b3\n",
      "  experiment_tag: 0_alpha=0.001,beta=1\n",
      "  hostname: cluster2021-5\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 10.091008092716553\n",
      "  neg_mean_loss: -10.091008092716553\n",
      "  node_ip: 192.168.1.138\n",
      "  pid: 38543\n",
      "  time_since_restore: 0.03998422622680664\n",
      "  time_this_iter_s: 0.0018901824951171875\n",
      "  time_total_s: 0.03998422622680664\n",
      "  timestamp: 1642160074\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: f23b7_00000\n",
      "  \n",
      "Result for training_function_f23b7_00001:\n",
      "  date: 2022-01-14_19-34-35\n",
      "  done: false\n",
      "  experiment_id: f7d237183c694638a2c005c51daefd57\n",
      "  hostname: cluster2021-5\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.2\n",
      "  neg_mean_loss: -10.2\n",
      "  node_ip: 192.168.1.138\n",
      "  pid: 38492\n",
      "  time_since_restore: 0.004258394241333008\n",
      "  time_this_iter_s: 0.004258394241333008\n",
      "  time_total_s: 0.004258394241333008\n",
      "  timestamp: 1642160075\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f23b7_00001\n",
      "  \n",
      "Result for training_function_f23b7_00001:\n",
      "  date: 2022-01-14_19-34-35\n",
      "  done: true\n",
      "  experiment_id: f7d237183c694638a2c005c51daefd57\n",
      "  experiment_tag: 1_alpha=0.01,beta=2\n",
      "  hostname: cluster2021-5\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 10.110802775024776\n",
      "  neg_mean_loss: -10.110802775024776\n",
      "  node_ip: 192.168.1.138\n",
      "  pid: 38492\n",
      "  time_since_restore: 0.023764371871948242\n",
      "  time_this_iter_s: 0.0016367435455322266\n",
      "  time_total_s: 0.023764371871948242\n",
      "  timestamp: 1642160075\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: f23b7_00001\n",
      "  \n",
      "Result for training_function_f23b7_00002:\n",
      "  date: 2022-01-14_19-34-35\n",
      "  done: false\n",
      "  experiment_id: e4b372ac585841288a6d593beca313ea\n",
      "  hostname: cluster2021-5\n",
      "  iterations_since_restore: 1\n",
      "  mean_loss: 10.1\n",
      "  neg_mean_loss: -10.1\n",
      "  node_ip: 192.168.1.138\n",
      "  pid: 38533\n",
      "  time_since_restore: 0.004241466522216797\n",
      "  time_this_iter_s: 0.004241466522216797\n",
      "  time_total_s: 0.004241466522216797\n",
      "  timestamp: 1642160075\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f23b7_00002\n",
      "  \n",
      "Result for training_function_f23b7_00002:\n",
      "  date: 2022-01-14_19-34-35\n",
      "  done: true\n",
      "  experiment_id: e4b372ac585841288a6d593beca313ea\n",
      "  experiment_tag: 2_alpha=0.1,beta=1\n",
      "  hostname: cluster2021-5\n",
      "  iterations_since_restore: 10\n",
      "  mean_loss: 9.274311926605503\n",
      "  neg_mean_loss: -9.274311926605503\n",
      "  node_ip: 192.168.1.138\n",
      "  pid: 38533\n",
      "  time_since_restore: 0.026083707809448242\n",
      "  time_this_iter_s: 0.004421710968017578\n",
      "  time_total_s: 0.026083707809448242\n",
      "  timestamp: 1642160075\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 10\n",
      "  trial_id: f23b7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-01-14 19:34:35 (running for 00:00:01.51)<br>Memory usage on this node: 47.3/376.2 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/96 CPUs, 0/2 GPUs, 0.0/224.09 GiB heap, 0.0/100.03 GiB objects (0.0/1.0 accelerator_type:V100S)<br>Result logdir: /home/liuxinliang/ray_results/training_function_2022-01-14_19-34-33<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  alpha</th><th style=\"text-align: right;\">  beta</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  neg_mean_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>training_function_f23b7_00000</td><td>TERMINATED</td><td>192.168.1.138:38543</td><td style=\"text-align: right;\">  0.001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">10.091  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       0.0399842</td><td style=\"text-align: right;\">      -10.091  </td></tr>\n",
       "<tr><td>training_function_f23b7_00001</td><td>TERMINATED</td><td>192.168.1.138:38492</td><td style=\"text-align: right;\">  0.01 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">10.1108 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       0.0237644</td><td style=\"text-align: right;\">      -10.1108 </td></tr>\n",
       "<tr><td>training_function_f23b7_00002</td><td>TERMINATED</td><td>192.168.1.138:38533</td><td style=\"text-align: right;\">  0.1  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\"> 9.27431</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">       0.0260837</td><td style=\"text-align: right;\">       -9.27431</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-14 19:34:35,620\tINFO tune.py:626 -- Total run time: 2.49 seconds (1.50 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config:  {'alpha': 0.1, 'beta': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuxinliang/anaconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py:262: UserWarning: Dataframes will use '/' instead of '.' to delimit nested result keys in future versions of Ray. For forward compatibility, set the environment variable TUNE_RESULT_DELIM='/'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "\n",
    "def objective(step, alpha, beta):\n",
    "    return (0.1 + alpha * step / 100)**(-1) + beta * 0.1\n",
    "\n",
    "\n",
    "def training_function(config):\n",
    "    # Hyperparameters\n",
    "    alpha, beta = config[\"alpha\"], config[\"beta\"]\n",
    "    for step in range(10):\n",
    "        # Iterative training function - can be any arbitrary training procedure.\n",
    "        intermediate_score = objective(step, alpha, beta)\n",
    "        # Feed the score back back to Tune.\n",
    "        tune.report(mean_loss=intermediate_score)\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    training_function,\n",
    "    config={\n",
    "        \"alpha\": tune.grid_search([0.001, 0.01, 0.1]),\n",
    "        \"beta\": tune.choice([1, 2, 3])\n",
    "    })\n",
    "\n",
    "print(\"Best config: \", analysis.get_best_config(\n",
    "    metric=\"mean_loss\", mode=\"min\"))\n",
    "\n",
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "665aa457-8c6c-4733-a755-ed2bbad9901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "loss_train = np.array(train_l2_rec)\n",
    "loss_val = np.array(test_l2_rec)\n",
    "plt.semilogy(loss_train, label='train')\n",
    "plt.semilogy(loss_val, label='valid')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8db1518b-2823-4631-9fe7-18247c411b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918ed18371974f6abb512a30255a4b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-5, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=gamma)\n",
    "epochs = 100\n",
    "with tqdm(total=epochs) as pbar_ep:\n",
    "                            \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_l2 = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "\n",
    "            loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_l2 += loss.item()\n",
    "            \n",
    "        ############################\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                out = model(x).reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "\n",
    "                test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "        train_l2/= ntrain\n",
    "        test_l2 /= ntest\n",
    "        train_l2_rec.append(train_l2); test_l2_rec.append(test_l2)\n",
    "  \n",
    "        desc = f\"epoch: [{epoch+1}/{epochs}]\"\n",
    "        desc += f\" | current lr: {lr:.3e}\"\n",
    "        desc += f\"| train loss: {train_l2:.3e} \"\n",
    "        desc += f\"| val loss: {test_l2:.3e} \"\n",
    "        pbar_ep.set_description(desc)\n",
    "        pbar_ep.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0cc3865a-3618-4ef4-95ee-0d50f437198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "loss_train = np.array(train_l2_rec)\n",
    "loss_val = np.array(test_l2_rec)\n",
    "plt.semilogy(loss_train, label='train')\n",
    "plt.semilogy(loss_val, label='valid')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a3495da-03e9-44af-916b-6a937fe93255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502f91907fca4794b7f2ec34b22652a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2000478971749544 0.16809501552581788 0.11760571479797363\n",
      "1 2.184341622516513 0.10978863155841827 0.10487501382827759\n",
      "2 2.184962471947074 0.08811287760734558 0.07563260436058045\n",
      "3 2.1857729591429234 0.062457907021045686 0.05769967675209045\n",
      "4 2.1864749528467655 0.04941212868690491 0.046010205149650575\n",
      "5 2.189503660425544 0.043370871126651764 0.04414472758769989\n",
      "6 2.1889094039797783 0.03647813540697098 0.03606738865375519\n",
      "7 2.188121424987912 0.03103893059492111 0.03147760391235352\n",
      "8 2.1905626580119133 0.028406385868787767 0.028912636041641234\n",
      "9 2.1889375131577253 0.025635686844587325 0.03147736191749573\n",
      "10 2.191672168672085 0.02593327847123146 0.026791725754737854\n",
      "11 2.1907197646796703 0.02302770036458969 0.023858633637428284\n",
      "12 2.1924042385071516 0.020490209013223647 0.023010475635528563\n",
      "13 2.1918165925890207 0.02017829802632332 0.026213593184947967\n",
      "14 2.1943672075867653 0.021745255082845687 0.0214003923535347\n",
      "15 2.196720926091075 0.017981269150972368 0.024211947321891785\n",
      "16 2.1963488068431616 0.018428474009037018 0.02307907968759537\n",
      "17 2.195808121934533 0.016443902194499968 0.01936306744813919\n",
      "18 2.198376201093197 0.015601400643587112 0.018748098313808442\n",
      "19 2.1963324155658484 0.015429243922233582 0.020379310846328734\n",
      "20 2.1957647539675236 0.015570640742778778 0.018205908536911012\n",
      "21 2.198206016793847 0.01472911188006401 0.020535570085048676\n",
      "22 2.1966327354311943 0.01478575749695301 0.01888451486825943\n",
      "23 2.1996240112930536 0.01422644767165184 0.019370070099830626\n",
      "24 2.1968113500624895 0.015246205776929856 0.018903412222862244\n",
      "25 2.2002021446824074 0.013307017117738724 0.016380681991577148\n",
      "26 2.1985758766531944 0.01398737944662571 0.01749765604734421\n",
      "27 2.199168773368001 0.013348812878131866 0.01572180539369583\n",
      "28 2.200746603310108 0.012926973834633827 0.015936895608901977\n",
      "29 2.2000441644340754 0.012043210044503212 0.016328126192092896\n",
      "30 2.19952492415905 0.012772205591201782 0.015514287650585174\n",
      "31 2.2000389769673347 0.011678346857428551 0.015171238780021667\n",
      "32 2.2019781190901995 0.01195804938673973 0.014691772162914276\n",
      "33 2.1983626894652843 0.01200651279091835 0.01931651771068573\n",
      "34 2.2000115737318993 0.011933995813131332 0.016575047075748445\n",
      "35 2.2027852926403284 0.011604236513376236 0.015479080379009247\n",
      "36 2.199141776189208 0.011938360318541527 0.014843538999557496\n",
      "37 2.200569223612547 0.012155062854290008 0.014868634939193725\n",
      "38 2.198232289403677 0.011025070935487748 0.014002380073070525\n",
      "39 2.2024592384696007 0.01056506136059761 0.01511978656053543\n",
      "40 2.2009475007653236 0.010359279796481132 0.01387424573302269\n",
      "41 2.203441696241498 0.010486141070723534 0.013835030496120454\n",
      "42 2.2019195444881916 0.012180719181895255 0.015618373751640319\n",
      "43 2.2015433311462402 0.011570037603378295 0.01368615448474884\n",
      "44 2.203279722481966 0.010630005985498429 0.014475347399711609\n",
      "45 2.202749965712428 0.010391445100307465 0.014280562996864319\n",
      "46 2.2007372118532658 0.010710969001054765 0.01344658836722374\n",
      "47 2.203380726277828 0.010055937483906745 0.014331458806991578\n",
      "48 2.202616546303034 0.010031457588076591 0.013258315622806549\n",
      "49 2.2005518041551113 0.009885738834738731 0.014121011793613435\n",
      "50 2.203298093751073 0.0103443673402071 0.012644805014133453\n",
      "51 2.2047624550759792 0.009455108150839806 0.013464326411485672\n",
      "52 2.2027359679341316 0.009097040385007858 0.012892515063285828\n",
      "53 2.199695073068142 0.009964808344841003 0.01366099089384079\n",
      "54 2.201967952772975 0.009419632747769356 0.013106131851673126\n",
      "55 2.2001687549054623 0.008977338254451751 0.012254321426153182\n",
      "56 2.2009406331926584 0.00875797811150551 0.013362681865692139\n",
      "57 2.199439249932766 0.009074873104691506 0.01230269879102707\n",
      "58 2.2012015115469694 0.00944132249057293 0.014367322325706483\n",
      "59 2.2026836778968573 0.009741764530539513 0.013010763823986053\n",
      "60 2.201715089380741 0.010474941849708557 0.016206347644329072\n",
      "61 2.203667575493455 0.011244164645671845 0.01333118498325348\n",
      "62 2.2022492848336697 0.010811811789870262 0.012912669181823731\n",
      "63 2.2023655399680138 0.009020341545343398 0.01258462518453598\n",
      "64 2.201379932463169 0.011639065280556679 0.012370907366275788\n",
      "65 2.2030273396521807 0.009671331986784935 0.013592223972082138\n",
      "66 2.2020313143730164 0.00944955526292324 0.012233947962522506\n",
      "67 2.200588211417198 0.008448799073696136 0.012112360447645187\n",
      "68 2.2017085552215576 0.008366198062896728 0.011895933002233506\n",
      "69 2.2022140249609947 0.008425684288144112 0.013606483340263367\n",
      "70 2.20100293494761 0.010366453394293785 0.014334367215633392\n",
      "71 2.201167220249772 0.00951040194928646 0.012131774872541428\n",
      "72 2.2006130535155535 0.008343906357884408 0.01230162650346756\n",
      "73 2.199954142794013 0.008271864488720894 0.011775214523077011\n",
      "74 2.202126171439886 0.008356643497943878 0.011637235283851624\n",
      "75 2.2010183073580265 0.008873125970363617 0.011617541909217835\n",
      "76 2.213364604860544 0.009414702489972114 0.012796151340007783\n",
      "77 2.201977202668786 0.00863439853489399 0.0117401522397995\n",
      "78 2.2037091199308634 0.010196868672966958 0.015351634323596954\n",
      "79 2.202208699658513 0.010180385857820511 0.012307545989751817\n",
      "80 2.2183805368840694 0.008936531275510789 0.011679420918226243\n",
      "81 2.201956629753113 0.009210588321089744 0.011750497967004775\n",
      "82 2.2031629234552383 0.007870489209890365 0.012020339220762254\n",
      "83 2.2022090181708336 0.008557949244976043 0.011849368214607239\n",
      "84 2.200884575024247 0.008542349979281426 0.01182755708694458\n",
      "85 2.1989183891564608 0.008301268517971038 0.012458246052265167\n",
      "86 2.200588759034872 0.008417131543159486 0.01085268035531044\n",
      "87 2.2014849297702312 0.008107390612363815 0.01134063422679901\n",
      "88 2.200502745807171 0.007878140553832054 0.012533526718616485\n",
      "89 2.2002071011811495 0.008419126659631728 0.01266590043902397\n",
      "90 2.2023372761905193 0.00936423783004284 0.01298053190112114\n",
      "91 2.2035288121551275 0.009234087616205216 0.012903819680213929\n",
      "92 2.2015946600586176 0.009265271455049514 0.01166803240776062\n",
      "93 2.2046315763145685 0.007831508055329324 0.01119927316904068\n",
      "94 2.203221680596471 0.007692583948373795 0.0109325011074543\n",
      "95 2.2013049870729446 0.007777297496795654 0.012162904441356658\n",
      "96 2.2022825330495834 0.00865636357665062 0.013679929524660111\n",
      "97 2.2004763800650835 0.00808989842236042 0.011245622783899307\n",
      "98 2.202549112960696 0.007430499598383903 0.010816694051027299\n",
      "99 2.2053847648203373 0.007526771962642669 0.011467040479183198\n",
      "100 2.201769210398197 0.006638467967510223 0.010212491899728775\n",
      "101 2.202947288751602 0.006590180687606335 0.010395377576351165\n",
      "102 2.204673884436488 0.006480479799211025 0.010759896636009215\n",
      "103 2.2001196332275867 0.006201346695423126 0.01012059211730957\n",
      "104 2.202724203467369 0.006176559813320636 0.01027940034866333\n",
      "105 2.202077094465494 0.006344865098595619 0.010374054312705994\n",
      "106 2.200673094019294 0.006094022542238235 0.009988867938518524\n",
      "107 2.2001899387687445 0.005940627977252006 0.00997173547744751\n",
      "108 2.2025292105972767 0.006264746695756912 0.010415262579917907\n",
      "109 2.1994433663785458 0.00619695008546114 0.010233296304941178\n",
      "110 2.201107420027256 0.006127147272229194 0.01003223866224289\n",
      "111 2.2013934794813395 0.006003526911139488 0.010092874616384506\n",
      "112 2.2002417389303446 0.006402861461043358 0.010130967795848846\n",
      "113 2.20112038590014 0.006164882645010948 0.010015653371810913\n",
      "114 2.2007462847977877 0.006079889632761478 0.009976547062397003\n",
      "115 2.2025647833943367 0.00626422169059515 0.009961156398057938\n",
      "116 2.2009149361401796 0.005880039803683758 0.010040163099765777\n",
      "117 2.2010444961488247 0.006118709340691566 0.010196991264820099\n",
      "118 2.2017225325107574 0.006232635885477066 0.010027232617139815\n",
      "119 2.2011570539325476 0.006005680151283741 0.009902142137289047\n",
      "120 2.2001893781125546 0.006204968944191933 0.009891306608915329\n",
      "121 2.203284226357937 0.006108883909881115 0.010562883466482162\n",
      "122 2.201904132962227 0.006537795081734657 0.010282440781593323\n",
      "123 2.2016642931848764 0.006170556619763374 0.00993196502327919\n",
      "124 2.204367224127054 0.006047015197575092 0.010219515562057494\n",
      "125 2.20056045986712 0.006155373461544514 0.010128027945756912\n",
      "126 2.2037760112434626 0.006613451354205609 0.010302226841449737\n",
      "127 2.2039350010454655 0.006187481574714184 0.009998817443847657\n",
      "128 2.2041965164244175 0.006730429224669933 0.010311537086963654\n",
      "129 2.202977977693081 0.0060788365826010705 0.009815645515918731\n",
      "130 2.2014582585543394 0.006123730272054672 0.009929226040840148\n",
      "131 2.2036950420588255 0.006102594658732414 0.009920815825462342\n",
      "132 2.200990930199623 0.006541459366679192 0.010826753377914429\n",
      "133 2.203332282602787 0.006380886293947697 0.010369119048118592\n",
      "134 2.202047735452652 0.006254313282668591 0.010312363505363464\n",
      "135 2.2000268530100584 0.006507043413817882 0.01021807000041008\n",
      "136 2.2025344781577587 0.00652714666724205 0.011115280836820602\n",
      "137 2.202773654833436 0.0062030087858438495 0.009722283184528351\n",
      "138 2.2003931496292353 0.0058855202570557595 0.009634989202022553\n",
      "139 2.204006377607584 0.005819294042885304 0.009595420807600022\n",
      "140 2.2023606058210135 0.006788114227354526 0.010537602901458741\n",
      "141 2.2031464278697968 0.005994523540139198 0.009705033302307129\n",
      "142 2.204746436327696 0.005718030214309692 0.009773845672607423\n",
      "143 2.202333515509963 0.00578302288800478 0.009725706279277801\n",
      "144 2.200423227623105 0.00573347233235836 0.009634932726621628\n",
      "145 2.203724443912506 0.00597384412586689 0.009618528932332993\n",
      "146 2.2043233159929514 0.005927313551306724 0.010842909067869187\n",
      "147 2.200724896043539 0.005989376977086067 0.009523305296897887\n",
      "148 2.2013196516782045 0.005827088378369808 0.009810841977596282\n",
      "149 2.2011099122464657 0.006902950011193752 0.010762350559234619\n",
      "150 2.200443562120199 0.0062627622485160826 0.010018012523651122\n",
      "151 2.200931252911687 0.006129890345036983 0.009753951281309127\n",
      "152 2.200004294514656 0.005895423240959644 0.009505116641521453\n",
      "153 2.202121565118432 0.0057882629111409185 0.009707715511322022\n",
      "154 2.2065805420279503 0.005711423687636852 0.00964613065123558\n",
      "155 2.1993858460336924 0.005969037055969238 0.011017716526985168\n",
      "156 2.205565018579364 0.006130597889423371 0.009537721276283264\n",
      "157 2.20102671161294 0.005867669612169266 0.009676179885864257\n",
      "158 2.19912863522768 0.005652325294911861 0.009379947781562805\n",
      "159 2.200214847922325 0.00593309635668993 0.009915527403354645\n",
      "160 2.2004405222833157 0.005871628217399121 0.009429499208927154\n",
      "161 2.2009714040905237 0.006457048282027244 0.01003706842660904\n",
      "162 2.20140546746552 0.005867885567247867 0.009512584954500199\n",
      "163 2.2022500410676003 0.005995370373129845 0.00940572440624237\n",
      "164 2.200525611639023 0.005536638557910919 0.009642904549837112\n",
      "165 2.2022181879729033 0.0058712918683886525 0.009566779881715774\n",
      "166 2.2010724022984505 0.005670035801827907 0.009368515014648438\n",
      "167 2.200897566974163 0.005477762974798679 0.009316810220479966\n",
      "168 2.2035957518965006 0.005694985061883926 0.009586308896541596\n",
      "169 2.200384622439742 0.00562122106552124 0.009645964950323105\n",
      "170 2.200293743982911 0.005604361228644848 0.009978033006191253\n",
      "171 2.2018516529351473 0.006344568751752377 0.009570512175559997\n",
      "172 2.200485587120056 0.005641868025064468 0.009200815558433533\n",
      "173 2.200575079768896 0.005917221896350384 0.00980929121375084\n",
      "174 2.198892092332244 0.005581442683935165 0.009186845123767853\n",
      "175 2.2006752844899893 0.005721625454723835 0.0099052894115448\n",
      "176 2.201655864715576 0.00603139242529869 0.010080824196338654\n",
      "177 2.2025509793311357 0.006106270991265774 0.011970262676477432\n",
      "178 2.2029713299125433 0.006853476390242577 0.009711103737354279\n",
      "179 2.1971935722976923 0.006410328343510628 0.01011488914489746\n",
      "180 2.2028646133840084 0.006674450971186161 0.010145380198955535\n",
      "181 2.2046702168881893 0.006097338944673538 0.009291598945856095\n",
      "182 2.2044843956828117 0.006064519934356213 0.010702208429574967\n",
      "183 2.201413819566369 0.006346595153212548 0.009390077888965606\n",
      "184 2.20399041287601 0.005490844935178756 0.009274310916662216\n",
      "185 2.2010063137859106 0.00579643926024437 0.009213355481624603\n",
      "186 2.1986962985247374 0.005500720381736756 0.009302717298269272\n",
      "187 2.2030529230833054 0.005633553117513656 0.00935425266623497\n",
      "188 2.2012695893645287 0.005626781217753887 0.009244614541530609\n",
      "189 2.2006723154336214 0.005597508206963539 0.009171065539121628\n",
      "190 2.20215717703104 0.005295760735869408 0.009138717949390411\n",
      "191 2.2026137746870518 0.005405388295650482 0.009309234470129013\n",
      "192 2.202411064878106 0.005801669768989087 0.00974219799041748\n",
      "193 2.203365035355091 0.005879578240215779 0.00957211509346962\n",
      "194 2.2016104832291603 0.005835719399154186 0.010212233662605286\n",
      "195 2.202571179717779 0.006021677576005459 0.01009203851222992\n",
      "196 2.2039111014455557 0.005801168501377105 0.0093766587972641\n",
      "197 2.201856169849634 0.006161742568016052 0.009806048721075059\n",
      "198 2.202043743804097 0.005833186492323875 0.00982480749487877\n",
      "199 2.202419748529792 0.006186041884124279 0.009296195060014724\n",
      "200 2.20050791464746 0.005073561318218708 0.009116534292697907\n",
      "201 2.2014631535857916 0.004872771546244621 0.00895520269870758\n",
      "202 2.2030448131263256 0.004771397434175015 0.008921376913785934\n",
      "203 2.204995170235634 0.004772452548146248 0.00904685616493225\n",
      "204 2.2032776959240437 0.0047632577195763585 0.008886766880750656\n",
      "205 2.201857104897499 0.004739807762205601 0.008881921917200088\n",
      "206 2.20373722165823 0.004737263537943363 0.008949642777442932\n",
      "207 2.204452680423856 0.004712764620780945 0.008918980360031128\n",
      "208 2.2035774178802967 0.004693703800439835 0.008924102634191513\n",
      "209 2.20259465649724 0.004734091937541962 0.008873552232980728\n",
      "210 2.2035497035831213 0.004703596390783786 0.008806488364934922\n",
      "211 2.203019505366683 0.004653368823230267 0.008794891983270645\n",
      "212 2.2045781556516886 0.004724664263427257 0.00882490947842598\n",
      "213 2.203190268948674 0.004691652208566666 0.00883415549993515\n",
      "214 2.2025114819407463 0.004691732615232468 0.00887488767504692\n",
      "215 2.203813459724188 0.004718357227742672 0.008790684938430786\n",
      "216 2.203161744400859 0.004718155324459076 0.0088092602789402\n",
      "217 2.20278193987906 0.004718163162469864 0.008814472556114197\n",
      "218 2.204199969768524 0.004784728869795799 0.008864886015653611\n",
      "219 2.205327505245805 0.004728338763117791 0.00875612735748291\n",
      "220 2.200805000960827 0.004712113678455353 0.008826363980770111\n",
      "221 2.203758740797639 0.004719609566032887 0.008821993470191955\n",
      "222 2.2049720864742994 0.004676051639020443 0.008759352564811706\n",
      "223 2.204873126000166 0.0047589405998587605 0.009081048965454102\n",
      "224 2.2013687174767256 0.004895787961781025 0.008969336450099945\n",
      "225 2.2025528624653816 0.004759184807538986 0.00878535896539688\n",
      "226 2.203080866485834 0.004676639288663864 0.008769845366477966\n",
      "227 2.2026050947606564 0.004710508912801743 0.008897782415151595\n",
      "228 2.2009276654571295 0.005095335580408573 0.008956470042467118\n",
      "229 2.2030761893838644 0.005180670969188213 0.00872223436832428\n",
      "230 2.203354850411415 0.0047767583206295965 0.008795371055603027\n",
      "231 2.2020925413817167 0.004730146139860153 0.00872698038816452\n",
      "232 2.2003015000373125 0.004632541999220848 0.008682125508785247\n",
      "233 2.202807491645217 0.004641724295914173 0.008800711035728454\n",
      "234 2.2025801334530115 0.004625479646027088 0.008704150319099426\n",
      "235 2.20269837975502 0.004758703015744686 0.008996778577566146\n",
      "236 2.2051817402243614 0.004791046112775803 0.008805130273103715\n",
      "237 2.1992066763341427 0.004857122078537941 0.008864011168479919\n",
      "238 2.2016922030597925 0.004662800781428814 0.00877747431397438\n",
      "239 2.202580964192748 0.004674440823495388 0.008751031160354614\n",
      "240 2.1991164535284042 0.004666876748204232 0.008728013336658479\n",
      "241 2.2042377181351185 0.004864695757627487 0.008992723524570465\n",
      "242 2.2028065975755453 0.0047092726305127145 0.008757997453212738\n",
      "243 2.2013881783932447 0.004721580974757672 0.008692824244499207\n",
      "244 2.203593974933028 0.004675437055528164 0.008745327144861221\n",
      "245 2.2029570546001196 0.0046318163871765135 0.008779833316802979\n",
      "246 2.2029733546078205 0.0050415356233716015 0.008757095485925674\n",
      "247 2.202383002266288 0.004940998554229736 0.008855509907007217\n",
      "248 2.204059971496463 0.005120566047728062 0.009385168254375457\n",
      "249 2.202158475294709 0.005009629637002945 0.008884994387626648\n",
      "250 2.201961377635598 0.004652208045125008 0.008677470088005066\n",
      "251 2.2039974201470613 0.00469154591858387 0.008749057203531266\n",
      "252 2.2016133964061737 0.0047766070142388345 0.008646130114793777\n",
      "253 2.2021914701908827 0.004740610785782337 0.00887623056769371\n",
      "254 2.1998717673122883 0.004791509330272675 0.008763070255517959\n",
      "255 2.203934410586953 0.004631701841950417 0.008602863252162933\n",
      "256 2.203859696164727 0.004602180570363998 0.008993191123008728\n",
      "257 2.2014923002570868 0.004649591825902462 0.00862806722521782\n",
      "258 2.203695809468627 0.004642003312706948 0.008692687749862671\n",
      "259 2.201974496245384 0.0048557248711586 0.008795601278543473\n",
      "260 2.2033742927014828 0.004752435319125652 0.009025066494941711\n",
      "261 2.202524935826659 0.0048455529958009716 0.0086843079328537\n",
      "262 2.2116376478224993 0.004781521774828434 0.008749493807554245\n",
      "263 2.2040492575615644 0.004588054522871971 0.008770439326763153\n",
      "264 2.216737598180771 0.004561855763196945 0.008638595938682556\n",
      "265 2.2020990177989006 0.0045232224017381665 0.008699278086423875\n",
      "266 2.2089936584234238 0.004495827160775662 0.008625054508447647\n",
      "267 2.205291310325265 0.004731953993439674 0.008609486669301987\n",
      "268 2.1977369748055935 0.004565885432064533 0.008531063795089722\n",
      "269 2.1947220265865326 0.004533042028546333 0.008542591631412506\n",
      "270 2.195371687412262 0.0049391546696424486 0.008696611523628234\n",
      "271 2.1939355358481407 0.004683366805315017 0.00864037573337555\n",
      "272 2.1944685392081738 0.004615823112428188 0.008582064807415008\n",
      "273 2.2006247770041227 0.004557059697806835 0.008596458286046983\n",
      "274 2.202393863350153 0.0045414136499166485 0.00862207755446434\n",
      "275 2.1979042254388332 0.004924517035484314 0.008610332012176513\n",
      "276 2.2005298510193825 0.004611132889986038 0.008644892275333405\n",
      "277 2.2000373117625713 0.004767359174787998 0.008617927879095077\n",
      "278 2.206610267981887 0.004610904239118099 0.008605281412601471\n",
      "279 2.2059821356087923 0.0045219317302107815 0.00855441614985466\n",
      "280 2.2059291917830706 0.004437828801572323 0.00853821039199829\n",
      "281 2.203906996175647 0.004768884427845478 0.009594702571630477\n",
      "282 2.2045247238129377 0.005203775480389595 0.008910713791847229\n",
      "283 2.206217125058174 0.0046774794086813926 0.008561426401138305\n",
      "284 2.2011934872716665 0.004577695935964584 0.008784625083208084\n",
      "285 2.204703437164426 0.004641123294830322 0.00862277165055275\n",
      "286 2.205011712387204 0.004757147297263145 0.0086813984811306\n",
      "287 2.206807928159833 0.00465489399433136 0.008733205944299698\n",
      "288 2.201278129592538 0.004494438476860523 0.008502586781978606\n",
      "289 2.2043077908456326 0.004431238785386086 0.008513573557138443\n",
      "290 2.2038966678082943 0.004488376684486866 0.008484841883182525\n",
      "291 2.203804736956954 0.004583235248923302 0.008596008121967315\n",
      "292 2.2032969892024994 0.004706350155174732 0.0085835562646389\n",
      "293 2.202954150736332 0.004509229809045791 0.008563984632492066\n",
      "294 2.2056376542896032 0.004455366343259812 0.008529987186193466\n",
      "295 2.2049886099994183 0.004414246521890163 0.008550869971513749\n",
      "296 2.2040711697191 0.00445768740028143 0.008537014722824096\n",
      "297 2.2053881492465734 0.004510080344974994 0.008805351108312607\n",
      "298 2.2021976467221975 0.004772920601069927 0.008588250875473023\n",
      "299 2.198442468419671 0.004549997702240944 0.008563674390316009\n",
      "300 2.197035029530525 0.004271326169371605 0.008443815410137176\n",
      "301 2.1986323464661837 0.004229318931698799 0.008443028181791306\n",
      "302 2.2004714850336313 0.0041891299486160275 0.00844028040766716\n",
      "303 2.195879101753235 0.004177963763475418 0.00844448134303093\n",
      "304 2.199299294501543 0.004188420899212361 0.00846372202038765\n",
      "305 2.196629237383604 0.004166875176131725 0.008391505777835847\n",
      "306 2.1984021328389645 0.004181808449327946 0.008432856351137162\n",
      "307 2.1981246806681156 0.004178287200629711 0.008408517241477967\n",
      "308 2.1963076069951057 0.004175056733191013 0.008472316414117814\n",
      "309 2.200195012614131 0.004170642554759979 0.008426867127418518\n",
      "310 2.1966799795627594 0.0041562547534704205 0.00842928260564804\n",
      "311 2.19815537892282 0.004178302995860576 0.008379206508398056\n",
      "312 2.1987194437533617 0.004162421680986881 0.008368908017873765\n",
      "313 2.199618024751544 0.004180046901106835 0.008392558246850968\n",
      "314 2.2027268446981907 0.004208101727068424 0.008342894166707993\n",
      "315 2.2049496062099934 0.004177265010774135 0.008424333930015564\n",
      "316 2.204598566517234 0.004179493218660355 0.008392152041196824\n",
      "317 2.2031168434768915 0.004161359660327434 0.008384326696395874\n",
      "318 2.202047247439623 0.004180888958275318 0.008518899828195571\n",
      "319 2.2045568618923426 0.004193311028182507 0.00840743511915207\n",
      "320 2.2048368770629168 0.004199455223977566 0.008472235798835754\n",
      "321 2.2012445852160454 0.00418835997581482 0.008407751023769379\n",
      "322 2.2034968324005604 0.004260521560907364 0.008393775522708893\n",
      "323 2.2036472763866186 0.004201807789504528 0.008502108752727508\n",
      "324 2.205231385305524 0.004188858352601528 0.008390682190656662\n",
      "325 2.202543018385768 0.004160762868821621 0.008396194726228714\n",
      "326 2.2014526668936014 0.00415525196492672 0.008423395156860351\n",
      "327 2.2012457381933928 0.004163624979555607 0.00841717302799225\n",
      "328 2.2021998539566994 0.004154493354260921 0.008382645696401595\n",
      "329 2.2026524525135756 0.004150799132883549 0.00837660238146782\n",
      "330 2.2045191694051027 0.0041593312025070195 0.008380316644906998\n",
      "331 2.2040129490196705 0.00413174519687891 0.008379550278186798\n",
      "332 2.202684970572591 0.0041755494549870495 0.008345510363578796\n",
      "333 2.205581210553646 0.004184107325971126 0.0083652463555336\n",
      "334 2.203576732426882 0.004179011158645153 0.008393428325653075\n",
      "335 2.2015138529241085 0.004164879508316517 0.008353022634983063\n",
      "336 2.203195648267865 0.004185909532010555 0.008317140191793441\n",
      "337 2.2018344029784203 0.004129976645112038 0.008244518488645554\n",
      "338 2.2046592701226473 0.0041390153765678405 0.008330173790454865\n",
      "339 2.2044978979974985 0.00414355705678463 0.008312645256519318\n",
      "340 2.2042573019862175 0.004259950563311577 0.00843481495976448\n",
      "341 2.201775101944804 0.004131795868277549 0.0083842995762825\n",
      "342 2.2097797878086567 0.004156974919140339 0.00834653079509735\n",
      "343 2.2019737362861633 0.0041757005155086515 0.008391591012477875\n",
      "344 2.2016626931726933 0.0041248142570257185 0.008286742269992828\n",
      "345 2.2018410712480545 0.004138597138226032 0.008343591541051864\n",
      "346 2.2026172187179327 0.004125055156648159 0.008370131403207779\n",
      "347 2.202719047665596 0.004117329262197018 0.00831481248140335\n",
      "348 2.2007271107286215 0.004137521043419838 0.008423721194267273\n",
      "349 2.201614184305072 0.004134850412607193 0.008375787138938905\n",
      "350 2.202784840017557 0.004116341516375542 0.008319347202777862\n",
      "351 2.2070557922124863 0.004138698250055313 0.008400335013866424\n",
      "352 2.2029719948768616 0.00419193708896637 0.008304110914468765\n",
      "353 2.198864597827196 0.004137073338031769 0.008367960602045059\n",
      "354 2.2033132910728455 0.004182447321712971 0.008302859961986542\n",
      "355 2.19980807043612 0.004144368611276149 0.008375697135925293\n",
      "356 2.2034975606948137 0.004138568669557571 0.008367471843957901\n",
      "357 2.205926064401865 0.004077239789068699 0.008277552872896195\n",
      "358 2.2016027234494686 0.0041923198103904725 0.008423299044370652\n",
      "359 2.2059333734214306 0.004181001737713814 0.008324898779392242\n",
      "360 2.200559763237834 0.004155691049993038 0.00833558827638626\n",
      "361 2.203178908675909 0.0041653415337204935 0.00833353504538536\n",
      "362 2.2048106975853443 0.004123874977231026 0.008322941958904267\n",
      "363 2.20447307638824 0.004098756402730942 0.00832555815577507\n",
      "364 2.2032341938465834 0.004091374434530735 0.008319317102432252\n",
      "365 2.2038541231304407 0.004066244810819626 0.00829470083117485\n",
      "366 2.2040077093988657 0.004048884831368923 0.008312627822160721\n",
      "367 2.202467694878578 0.004075356610119343 0.008294616788625718\n",
      "368 2.202620880678296 0.004160606428980827 0.008332726955413818\n",
      "369 2.2046141047030687 0.004189222924411297 0.00826097920536995\n",
      "370 2.20350837148726 0.004158891662955284 0.008297873735427857\n",
      "371 2.2012318782508373 0.00418495973944664 0.008291866928339005\n",
      "372 2.203675616532564 0.004121469281613827 0.008276594579219818\n",
      "373 2.2056096754968166 0.004115906842052937 0.008248756974935531\n",
      "374 2.2067240308970213 0.004095834046602249 0.008349033147096634\n",
      "375 2.2029518336057663 0.004095301926136017 0.00836264356970787\n",
      "376 2.2042744420468807 0.004152683265507221 0.008320540487766266\n",
      "377 2.2033410724252462 0.00406664277613163 0.008247273862361909\n",
      "378 2.2017865628004074 0.004103831380605698 0.008290886133909225\n",
      "379 2.2013352420181036 0.004076084561645985 0.008278412520885467\n",
      "380 2.203637657687068 0.004100102603435516 0.008228798359632493\n",
      "381 2.204680198803544 0.004187763839960099 0.0083084237575531\n",
      "382 2.2030310910195112 0.004302004598081112 0.008337942212820052\n",
      "383 2.203336976468563 0.004168260112404823 0.00839788556098938\n",
      "384 2.203184200450778 0.004114049971103668 0.008366170078516007\n",
      "385 2.2036681957542896 0.004062007032334804 0.008257712125778199\n",
      "386 2.2041349951177835 0.00409187899529934 0.008295793235301972\n",
      "387 2.199145559221506 0.0040672805309295654 0.00826699212193489\n",
      "388 2.197068467736244 0.0040400934889912605 0.0082359117269516\n",
      "389 2.196683567017317 0.004037588387727737 0.008388483226299285\n",
      "390 2.1992223393172026 0.0040616757795214654 0.008299140483140946\n",
      "391 2.2036583982408047 0.004103737592697144 0.008436791151762008\n",
      "392 2.200212372466922 0.004236320480704308 0.008273180574178696\n",
      "393 2.202026881277561 0.004068377017974853 0.008295858204364777\n",
      "394 2.204448727890849 0.004058368347585202 0.008225522488355636\n",
      "395 2.201332852244377 0.004063955463469028 0.00822592169046402\n",
      "396 2.202171292155981 0.004081007435917854 0.008208656311035156\n",
      "397 2.203186323866248 0.004024736680090428 0.008228717744350434\n",
      "398 2.20229434967041 0.003993756845593452 0.008201818168163299\n",
      "399 2.201430333778262 0.004096757255494595 0.008266265839338302\n",
      "400 2.203765209764242 0.003960565611720085 0.00825152114033699\n",
      "401 2.202260285615921 0.003921616263687611 0.008216426074504853\n",
      "402 2.2015313785523176 0.003908218815922737 0.008177863210439682\n",
      "403 2.2029797844588757 0.003909712791442871 0.008230149149894714\n",
      "404 2.2041610404849052 0.0039007781744003294 0.008194401413202285\n",
      "405 2.201220916584134 0.0039005673825740815 0.008204651921987533\n",
      "406 2.2009126748889685 0.003902516707777977 0.00818715661764145\n",
      "407 2.2037559617310762 0.0039010511711239813 0.008196271061897277\n",
      "408 2.202855782583356 0.0039025372341275214 0.008181798458099365\n",
      "409 2.2036675922572613 0.0038990197330713273 0.008207421749830246\n",
      "410 2.2023857720196247 0.003905326448380947 0.008172539919614792\n",
      "411 2.2018054872751236 0.003902833364903927 0.008207998722791671\n",
      "412 2.203294586390257 0.003904353253543377 0.00818690836429596\n",
      "413 2.201566280797124 0.003895207919180393 0.008196477890014648\n",
      "414 2.196881426498294 0.0038994250670075416 0.008169726729393005\n",
      "415 2.2004677411168814 0.0039058361053466795 0.008155534267425537\n",
      "416 2.1980430334806442 0.0039051667749881746 0.008161346167325974\n",
      "417 2.198391927406192 0.0038965833857655527 0.00818242684006691\n",
      "418 2.2116872798651457 0.003903630517423153 0.00817561313509941\n",
      "419 2.2086929976940155 0.0038936000168323516 0.008171215653419495\n",
      "420 2.2008826211094856 0.00391315970569849 0.008137544095516204\n",
      "421 2.199569223448634 0.0038932752311229704 0.008184215724468232\n",
      "422 2.198142061010003 0.0039054913371801376 0.008184840530157089\n",
      "423 2.2027563359588385 0.003886471450328827 0.008164967149496079\n",
      "424 2.1929750982671976 0.003896459773182869 0.008193899840116502\n",
      "425 2.1924438644200563 0.003927443139255047 0.008279312402009964\n",
      "426 2.195558536797762 0.0039247612729668615 0.008148747384548188\n",
      "427 2.1969429925084114 0.0038915135785937307 0.00818756252527237\n",
      "428 2.1987414341419935 0.003888731151819229 0.008197880536317825\n",
      "429 2.198755444958806 0.003883900597691536 0.00820455014705658\n",
      "430 2.198713904246688 0.00390922711789608 0.008211214393377304\n",
      "431 2.1967292614281178 0.003897541083395481 0.00816423624753952\n",
      "432 2.2017131503671408 0.0038844067379832266 0.008151027113199235\n",
      "433 2.20325755700469 0.0038888450562953947 0.00816119059920311\n",
      "434 2.200852630659938 0.0039042746126651765 0.008146464824676514\n",
      "435 2.1999129578471184 0.003917957931756973 0.008179825991392135\n",
      "436 2.1998017858713865 0.0038970595449209213 0.008156159967184067\n",
      "437 2.2001366894692183 0.0039012588039040565 0.008239840269088744\n",
      "438 2.199146533384919 0.0038990354165434836 0.008184582591056824\n",
      "439 2.1990533005446196 0.003875945694744587 0.008157974779605866\n",
      "440 2.199044104665518 0.003895575501024723 0.008221518993377686\n",
      "441 2.201933017000556 0.003889212913811207 0.008194160908460616\n",
      "442 2.1984460074454546 0.003947972021996975 0.008197359442710876\n",
      "443 2.1994628570973873 0.0039038686528801918 0.008173417299985886\n",
      "444 2.2002281341701746 0.0038928089439868928 0.008140062689781189\n",
      "445 2.200041852891445 0.0038673310205340383 0.00815640926361084\n",
      "446 2.19848127476871 0.003866364523768425 0.008123044520616532\n",
      "447 2.1997302509844303 0.0038722802251577376 0.008143869340419769\n",
      "448 2.196758158504963 0.003888388752937317 0.008142913281917573\n",
      "449 2.196397203952074 0.0038712069764733313 0.008150001764297485\n",
      "450 2.194253485649824 0.003868069089949131 0.008118448853492737\n",
      "451 2.1915133874863386 0.003867250956594944 0.008104593902826309\n",
      "452 2.1927473545074463 0.00387200927734375 0.008159685283899307\n",
      "453 2.1915146354585886 0.0038825501054525376 0.008130595833063126\n",
      "454 2.1925321258604527 0.0038638154044747354 0.008146992027759552\n",
      "455 2.193311968818307 0.0038606119230389594 0.008134219199419021\n",
      "456 2.1907762456685305 0.003865350104868412 0.008126886337995529\n",
      "457 2.1920787170529366 0.0038661823868751527 0.00810650959610939\n",
      "458 2.1912777218967676 0.003884935535490513 0.00814265474677086\n",
      "459 2.1966589093208313 0.0038656965419650077 0.008170821368694306\n",
      "460 2.1908131781965494 0.0038856264054775236 0.00814820870757103\n",
      "461 2.1905570346862078 0.0038589296638965606 0.008104405999183655\n",
      "462 2.193040855228901 0.003877898618578911 0.00816398173570633\n",
      "463 2.191598678007722 0.0038585940301418304 0.008127950131893158\n",
      "464 2.193320544436574 0.003871010676026344 0.008131627887487412\n",
      "465 2.192832291126251 0.003909208685159683 0.008186206072568893\n",
      "466 2.194072339683771 0.0038778092488646505 0.008182572573423386\n",
      "467 2.191680261865258 0.0038604650720953943 0.008126609176397324\n",
      "468 2.1917434837669134 0.0038380110785365103 0.008102722465991974\n",
      "469 2.1919196769595146 0.003845829948782921 0.008131497800350189\n",
      "470 2.193940229713917 0.003858662322163582 0.00814596489071846\n",
      "471 2.1969836428761482 0.0038624765649437906 0.008097218424081802\n",
      "472 2.198889112100005 0.0038484763875603675 0.008088202327489852\n",
      "473 2.197174772620201 0.0038494771271944048 0.008158377707004547\n",
      "474 2.1991006303578615 0.0038917848765850065 0.008111715018749237\n",
      "475 2.196335470303893 0.003872554585337639 0.008094232678413391\n",
      "476 2.203088028356433 0.003850202053785324 0.008145466595888138\n",
      "477 2.200069172307849 0.003845000833272934 0.00809739738702774\n",
      "478 2.198135197162628 0.0038485252559185027 0.008113997876644135\n",
      "479 2.1968393698334694 0.003834512084722519 0.008141872882843017\n",
      "480 2.197865368798375 0.003843744695186615 0.008122340440750123\n",
      "481 2.1967574656009674 0.0038407535180449487 0.008138168603181839\n",
      "482 2.199873488396406 0.0038311251774430276 0.008124962151050567\n",
      "483 2.1978522557765245 0.003827280692756176 0.00811738297343254\n",
      "484 2.1986337266862392 0.0038629449307918547 0.008122068494558335\n",
      "485 2.2002142686396837 0.003862091809511185 0.008141998797655106\n",
      "486 2.19824893027544 0.003841907076537609 0.008133576065301896\n",
      "487 2.199796037748456 0.003870764672756195 0.008095938116312026\n",
      "488 2.1964040119200945 0.0038314984366297723 0.008132549673318864\n",
      "489 2.2048110477626324 0.003813391625881195 0.008108020573854447\n",
      "490 2.1984166875481606 0.003827844589948654 0.008088472932577133\n",
      "491 2.213882466778159 0.003848033018410206 0.008121013045310974\n",
      "492 2.1977907568216324 0.0038204491212964057 0.008091870695352554\n",
      "493 2.1949372235685587 0.0038060528337955477 0.00812213569879532\n",
      "494 2.199806382879615 0.003820866271853447 0.008111985176801681\n",
      "495 2.1979171857237816 0.003830581910908222 0.008112940788269043\n",
      "496 2.198146503418684 0.003822204053401947 0.008068505972623825\n",
      "497 2.1976218819618225 0.0038191818222403527 0.008092998266220093\n",
      "498 2.198207540437579 0.0038251656293869017 0.008098063915967941\n",
      "499 2.1974942721426487 0.0038252536207437514 0.008125755339860916\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "model = FNO2d(10, 10, 32).to(device)\n",
    "print(count_params(model))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "#                        div_factor=1e4, \n",
    "#                        final_div_factor=1e4,\n",
    "#                        pct_start=0.3,\n",
    "#                        steps_per_epoch=len(train_loader)*5, \n",
    "#                        epochs=100)\n",
    "\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_normalizer.cuda(device)\n",
    "\n",
    "with tqdm(total=epochs) as pbar_ep:\n",
    "                            \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_l2 = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "    #         y = y_normalizer.decode(y)\n",
    "\n",
    "            loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_l2 += loss.item()\n",
    "            \n",
    "        ############################\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        desc = f\"epoch: [{epoch+1}/{epochs}]\"\n",
    "        desc += f\" | current lr: {lr:.3e}\"\n",
    "        pbar_ep.set_description(desc)\n",
    "        pbar_ep.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                out = model(x).reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "\n",
    "                test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "        train_l2/= ntrain\n",
    "        test_l2 /= ntest\n",
    "\n",
    "        t2 = default_timer()\n",
    "        print(epoch, t2-t1, train_l2, test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b42f9e-c35d-45a6-aa5b-2a7921928148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae80e7a82f13409796e05f84a57f2011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.918366344645619 0.009056274086236953 0.008617604970932008\n",
      "1 4.928460575640202 0.00500779564678669 0.008468602001667023\n",
      "2 4.9300401750952005 0.004564413249492645 0.008360137343406676\n",
      "3 4.9281385857611895 0.00450154996663332 0.008593452274799346\n",
      "4 4.924270730465651 0.00499767529964447 0.009178255498409272\n",
      "5 4.923990942537785 0.005724080041050911 0.008358342945575714\n",
      "6 4.9266063664108515 0.005046013318002224 0.008514917194843293\n",
      "7 4.925262816250324 0.004867603905498981 0.008198720663785934\n",
      "8 4.920864265412092 0.005174784913659095 0.008254533857107163\n",
      "9 4.913515890017152 0.004964656621217728 0.008644744008779525\n",
      "10 4.9328363090753555 0.005462375804781914 0.00828424796462059\n",
      "11 4.933052422478795 0.0048614168912172314 0.008169989734888077\n",
      "12 4.933904567733407 0.005888080328702926 0.008661424219608306\n",
      "13 4.93487804569304 0.005472562044858933 0.008625458329916\n",
      "14 4.935931822285056 0.005552706815302372 0.008440094143152237\n",
      "15 4.933305909857154 0.005103006221354007 0.008465375453233719\n",
      "16 4.936167607083917 0.004942493945360183 0.008326175808906554\n",
      "17 4.937106791883707 0.004747036255896091 0.00804888218641281\n",
      "18 4.93601761572063 0.004751393914222717 0.008836477994918823\n",
      "19 4.938540523871779 0.0060909283384680745 0.008656562566757202\n",
      "20 4.938748612999916 0.005871943920850753 0.008530775606632233\n",
      "21 4.943419616669416 0.0051968424916267394 0.008337021172046662\n",
      "22 4.944688498973846 0.005194734908640385 0.008711311966180801\n",
      "23 4.93972834944725 0.005757382467389107 0.008353491574525833\n",
      "24 4.938447730615735 0.005012354984879494 0.008808583170175553\n",
      "25 4.937927834689617 0.005436412990093231 0.009126791954040528\n",
      "26 4.938730077818036 0.005530905082821846 0.009071991890668868\n",
      "27 4.9384480603039265 0.005404198624193669 0.008730356395244599\n",
      "28 4.93894237279892 0.004978611469268799 0.008043754249811172\n",
      "29 4.937468912452459 0.0048667735308408734 0.008153461664915086\n",
      "30 4.937245722860098 0.004679376319050789 0.00816828727722168\n",
      "31 4.940673284232616 0.005211997076869011 0.00868791863322258\n",
      "32 4.940180854871869 0.004987877741456032 0.008133656829595565\n",
      "33 4.940317628905177 0.004861532457172871 0.00836560919880867\n",
      "34 4.940222725272179 0.005022564470767975 0.008479880541563034\n",
      "35 4.939244220033288 0.005107859291136265 0.008223986029624939\n",
      "36 4.940427802503109 0.005192163027822971 0.008960759490728379\n",
      "37 4.936070265248418 0.005465095929801464 0.008274443596601486\n",
      "38 4.937890876084566 0.005031841307878494 0.008549448996782304\n",
      "39 4.940582355484366 0.004824514456093311 0.0085567145049572\n",
      "40 4.945553045719862 0.005249010168015957 0.008260620534420013\n",
      "41 4.9478520434349775 0.005286949172616005 0.008333480060100555\n",
      "42 4.942636804655194 0.004969340443611145 0.008248812854290008\n",
      "43 4.938001507893205 0.004960915029048919 0.008349128067493439\n",
      "44 4.940116161480546 0.0048498277142643926 0.00883449375629425\n",
      "45 4.940872183069587 0.004952618561685085 0.008617058098316192\n",
      "46 4.940271010622382 0.005468872636556626 0.008469270169734954\n",
      "47 4.94018823094666 0.005586233958601952 0.008241183310747146\n",
      "48 4.941230829805136 0.005816254325211048 0.008612944781780242\n",
      "49 4.939422043040395 0.005082035951316357 0.008415016829967498\n",
      "50 4.940143136307597 0.005018517933785915 0.009300858080387116\n",
      "51 4.941123586148024 0.005378130853176117 0.008485556989908218\n",
      "52 4.939960051327944 0.004954076528549194 0.008348729312419891\n",
      "53 4.940937636420131 0.005213285952806472 0.008110744953155518\n",
      "54 4.939573233947158 0.00524416321516037 0.008057798892259598\n",
      "55 4.9398391749709845 0.005202964305877685 0.007916735410690308\n",
      "56 4.941081678494811 0.00507565575838089 0.00824959397315979\n",
      "57 4.939976904541254 0.004990370713174343 0.00863239422440529\n",
      "58 4.940271424129605 0.005135113000869751 0.00827794373035431\n",
      "59 4.939245563000441 0.004914793394505978 0.008749879449605941\n",
      "60 4.938196642324328 0.004574766166508198 0.00862786829471588\n",
      "61 4.940435918048024 0.005368951104581356 0.008490643054246903\n",
      "62 4.940006587654352 0.005334071516990661 0.008780336678028107\n",
      "63 4.940262444317341 0.005738588958978653 0.008222468793392182\n",
      "64 4.939991375431418 0.005412913493812084 0.007944392114877701\n",
      "65 4.941762149333954 0.004850539997220039 0.0079351095110178\n",
      "66 4.952775627374649 0.005037279710173607 0.008313188254833221\n",
      "67 4.93769078142941 0.00489894787967205 0.008517146557569504\n",
      "68 4.9378338642418385 0.005395349964499473 0.008955998569726944\n",
      "69 4.934910988435149 0.005214319422841072 0.008382085263729095\n",
      "70 4.939135417342186 0.004875553339719773 0.007920855432748794\n",
      "71 4.936915995553136 0.005355980671942234 0.008228293210268021\n",
      "72 4.941818289458752 0.005165462277829647 0.008644042611122131\n",
      "73 4.937542947009206 0.004783873587846756 0.00820103794336319\n",
      "74 4.935812685638666 0.005329876735806465 0.008381300419569016\n",
      "75 4.936133269220591 0.005013956874608994 0.008001307845115662\n",
      "76 4.93737618252635 0.005040423817932606 0.008592247366905212\n",
      "77 4.937461841851473 0.004672597624361515 0.007887579947710038\n",
      "78 4.935664780437946 0.005221594370901585 0.009624045789241791\n",
      "79 4.93612303212285 0.004958058342337608 0.007942154258489608\n",
      "80 4.9338660556823015 0.004737505361437798 0.00901531159877777\n",
      "81 4.935543606057763 0.00514396969974041 0.00918137550354004\n",
      "82 4.932993991300464 0.004965250588953495 0.008257229179143906\n",
      "83 4.932784849777818 0.005111668065190316 0.008535599559545517\n",
      "84 4.936688706278801 0.005200741343200207 0.008991173803806304\n",
      "85 4.935927033424377 0.005118866495788098 0.008271872848272323\n",
      "86 4.93625408038497 0.004783484384417534 0.00811093881726265\n",
      "87 4.938581423833966 0.004874961346387863 0.008026804774999619\n",
      "88 4.934924570843577 0.004801600597798824 0.008162069022655488\n",
      "89 4.934573415666819 0.0048682702705264095 0.008873651027679444\n",
      "90 4.935222344473004 0.004726963192224503 0.008369522243738175\n",
      "91 4.935583915561438 0.004928972758352757 0.008320356756448745\n",
      "92 4.9381098523736 0.004783593483269215 0.008183989375829696\n",
      "93 4.935875121504068 0.004600173979997635 0.008057983964681626\n",
      "94 4.936039503663778 0.0048034868240356444 0.008115506619215012\n",
      "95 4.938040319830179 0.004877766340970993 0.009094015657901765\n",
      "96 4.934846434742212 0.0051177796423435215 0.008074839115142823\n",
      "97 4.933844866231084 0.004522476732730866 0.008221855610609055\n",
      "98 4.953829118981957 0.004920690231025219 0.008073024153709411\n",
      "99 4.9343110248446465 0.005253450833261013 0.008224777430295944\n",
      "100 4.934379879385233 0.003981113851070404 0.007592229247093201\n",
      "101 4.934304067865014 0.0035800243094563484 0.0074559634923934935\n",
      "102 4.93475703522563 0.0034940350130200385 0.007518125399947166\n",
      "103 4.935076359659433 0.003344794623553753 0.007407158240675926\n",
      "104 4.931900188326836 0.0033035942502319814 0.007246937900781632\n",
      "105 4.9354209173470736 0.0032660151682794092 0.00722768634557724\n",
      "106 4.934175534173846 0.0033027699440717696 0.007354470491409301\n",
      "107 4.935660637915134 0.003351225435733795 0.0073282158374786376\n",
      "108 4.934855759143829 0.003288488510996103 0.007194283008575439\n",
      "109 4.935591695830226 0.00337228025496006 0.007330643087625504\n",
      "110 4.936289303004742 0.0033474426120519638 0.008111196458339692\n",
      "111 4.93539971485734 0.0033955023773014547 0.007384120523929596\n",
      "112 4.936025647446513 0.0032800636477768423 0.007254183515906334\n",
      "113 4.935029990971088 0.0034577663466334343 0.007664743661880493\n",
      "114 4.932009268552065 0.003434316523373127 0.007482006847858429\n",
      "115 4.935160236433148 0.00337790072709322 0.007257091775536537\n",
      "116 4.937204077839851 0.0033464583568274973 0.00747880682349205\n",
      "117 4.932741174474359 0.0035331278964877127 0.007404531314969063\n",
      "118 4.936368145048618 0.0032960924617946147 0.007243980169296265\n",
      "119 4.938636405393481 0.0033908736631274223 0.007300251200795174\n",
      "120 4.935925032943487 0.003743069238960743 0.00793632835149765\n",
      "121 4.93503562361002 0.003869268298149109 0.007446922510862351\n",
      "122 4.935945451259613 0.0033807860389351843 0.007322025150060653\n",
      "123 4.935452468693256 0.0035754164904356 0.007203870564699173\n",
      "124 4.935964819043875 0.003364014737308025 0.007569309920072555\n",
      "125 4.935575354844332 0.003354885369539261 0.00734418511390686\n",
      "126 4.935842270031571 0.003375978346914053 0.007465491592884063\n",
      "127 4.9318726267665625 0.0033015683963894846 0.00739078938961029\n",
      "128 4.932993965223432 0.003338299084454775 0.007257571816444397\n",
      "129 4.933412082493305 0.0035494016855955124 0.00790858618915081\n",
      "130 4.933294326066971 0.003793014995753765 0.0073051967471838\n",
      "131 4.933166516944766 0.0037109876945614815 0.007509059309959412\n",
      "132 4.934746289625764 0.003519380420446396 0.007302959263324737\n",
      "133 4.935245538130403 0.0034296273663640023 0.007642343565821648\n",
      "134 4.934543505311012 0.003346486747264862 0.007326772958040237\n",
      "135 4.934049319475889 0.0033756458126008512 0.007593991309404373\n",
      "136 4.934341577813029 0.003950819790363312 0.0073531392961740496\n",
      "137 4.937263047322631 0.0037672704122960568 0.007816150933504105\n",
      "138 4.932955911383033 0.0035733099468052386 0.007368168458342552\n",
      "139 4.934274746105075 0.0032194058895111083 0.007365305423736573\n",
      "140 4.935591738671064 0.0032785749919712545 0.007479865849018097\n",
      "141 4.935284676030278 0.0036892164424061776 0.0072671004384756085\n",
      "142 4.935442583635449 0.0036421525180339812 0.007488014549016953\n",
      "143 4.933769250288606 0.0037268931046128273 0.0075520587712526325\n",
      "144 4.93386509642005 0.003596652094274759 0.0073745914548635485\n",
      "145 4.935151666402817 0.0033786168321967127 0.007365623936057091\n",
      "146 4.934526897966862 0.0034830474816262723 0.007238958775997162\n",
      "147 4.935962123796344 0.0035639220401644705 0.007362121641635895\n",
      "148 4.931694254279137 0.0033293633796274663 0.007382280379533768\n",
      "149 4.932118944823742 0.0031982941217720507 0.007294920682907105\n",
      "150 4.933897158131003 0.003187249016016722 0.007232295647263527\n",
      "151 4.93195016682148 0.003338817685842514 0.007414720058441162\n",
      "152 4.9377774223685265 0.0033202630467712877 0.007255907654762268\n",
      "153 4.937401296570897 0.0032873531095683577 0.007283849269151687\n",
      "154 4.935357505455613 0.0031575769074261187 0.007386676073074341\n",
      "155 4.935424100607634 0.003288146946579218 0.007502040266990662\n",
      "156 4.935529384762049 0.0033658343702554702 0.007436010390520096\n",
      "157 4.935555772855878 0.0035337799862027166 0.007614354938268662\n",
      "158 4.934945791959763 0.003572817612439394 0.007430892288684845\n",
      "159 4.941722920164466 0.0034694959819316865 0.007489023581147194\n",
      "160 4.939226843416691 0.003502581063657999 0.007353433147072792\n",
      "161 4.935157870873809 0.003314915496855974 0.007336448431015014\n",
      "162 4.934755032882094 0.0032802579440176487 0.007253986746072769\n",
      "163 4.935374869033694 0.0031318523064255716 0.007248646765947342\n",
      "164 4.93656312674284 0.003263395708054304 0.007336959838867187\n",
      "165 4.941286796703935 0.0032988992296159266 0.0074200727790594105\n",
      "166 4.935477992519736 0.003312529120594263 0.007372872456908226\n",
      "167 4.934225495904684 0.0032558959499001503 0.007409428060054779\n",
      "168 4.937334693968296 0.003496708743274212 0.007456762790679932\n",
      "169 4.935196857899427 0.003845474548637867 0.007559395134449005\n",
      "170 4.9328373447060585 0.0035743471160531045 0.007404811233282089\n",
      "171 4.932528216391802 0.0033609755150973797 0.00758400522172451\n",
      "172 4.934517065063119 0.003594311200082302 0.007290944308042526\n",
      "173 4.934189651161432 0.003398230202496052 0.007900353595614433\n",
      "174 4.93538904748857 0.00350173020362854 0.007493624240159988\n",
      "175 4.932813471183181 0.003345984622836113 0.007216409370303154\n",
      "176 4.934627693146467 0.003546036522835493 0.007334989830851555\n",
      "177 4.93574714101851 0.003473666399717331 0.007607072815299034\n",
      "178 4.933940514922142 0.003840541981160641 0.007770590186119079\n",
      "179 4.937164017930627 0.003903673656284809 0.007301489561796188\n",
      "180 4.934517351910472 0.004080012664198876 0.008465906381607055\n",
      "181 4.9356606509536505 0.003587551571428776 0.007361607849597931\n",
      "182 4.937485169619322 0.003260216698050499 0.007532231211662292\n",
      "183 4.935218276455998 0.003386859193444252 0.0072150838375091555\n",
      "184 4.936747685074806 0.003214269235730171 0.007432744652032852\n",
      "185 4.932474693283439 0.003474358532577753 0.0073624013364315035\n",
      "186 4.934996832162142 0.003394808176904917 0.007607022747397423\n",
      "187 4.937458120286465 0.003681382305920124 0.007382131293416023\n",
      "188 4.93753482028842 0.0034679627604782583 0.0074532231688499454\n",
      "189 4.939527390524745 0.003502025190740824 0.007194214463233948\n",
      "190 4.935142470523715 0.0034060272946953773 0.0074176152795553205\n",
      "191 4.9383200984448195 0.0032902703322470186 0.007560253590345383\n",
      "192 4.937515901401639 0.003216645922511816 0.0071847854554653165\n",
      "193 4.936206880956888 0.003095866899937391 0.007325239926576615\n",
      "194 4.936471577733755 0.0030826511196792128 0.007254289090633392\n",
      "195 4.946242421865463 0.003418848104774952 0.007630158364772797\n",
      "196 4.924752023071051 0.0033697368539869787 0.007473504841327667\n",
      "197 4.929161656647921 0.0034844954088330267 0.0076185187697410586\n",
      "198 4.938086554408073 0.003337601415812969 0.007350063547492027\n",
      "199 4.937254270538688 0.0031478570848703386 0.007194732129573822\n",
      "200 4.933584010228515 0.00279839688912034 0.007101485505700111\n",
      "201 4.936350800096989 0.002642270252108574 0.007112899050116539\n",
      "202 4.937322068959475 0.0025930326282978056 0.00710674598813057\n",
      "203 4.934316020458937 0.0025832537300884725 0.007093518972396851\n",
      "204 4.94717014580965 0.0025869666785001753 0.007075183987617493\n",
      "205 4.93290407396853 0.002559948604553938 0.0070634853839874265\n",
      "206 4.934192208573222 0.002561359144747257 0.007132146432995796\n",
      "207 4.9354698192328215 0.0025693207457661627 0.0070605514943599705\n",
      "208 4.932493828237057 0.0025439660809934137 0.007106888145208359\n",
      "209 4.931810384616256 0.002563692819327116 0.007084201276302338\n",
      "210 4.9395064283162355 0.0026041184812784193 0.007092641592025757\n",
      "211 4.932467933744192 0.002621923767030239 0.007129282355308532\n",
      "212 4.935646019876003 0.0026288613863289356 0.007066536396741867\n",
      "213 4.933828137814999 0.0026083703078329562 0.00712615042924881\n",
      "214 4.9362402986735106 0.00258864126726985 0.0070657461881637575\n",
      "215 4.937067627906799 0.0025992577262222765 0.007117727398872375\n",
      "216 4.937021249905229 0.002559965968132019 0.007151780799031258\n",
      "217 4.937797887250781 0.0025958155579864977 0.00710265502333641\n",
      "218 4.945484256371856 0.0025974944680929183 0.007134959250688553\n",
      "219 4.9399976991117 0.0026737029887735843 0.007203576639294624\n",
      "220 4.940261613577604 0.002976380843669176 0.00731062650680542\n",
      "221 4.936370585113764 0.002699956014752388 0.007158534973859787\n",
      "222 4.936458909884095 0.0026088661141693593 0.00728855699300766\n",
      "223 4.937760308384895 0.0026516219563782217 0.0072153258323669435\n",
      "224 4.939151860773563 0.002696061238646507 0.007192524075508118\n",
      "225 4.938614448532462 0.0026502797231078147 0.0070426765084266665\n",
      "226 4.940342942252755 0.0026146378368139267 0.007062329798936844\n",
      "227 4.943065686151385 0.002653038002550602 0.0070709212124347685\n",
      "228 4.95099239423871 0.0027354706525802614 0.007057263851165772\n",
      "229 4.936690473929048 0.002655010133981705 0.007142167687416077\n",
      "230 4.946818482130766 0.00264537850394845 0.007082294449210167\n",
      "231 4.950764054432511 0.002602023731917143 0.007106268629431724\n",
      "232 4.937641257420182 0.002561227437108755 0.0070504752546548845\n",
      "233 4.936371142044663 0.002564279079437256 0.007080010920763016\n",
      "234 4.9414690136909485 0.00258798373863101 0.007145996838808059\n",
      "235 4.939035769551992 0.002646779078990221 0.007166099548339844\n",
      "236 4.9374791122972965 0.002627609945833683 0.007255349308252335\n",
      "237 4.936208561062813 0.0028351956978440286 0.007171280682086945\n",
      "238 4.933827541768551 0.0027250334322452543 0.007141453474760056\n",
      "239 4.94299004226923 0.002725060112774372 0.007103363126516342\n",
      "240 4.93672232888639 0.002780044265091419 0.007325019240379333\n",
      "241 4.9349018186330795 0.0028239845745265486 0.007258926555514336\n",
      "242 4.943131880834699 0.0027971477657556536 0.007136590108275414\n",
      "243 4.933159660547972 0.0026701314374804495 0.007179632261395455\n",
      "244 4.947676943615079 0.0026337471567094324 0.007299585789442062\n",
      "245 4.930133303627372 0.002719211410731077 0.007117572799324989\n",
      "246 4.934193275868893 0.0026205994486808775 0.007160715982317925\n",
      "247 4.938176253810525 0.002608702406287193 0.007170252203941345\n",
      "248 4.943343313410878 0.0025548214763402937 0.007172956690192222\n",
      "249 4.937662875279784 0.0025801786296069623 0.0071129737049341205\n",
      "250 4.934048868715763 0.0026037707962095736 0.0070468300580978395\n",
      "251 4.935041639953852 0.0026494816541671753 0.0071534614264965055\n",
      "252 4.933008974418044 0.002636514324694872 0.007142671346664429\n",
      "253 4.935722008347511 0.0025696771480143072 0.007154985070228576\n",
      "254 4.937514701858163 0.0026291393153369427 0.007110122442245483\n",
      "255 4.948386415839195 0.002571425314992666 0.007101147398352623\n",
      "256 4.936640039086342 0.0025556708201766015 0.007098610997200012\n",
      "257 4.937890533357859 0.0026176536418497562 0.007137904837727547\n",
      "258 4.947785330936313 0.0026485155522823333 0.007113557606935501\n",
      "259 4.940418366342783 0.0025162046663463116 0.007087984383106232\n",
      "260 4.936200050637126 0.002575490467250347 0.007105327695608139\n",
      "261 4.938225192949176 0.0028130807131528856 0.007120475023984909\n",
      "262 4.943927159532905 0.0027820374257862567 0.007241339683532715\n",
      "263 4.939570242539048 0.0026348107010126113 0.007092728316783905\n",
      "264 4.949205545708537 0.0026382757052779197 0.0071942004561424255\n",
      "265 4.939776757732034 0.0026112956553697585 0.007110580280423165\n",
      "266 4.948608981445432 0.0026270694322884083 0.007191377431154251\n",
      "267 4.935517808422446 0.0026464999243617057 0.007198072075843811\n",
      "268 4.933477461338043 0.00257188343629241 0.007099462002515793\n",
      "269 4.943750666454434 0.00264535378664732 0.007079648822546005\n",
      "270 4.934817556291819 0.0026081583611667156 0.007052924260497093\n",
      "271 4.936036394909024 0.0026256492286920547 0.007119361236691475\n",
      "272 4.933999788016081 0.002665047023445368 0.0072237178683280945\n",
      "273 4.933097057044506 0.0025653167366981506 0.007117956280708313\n",
      "274 4.9327417984604836 0.002636007387191057 0.007268178910017013\n",
      "275 4.94618184491992 0.0026208417043089866 0.0071244466304779056\n",
      "276 4.941160276532173 0.0026156820841133593 0.007340527176856995\n",
      "277 4.935900777578354 0.0026150326319038866 0.00704462468624115\n",
      "278 4.933639418333769 0.0027039402686059475 0.0070715749263763426\n",
      "279 4.93870235979557 0.0027217321954667566 0.007247039899230003\n",
      "280 4.93616427667439 0.002767062045633793 0.0070694155991077425\n",
      "281 4.939031768590212 0.002896982941776514 0.00740029901266098\n",
      "282 4.937759306281805 0.002790541648864746 0.007189058959484101\n",
      "283 4.93724830634892 0.0026646910570561886 0.007183625549077988\n",
      "284 4.937441676855087 0.0025805096104741096 0.0070551222562789916\n",
      "285 4.9402581714093685 0.0026027510948479177 0.007291227132081985\n",
      "286 4.936903992667794 0.002542194411158562 0.007173130363225937\n",
      "287 4.936607521027327 0.0025675697922706605 0.007098792940378189\n",
      "288 4.932181069627404 0.002650555856525898 0.0072208065539598465\n",
      "289 4.943930571898818 0.002583943273872137 0.007120955288410186\n",
      "290 4.970029531046748 0.0025258741565048694 0.007120910435914994\n",
      "291 4.936107086017728 0.002840561043471098 0.007155848443508148\n",
      "292 4.932929437607527 0.0027071428038179875 0.007224889248609543\n",
      "293 4.933931848034263 0.002500324122607708 0.007037987634539604\n",
      "294 4.942675506696105 0.0024809277690947054 0.007151073515415191\n",
      "295 4.931300349533558 0.0025011946260929106 0.007203469350934029\n",
      "296 4.932234991341829 0.0025015706829726696 0.0071097914874553685\n",
      "297 4.94070309214294 0.002539678655564785 0.007134480476379395\n",
      "298 4.941028796136379 0.002658456105738878 0.007183428406715393\n",
      "299 4.931428387761116 0.002568727694451809 0.007255744785070419\n",
      "300 4.942081205546856 0.0023958443999290467 0.007110584527254105\n",
      "301 4.932934798300266 0.002312549252063036 0.007015536725521088\n",
      "302 4.934768846258521 0.002282818157225847 0.00704279862344265\n",
      "303 4.936866534873843 0.002267855752259493 0.007090501189231872\n",
      "304 4.937064968049526 0.002268095187842846 0.007043807208538056\n",
      "305 4.944014776498079 0.002263698499649763 0.00704704001545906\n",
      "306 4.931307476013899 0.002268707998096943 0.007041678726673126\n",
      "307 4.936236901208758 0.002255931969732046 0.007041120454668998\n",
      "308 4.934100110083818 0.0022441451363265515 0.007063582912087441\n",
      "309 4.9355963207781315 0.002243853196501732 0.00703116700053215\n",
      "310 4.937728062272072 0.0022434052154421806 0.007033331915736198\n",
      "311 4.940057206898928 0.0022426557987928392 0.007040616124868393\n",
      "312 4.933659791946411 0.0022533820532262324 0.007102062702178955\n",
      "313 4.933947321027517 0.0022554176449775696 0.007037594020366669\n",
      "314 4.949074305593967 0.002267655335366726 0.006997394263744354\n",
      "315 4.935649309307337 0.0022580745853483675 0.007051655426621437\n",
      "316 4.937214205041528 0.0022686963751912116 0.007065347880125045\n",
      "317 4.929883090779185 0.002252067405730486 0.0070914938300848\n",
      "318 4.929112216457725 0.0022595387548208236 0.007008155286312104\n",
      "319 4.947082394734025 0.0022768235355615615 0.00706739068031311\n",
      "320 4.932612724602222 0.0022723991610109806 0.007027735784649849\n",
      "321 4.928893784061074 0.002296168550848961 0.0070056658983230595\n",
      "322 4.930378969758749 0.0022793162390589714 0.0070903280377388\n",
      "323 4.928866297006607 0.002318306088447571 0.007058757767081261\n",
      "324 4.951043516397476 0.002332073174417019 0.007024647295475006\n",
      "325 4.929737029597163 0.0022590181566774846 0.00712054006755352\n",
      "326 4.928985388949513 0.0023209451772272585 0.007008801251649857\n",
      "327 4.94522738084197 0.0022879337519407274 0.007072275057435035\n",
      "328 4.928771495819092 0.00227596091106534 0.00709108218550682\n",
      "329 4.9313532914966345 0.0022756186053156852 0.007075812518596649\n",
      "330 4.932413479313254 0.002243830531835556 0.007057397961616516\n",
      "331 4.938440214842558 0.002270226314663887 0.007116702795028687\n",
      "332 4.934792557731271 0.002280732624232769 0.007040189206600189\n",
      "333 4.932064356282353 0.0022998774237930775 0.007088311910629272\n",
      "334 4.932969322428107 0.0022493258118629457 0.007033121287822724\n",
      "335 4.934187131002545 0.0022423344254493714 0.007111044749617577\n",
      "336 4.930757271125913 0.002257022023200989 0.0070695909857749935\n",
      "337 4.948414923623204 0.0022896934859454633 0.007061620950698852\n",
      "338 4.935404824092984 0.0022725638262927534 0.00705699197947979\n",
      "339 4.934946844354272 0.002264974910765886 0.0071117677539587025\n",
      "340 4.935219246894121 0.002306951344013214 0.007108442932367325\n",
      "341 4.9321492947638035 0.0023829473219811916 0.0070944296568632124\n",
      "342 4.92951525747776 0.0023121772184967995 0.007151482999324799\n",
      "343 4.93107445910573 0.0022952727265655995 0.007123276069760323\n",
      "344 4.934502307325602 0.0022603107169270513 0.007119027078151703\n",
      "345 4.933781925588846 0.002235988385975361 0.007044745832681656\n",
      "346 4.931122746318579 0.0022479435503482818 0.007070543169975281\n",
      "347 4.932491082698107 0.0022782440185546873 0.007058577165007591\n",
      "348 4.954141775146127 0.0022415113374590874 0.007083727568387985\n",
      "349 4.934266537427902 0.0022505005970597266 0.007084599360823631\n",
      "350 4.93037997558713 0.002236826132982969 0.007053093910217285\n",
      "351 4.928196733817458 0.0022105051800608636 0.007033749520778656\n",
      "352 4.948262132704258 0.0022184405848383903 0.007107531726360321\n",
      "353 4.928956331685185 0.00222776385769248 0.007120130211114883\n",
      "354 4.938300237059593 0.0022457309514284135 0.007045334205031395\n",
      "355 4.934024605900049 0.0022560150288045405 0.007088114470243454\n",
      "356 4.9375560358166695 0.002291547913104296 0.00707345299422741\n",
      "357 4.938655378296971 0.0022852230966091155 0.00709233932197094\n",
      "358 4.930099867284298 0.0022686189375817774 0.00707937940955162\n",
      "359 4.940045857802033 0.002267204541712999 0.007079056948423385\n",
      "360 4.932012936100364 0.00223699015378952 0.00706178605556488\n",
      "361 4.933856287971139 0.0022609574049711227 0.007111386954784393\n",
      "362 4.931395040825009 0.0022606584541499616 0.007081754803657532\n",
      "363 4.928586836904287 0.002217560451477766 0.007091636508703232\n",
      "364 4.938285985961556 0.002214471437036991 0.00708496019244194\n",
      "365 4.932307759299874 0.0022116844318807124 0.0070855096727609635\n",
      "366 4.93859439343214 0.0022212678939104082 0.007090794593095779\n",
      "367 4.9333079271018505 0.0022385417819023134 0.007150583118200302\n",
      "368 4.93272634409368 0.002227079030126333 0.007142909318208695\n",
      "369 4.93188226595521 0.0022505417801439763 0.007096226662397385\n",
      "370 4.941139290109277 0.0022892993465065954 0.0070855429023504255\n",
      "371 4.933471098542213 0.002249565403908491 0.007077188342809677\n",
      "372 4.929604956880212 0.0022269468791782858 0.007046683356165886\n",
      "373 4.929102601483464 0.002205094765871763 0.007100342363119125\n",
      "374 4.931014474481344 0.0022270029745996 0.007122173309326172\n",
      "375 4.929100228473544 0.002210650842636824 0.007083758786320686\n",
      "376 4.931780904531479 0.0022238739244639875 0.00712301604449749\n",
      "377 4.932202305644751 0.0022219030410051345 0.007084831967949868\n",
      "378 4.931746557354927 0.002241318237036467 0.0070913112163543705\n",
      "379 4.930726522579789 0.0022193948961794375 0.007129139378666878\n",
      "380 4.930641332641244 0.0022304704673588274 0.007193892076611519\n",
      "381 4.929785709828138 0.002275627177208662 0.0071102318167686465\n",
      "382 4.93104338273406 0.0022704214341938496 0.007115587592124939\n",
      "383 4.932742970064282 0.0023012965843081474 0.007138154953718185\n",
      "384 4.931669341400266 0.0022435479275882244 0.0070768104493618015\n",
      "385 4.9313127137720585 0.002295304801315069 0.00709055170416832\n",
      "386 4.931337164714932 0.002272599246352911 0.007125224769115448\n",
      "387 4.930656561627984 0.0022783129177987574 0.007127131223678589\n",
      "388 4.931801822036505 0.002260624300688505 0.00713953822851181\n",
      "389 4.933568581938744 0.002285207621753216 0.007035418525338173\n",
      "390 4.933643434196711 0.0022631406113505365 0.0071123743057250974\n",
      "391 4.933631107211113 0.0022182374708354475 0.0071067333221435545\n",
      "392 4.9308920949697495 0.002226030007004738 0.007076391130685806\n",
      "393 4.934544924646616 0.002297747325152159 0.007069613188505173\n",
      "394 4.935750987380743 0.002274828590452671 0.0070854096859693525\n",
      "395 4.931853340938687 0.002293107010424137 0.007069414854049683\n",
      "396 4.930589510127902 0.002257592175155878 0.007115042358636856\n",
      "397 4.931937957182527 0.002257290530949831 0.007180046141147613\n",
      "398 4.933508921414614 0.0022548775747418402 0.007109144330024719\n",
      "399 4.931336648762226 0.0021951274424791335 0.00706402063369751\n",
      "400 4.930019963532686 0.00211887563765049 0.0070626547932624816\n",
      "401 4.947139011695981 0.0020914842784404753 0.007084079533815384\n",
      "402 4.927197081968188 0.002090305857360363 0.007058725506067276\n",
      "403 4.954908777028322 0.0020871519669890403 0.0070831354707479475\n",
      "404 4.932062771171331 0.0020883321836590767 0.007064183577895165\n",
      "405 4.931298907846212 0.0020852237045764924 0.007073456197977066\n",
      "406 4.931422745808959 0.002084560927003622 0.007080266252160072\n",
      "407 4.933554658666253 0.0020852000750601293 0.007068657726049424\n",
      "408 4.930010633543134 0.0020840099155902863 0.0070770162343978885\n",
      "409 4.930685272440314 0.0020822584480047228 0.00705817237496376\n",
      "410 4.932444337755442 0.0020821475982666015 0.007076902016997337\n",
      "411 4.930995477363467 0.0020834864266216753 0.007062051147222519\n",
      "412 4.931835373863578 0.0020800080336630345 0.007085063382983208\n",
      "413 4.931440327316523 0.002079387355595827 0.007075016051530838\n",
      "414 4.932260641828179 0.0020814252980053427 0.00704938217997551\n",
      "415 4.931855298578739 0.0020900721363723276 0.00708024762570858\n",
      "416 4.932942757382989 0.0020965456366539 0.007059974074363709\n",
      "417 4.934067267924547 0.0020868883207440375 0.007084231823682785\n",
      "418 4.9424089547246695 0.002082474425435066 0.007068657726049424\n",
      "419 4.931804506108165 0.0020800899788737297 0.007082792446017266\n",
      "420 4.931955276057124 0.0020801806040108206 0.00706327997148037\n",
      "421 4.929936107248068 0.002077591832727194 0.007091959118843079\n",
      "422 4.932422045618296 0.002073839783668518 0.00708148330450058\n",
      "423 4.931757301092148 0.0020789820477366446 0.0070621310919523235\n",
      "424 4.9304972141981125 0.002081157173961401 0.007097176387906075\n",
      "425 4.93336845189333 0.002089638650417328 0.0071127286553382876\n",
      "426 4.936700163409114 0.00210349290445447 0.007086309939622879\n",
      "427 4.940313307568431 0.002102959182113409 0.007118635103106499\n",
      "428 4.932602459564805 0.002086513563990593 0.007094272822141648\n",
      "429 4.932132551446557 0.002098810739815235 0.007120023742318154\n",
      "430 4.933014275506139 0.002099827330559492 0.007077664360404015\n",
      "431 4.934652652591467 0.0020765062719583513 0.007067297399044037\n",
      "432 4.932200053706765 0.002077230915427208 0.0070958960056304934\n",
      "433 4.932765331119299 0.002073841854929924 0.007103076353669167\n",
      "434 4.932026909664273 0.002071805641055107 0.007123873829841614\n",
      "435 4.932298932224512 0.0020752259530127048 0.007071290165185928\n",
      "436 4.931834366172552 0.002077883768826723 0.007066451832652092\n",
      "437 4.934006959199905 0.0020796522311866284 0.0071123851835727695\n",
      "438 4.931959005072713 0.0020737998858094213 0.007072760164737702\n",
      "439 4.935241801664233 0.0020701432563364507 0.007089223265647888\n",
      "440 4.934892732650042 0.002082165189087391 0.007098281085491181\n",
      "441 4.931512048467994 0.002089161854237318 0.007092547938227654\n",
      "442 4.94789420068264 0.0020940175466239454 0.007086466997861862\n",
      "443 4.938705436885357 0.0020761785730719568 0.007109055295586586\n",
      "444 4.932124055922031 0.0020695084035396577 0.0070831498503685\n",
      "445 4.93311426602304 0.0020682276114821433 0.007073693871498108\n",
      "446 4.932098228484392 0.0020774311944842336 0.007100393697619438\n",
      "447 4.931185035035014 0.0020766657814383508 0.007097522988915444\n",
      "448 4.934819675981998 0.0020727312080562115 0.007062633335590362\n",
      "449 4.933214735239744 0.002074789274483919 0.007065232023596764\n",
      "450 4.932012232020497 0.0020842925384640695 0.007099970653653145\n",
      "451 4.930629421025515 0.002066596157848835 0.007072674483060837\n",
      "452 4.931224258616567 0.0020684255734086036 0.007127968594431877\n",
      "453 4.934461180120707 0.0020704279467463494 0.007115514725446701\n",
      "454 4.933259787037969 0.0020693408250808715 0.007110097706317902\n",
      "455 4.931036062538624 0.0020614492930471897 0.007113365978002548\n",
      "456 4.945737337693572 0.002076091602444649 0.007101676613092423\n",
      "457 4.931791927665472 0.0020793816857039927 0.00712051048874855\n",
      "458 4.9306515753269196 0.002081991668790579 0.007094822227954865\n",
      "459 4.932421604171395 0.0020673872865736484 0.007105077281594276\n",
      "460 4.934284733608365 0.002060709610581398 0.007106176614761353\n",
      "461 4.946872148662806 0.002059012643992901 0.007097172439098358\n",
      "462 4.930814832448959 0.002055616747587919 0.007086865901947022\n",
      "463 4.933157671242952 0.002053649887442589 0.007104552537202835\n",
      "464 4.933215955272317 0.002083948217332363 0.007116317451000213\n",
      "465 4.934341233223677 0.0020766296535730364 0.007089011371135712\n",
      "466 4.93394155241549 0.002073636896908283 0.0070716005563735964\n",
      "467 4.948530711233616 0.0020744279362261294 0.007108196169137954\n",
      "468 4.935366680845618 0.0020616707280278206 0.007138345167040825\n",
      "469 4.932589756324887 0.002057336065918207 0.007101946026086808\n",
      "470 4.9426329433918 0.0020534130148589613 0.0070972716063261035\n",
      "471 4.9342107735574245 0.0020532753355801104 0.007073991745710373\n",
      "472 4.94934500567615 0.0020832365974783897 0.0071494878828525545\n",
      "473 4.935372080653906 0.00207852903008461 0.007107276767492295\n",
      "474 4.937248254194856 0.0020521055571734903 0.0071199312806129456\n",
      "475 4.933137757703662 0.0020628244318068025 0.007125220224261284\n",
      "476 4.9505627900362015 0.0020472772903740405 0.0071171408146619795\n",
      "477 4.934718741104007 0.002039998732507229 0.007115992084145546\n",
      "478 4.931211587041616 0.002043769132345915 0.007114183157682419\n",
      "479 4.933792060241103 0.002051080409437418 0.007092508673667908\n",
      "480 4.930506497621536 0.0020630744546651842 0.007108224779367447\n",
      "481 4.931451268494129 0.002047907669097185 0.007109128385782242\n",
      "482 4.928041467443109 0.002081386551260948 0.007120821997523308\n",
      "483 4.928620524704456 0.0020638493299484253 0.0071056121587753296\n",
      "484 4.93168487213552 0.0020518830008804797 0.007086510956287384\n",
      "485 4.933846067637205 0.002037892773747444 0.0070984457433223725\n",
      "486 4.931130759418011 0.002034477062523365 0.0070815164595842365\n",
      "487 4.92903845384717 0.0020415219962596895 0.0071192910522222515\n",
      "488 4.949582286179066 0.0020441706590354444 0.007099910080432892\n",
      "489 4.930882193148136 0.0020521818101406097 0.007082956433296203\n",
      "490 4.940349912270904 0.002050101067870855 0.007117024809122086\n",
      "491 4.948684763163328 0.002037956368178129 0.007129248008131981\n",
      "492 4.92791305296123 0.0020382168628275393 0.007133253514766693\n",
      "493 4.926904421299696 0.002035720035433769 0.007121581062674523\n",
      "494 4.92630766518414 0.0020398542881011963 0.007108437716960907\n",
      "495 4.932632053270936 0.002032087963074446 0.007107222154736519\n",
      "496 4.9304463788867 0.0020243133157491684 0.007114067375659942\n",
      "497 4.9293383825570345 0.002032536257058382 0.007109215185046196\n",
      "498 4.928801190108061 0.002034327544271946 0.007089265659451485\n",
      "499 4.930113168433309 0.0020354353450238705 0.007179632931947708\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "optimizer = Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "#                        div_factor=1e4, \n",
    "#                        final_div_factor=1e4,\n",
    "#                        pct_start=0.3,\n",
    "#                        steps_per_epoch=len(train_loader)*5, \n",
    "#                        epochs=100)\n",
    "\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_normalizer.cuda(device)\n",
    "\n",
    "with tqdm(total=epochs) as pbar_ep:\n",
    "                            \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_l2 = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "    #         y = y_normalizer.decode(y)\n",
    "\n",
    "            loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_l2 += loss.item()\n",
    "            \n",
    "        ############################\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        desc = f\"epoch: [{epoch+1}/{epochs}]\"\n",
    "        desc += f\" | current lr: {lr:.3e}\"\n",
    "        pbar_ep.set_description(desc)\n",
    "        pbar_ep.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                out = model(x).reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "\n",
    "                test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "        train_l2/= ntrain\n",
    "        test_l2 /= ntest\n",
    "\n",
    "        t2 = default_timer()\n",
    "        print(epoch, t2-t1, train_l2, test_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d489829-35f3-45e8-bed7-a61ce3b7a513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3698497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1faffc37a44e78ac5660d3cdacfd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.3923319783061743 0.16044498777389526 0.11486047267913818\n",
      "1 2.384324660524726 0.10454395282268523 0.09674615979194641\n",
      "2 2.382553832605481 0.0781529929637909 0.06425435423851013\n",
      "3 2.383607428520918 0.05705444824695587 0.052716965675354006\n",
      "4 2.3843337576836348 0.0441395748257637 0.04193912029266358\n",
      "5 2.384529035538435 0.03443048536777496 0.03420851469039917\n",
      "6 2.3851748313754797 0.030798122465610506 0.03239836692810059\n",
      "7 2.385246280580759 0.026988554507493972 0.027737122774124146\n",
      "8 2.3854966927319765 0.024463143557310104 0.02572058379650116\n",
      "9 2.385958466678858 0.021732020765542984 0.026579384803771974\n",
      "10 2.3858217149972916 0.02040632277727127 0.02472914069890976\n",
      "11 2.386238956823945 0.020067536652088165 0.022696007192134857\n",
      "12 2.3866615314036608 0.018655417203903197 0.022115610241889953\n",
      "13 2.386169945821166 0.018462661921977996 0.021509369015693666\n",
      "14 2.3866711165755987 0.018537365436553956 0.02128229081630707\n",
      "15 2.3873448316007853 0.016727054864168166 0.021875238120555876\n",
      "16 2.3900334257632494 0.016730079889297485 0.0200611612200737\n",
      "17 2.386820552870631 0.01597425001859665 0.018964518904685975\n",
      "18 2.3880337327718735 0.015741914689540865 0.0196816486120224\n",
      "19 2.387970771640539 0.01547142681479454 0.01945393055677414\n",
      "20 2.389088958501816 0.015495993971824646 0.018511529564857482\n",
      "21 2.3929865416139364 0.01466221985220909 0.01852798044681549\n",
      "22 2.3931658267974854 0.014386617094278336 0.019519395530223846\n",
      "23 2.389063261449337 0.014547412067651748 0.01728318452835083\n",
      "24 2.388981072232127 0.013836973011493683 0.016113426685333252\n",
      "25 2.3889083601534367 0.013502676144242286 0.017250418663024902\n",
      "26 2.3892899192869663 0.012733010038733483 0.016776044070720673\n",
      "27 2.3900619577616453 0.012980062931776047 0.015333778262138366\n",
      "28 2.388777745887637 0.012459897503256798 0.016121655702590942\n",
      "29 2.38794899918139 0.012428089767694474 0.016486437618732454\n",
      "30 2.388594936579466 0.012561808109283448 0.015270268321037292\n",
      "31 2.388289403170347 0.011895144045352937 0.01444325715303421\n",
      "32 2.391429975628853 0.011676516547799111 0.014996234774589539\n",
      "33 2.392131896689534 0.014776518285274506 0.0152349454164505\n",
      "34 2.3898045755922794 0.0119711924046278 0.01602373003959656\n",
      "35 2.3942877054214478 0.011619710773229599 0.014843409359455108\n",
      "36 2.396035760641098 0.011326427951455116 0.01440383017063141\n",
      "37 2.396409349516034 0.011414456263184547 0.014896699488162994\n",
      "38 2.396375350654125 0.011157701730728149 0.01429654836654663\n",
      "39 2.3955896832048893 0.012029919713735581 0.014683445990085602\n",
      "40 2.3950825668871403 0.01247947682440281 0.014812364280223846\n",
      "41 2.3958269767463207 0.012322133764624595 0.015372110903263092\n",
      "42 2.3962891567498446 0.011555867224931717 0.014639998376369477\n",
      "43 2.3956337440758944 0.01163427411019802 0.014628587067127228\n",
      "44 2.395506100729108 0.010899934470653534 0.01527503103017807\n",
      "45 2.3957126792520285 0.010748315319418908 0.014638413190841675\n",
      "46 2.3957306295633316 0.010792999058961868 0.013847151100635528\n",
      "47 2.395956639200449 0.011156304329633713 0.013682413399219513\n",
      "48 2.3959649838507175 0.011252136558294296 0.013372171372175216\n",
      "49 2.396262638270855 0.01032180392742157 0.013866842687129975\n",
      "50 2.394925506785512 0.010542754039168357 0.013715416491031647\n",
      "51 2.3952677939087152 0.011028717532753944 0.014692560881376266\n",
      "52 2.395639644935727 0.01106048859655857 0.014377503097057343\n",
      "53 2.3952928204089403 0.012042699709534645 0.013808236420154572\n",
      "54 2.39518135599792 0.011476864382624626 0.013361435681581497\n",
      "55 2.395292643457651 0.010124283462762833 0.013628272712230683\n",
      "56 2.3956344965845346 0.010060355737805367 0.014082524925470352\n",
      "57 2.395205883309245 0.010667250007390976 0.012895323783159256\n",
      "58 2.395301815122366 0.010036998689174651 0.012482178509235381\n",
      "59 2.3952072728425264 0.00975263211131096 0.013918516039848328\n",
      "60 2.3949177395552397 0.010119679570198058 0.012695268094539643\n",
      "61 2.3956654611974955 0.010155690312385559 0.013345632702112198\n",
      "62 2.3929364290088415 0.009902192413806915 0.012767678946256638\n",
      "63 2.392490418627858 0.0097557642608881 0.012974266558885575\n",
      "64 2.3928085826337337 0.010513525068759919 0.014431219100952148\n",
      "65 2.393033480271697 0.010113701313734055 0.012632119357585908\n",
      "66 2.392472768202424 0.009768539950251579 0.01246110662817955\n",
      "67 2.4086159840226173 0.00979388989508152 0.013271113336086273\n",
      "68 2.391480015590787 0.010456096485257148 0.012496522963047027\n",
      "69 2.3911095913499594 0.009698324963450432 0.014491360783576965\n",
      "70 2.3913201987743378 0.010485369831323624 0.014078721106052398\n",
      "71 2.3921415712684393 0.010432126745581628 0.012593031525611878\n",
      "72 2.3920722752809525 0.009995873406529427 0.01314338594675064\n",
      "73 2.3922207728028297 0.010313607722520829 0.012644828408956527\n",
      "74 2.391738433390856 0.009651682987809182 0.014933240562677384\n",
      "75 2.3921178728342056 0.010438027173280715 0.013149683624505996\n",
      "76 2.3924658335745335 0.010179744318127633 0.01307279646396637\n",
      "77 2.392261166125536 0.009545032680034638 0.012707050442695617\n",
      "78 2.39248825982213 0.009590185716748238 0.013012710511684417\n",
      "79 2.3922058045864105 0.009865973785519599 0.012654888331890107\n",
      "80 2.3918809425085783 0.009597654029726982 0.013470990061759948\n",
      "81 2.3927967827767134 0.00949314258992672 0.012540315836668014\n",
      "82 2.3930669855326414 0.00963645076751709 0.012574736773967744\n",
      "83 2.392040465027094 0.009672463044524193 0.014385448694229126\n",
      "84 2.3914681393653154 0.0095261792242527 0.012545596957206726\n",
      "85 2.385941121727228 0.00948487712442875 0.012468850314617157\n",
      "86 2.3861403353512287 0.009488917604088784 0.013114135563373566\n",
      "87 2.386788113042712 0.010043679654598237 0.012453594505786895\n",
      "88 2.3863254245370626 0.009720434576272965 0.011974646002054215\n",
      "89 2.386663554236293 0.009705096393823623 0.01305839627981186\n",
      "90 2.3871019892394543 0.00968111751973629 0.012873328328132629\n",
      "91 2.3865379691123962 0.009243140742182732 0.012756633460521699\n",
      "92 2.389031769707799 0.009420183092355728 0.012635159939527511\n",
      "93 2.3926906138658524 0.009604868292808533 0.012156730741262436\n",
      "94 2.3927397560328245 0.009913510113954544 0.012435353398323058\n",
      "95 2.3922786340117455 0.009753730088472367 0.01239376425743103\n",
      "96 2.3922228198498487 0.009892460390925407 0.013071692585945129\n",
      "97 2.392139093950391 0.009684450432658195 0.012007620930671693\n",
      "98 2.392146609723568 0.009028060272336005 0.012196595370769501\n",
      "99 2.392228165641427 0.00885767638683319 0.012219390720129014\n",
      "100 2.3928529880940914 0.007907635062932968 0.011225301176309585\n",
      "101 2.392445145174861 0.007522901684045792 0.011068919152021408\n",
      "102 2.3928098753094673 0.007450440138578415 0.011118618845939636\n",
      "103 2.392304005101323 0.00719577905535698 0.010887233316898346\n",
      "104 2.3924423810094595 0.007301453933119774 0.010952899158000946\n",
      "105 2.391950039193034 0.007389945894479752 0.010967231839895248\n",
      "106 2.3925810530781746 0.007285475134849549 0.010879875421524047\n",
      "107 2.3918698113411665 0.007474125444889069 0.011200896054506302\n",
      "108 2.3919258527457714 0.0071871021836996075 0.010632268637418747\n",
      "109 2.3923052921891212 0.00720312488079071 0.01093095436692238\n",
      "110 2.392275143414736 0.007147383779287338 0.010934492349624634\n",
      "111 2.3917872700840235 0.0073210433721542355 0.010771473348140716\n",
      "112 2.392397578805685 0.0071436486095190046 0.011169610470533371\n",
      "113 2.3928056862205267 0.007549413084983826 0.010798922628164292\n",
      "114 2.3918165806680918 0.007549992218613625 0.011389816403388977\n",
      "115 2.390628518536687 0.007509163722395897 0.010710075795650482\n",
      "116 2.389053849503398 0.007078590147197247 0.01073352426290512\n",
      "117 2.391906499862671 0.0070543276146054264 0.010697146207094192\n",
      "118 2.3917373549193144 0.007432356461882591 0.010567488819360734\n",
      "119 2.3916132487356663 0.007403115600347519 0.01077336773276329\n",
      "120 2.3913297094404697 0.007154462322592735 0.010637536197900772\n",
      "121 2.391458321362734 0.007273596912622452 0.01068465992808342\n",
      "122 2.392087746411562 0.007152632832527161 0.010767518281936646\n",
      "123 2.391787514090538 0.007021551981568336 0.010516397505998611\n",
      "124 2.3916768468916416 0.007204098954796791 0.01097863957285881\n",
      "125 2.391216119751334 0.007282272785902023 0.010858837217092514\n",
      "126 2.391221258789301 0.007487585172057152 0.011003725677728653\n",
      "127 2.3917259220033884 0.0072253717631101605 0.01058703064918518\n",
      "128 2.391283582895994 0.007066754013299942 0.010678340643644334\n",
      "129 2.390979401767254 0.007080491259694099 0.010525466501712799\n",
      "130 2.3908380270004272 0.007088620871305466 0.010556374192237854\n",
      "131 2.3906432054936886 0.0073138038367033 0.010635048300027847\n",
      "132 2.391360158100724 0.007969526380300521 0.01145561397075653\n",
      "133 2.390631791204214 0.008185576841235161 0.011640999019145965\n",
      "134 2.3901950996369123 0.007896356254816056 0.011213625371456147\n",
      "135 2.3943997845053673 0.00856330305337906 0.011073274463415146\n",
      "136 2.390968080610037 0.007797092780470848 0.010906643569469451\n",
      "137 2.392654400318861 0.0073753086626529695 0.010475375652313233\n",
      "138 2.393944438546896 0.007115366235375405 0.010506968051195144\n",
      "139 2.388665307313204 0.007328480377793312 0.011071246564388276\n",
      "140 2.389569664373994 0.00737109412252903 0.010480162054300309\n",
      "141 2.393383476883173 0.007028746418654919 0.010746373385190964\n",
      "142 2.3901659101247787 0.007679451376199722 0.010801899582147598\n",
      "143 2.38615676574409 0.007741487607359886 0.010733716040849686\n",
      "144 2.3866290114820004 0.007806794285774231 0.01077216625213623\n",
      "145 2.3861915543675423 0.007308100193738938 0.010638402700424194\n",
      "146 2.3860571291297674 0.007203315034508705 0.010906676948070525\n",
      "147 2.386539911851287 0.0075816365927457805 0.010919950902462006\n",
      "148 2.386658212170005 0.007931338161230088 0.010998804718255997\n",
      "149 2.3865075316280127 0.007317162558436394 0.0105723837018013\n",
      "150 2.3866240736097097 0.007382454946637153 0.01079800322651863\n",
      "151 2.3870732560753822 0.007170563593506813 0.010407503098249435\n",
      "152 2.387128785252571 0.007148086428642273 0.010855581015348434\n",
      "153 2.388097519055009 0.007525532752275467 0.01059880167245865\n",
      "154 2.38788160122931 0.007272351309657097 0.010180904120206833\n",
      "155 2.3884972482919693 0.007187620423734188 0.011033653765916824\n",
      "156 2.38808018155396 0.007809889346361161 0.01072038322687149\n",
      "157 2.3939387388527393 0.007106359958648682 0.010563574880361557\n",
      "158 2.393857942894101 0.0070355085730552675 0.010430693328380584\n",
      "159 2.393870923668146 0.006883066281676293 0.01062294140458107\n",
      "160 2.3941211495548487 0.0071169108301401135 0.010402880609035492\n",
      "161 2.3922682404518127 0.006956537142395973 0.010306496471166611\n",
      "162 2.387985644862056 0.007741032555699349 0.010678011626005172\n",
      "163 2.3876305241137743 0.007125004380941391 0.010235244184732437\n",
      "164 2.3873964808881283 0.007125040099024773 0.01053273856639862\n",
      "165 2.3878353722393513 0.007231870263814926 0.010977871268987655\n",
      "166 2.387567028403282 0.007195882514119148 0.010388745367527008\n",
      "167 2.3873679619282484 0.007220635265111923 0.01052136778831482\n",
      "168 2.3872183077037334 0.007432340681552887 0.010446460843086243\n",
      "169 2.3878846522420645 0.007501972213387489 0.01076622486114502\n",
      "170 2.3875151220709085 0.007485603004693985 0.010977066904306412\n",
      "171 2.387714918702841 0.00725407138466835 0.01031716987490654\n",
      "172 2.3880486991256475 0.007057232066988945 0.010364042520523071\n",
      "173 2.3879060056060553 0.007162297442555428 0.01041755199432373\n",
      "174 2.3878137320280075 0.007588753953576088 0.010774329602718354\n",
      "175 2.3877608813345432 0.00773119880259037 0.010466410964727401\n",
      "176 2.3878135662525892 0.007355882346630096 0.010300106853246688\n",
      "177 2.38787142559886 0.007536940976977349 0.010990647822618485\n",
      "178 2.387424185872078 0.007569709703326225 0.011258100271224975\n",
      "179 2.3869424909353256 0.007263805642724037 0.010632761269807816\n",
      "180 2.386942619457841 0.007021299466490745 0.010400045812129975\n",
      "181 2.3872559797018766 0.006856446057558059 0.010219543278217315\n",
      "182 2.3873633109033108 0.006768019989132881 0.010232554823160172\n",
      "183 2.3868938349187374 0.006889657944440842 0.010997899025678635\n",
      "184 2.38683707639575 0.007298075467348099 0.010187056362628937\n",
      "185 2.3871705792844296 0.00701411460340023 0.010444411635398864\n",
      "186 2.387189643457532 0.007184079930186272 0.010406147837638855\n",
      "187 2.3874177504330873 0.006884146027266979 0.01030537098646164\n",
      "188 2.3866359647363424 0.007044594749808311 0.01030197411775589\n",
      "189 2.386430138722062 0.0069637615382671355 0.011132091730833054\n",
      "190 2.387149404734373 0.0071318957805633545 0.010283658504486084\n",
      "191 2.3866522926837206 0.006894446849822998 0.010339518934488296\n",
      "192 2.386994732543826 0.0070798321068286894 0.010968787372112274\n",
      "193 2.3872574362903833 0.006840045966207981 0.010212013274431228\n",
      "194 2.3864314518868923 0.0070559483841061595 0.010441398918628693\n",
      "195 2.386498909443617 0.007096017874777317 0.010128431916236878\n",
      "196 2.3868213519454002 0.007054644331336022 0.01024609625339508\n",
      "197 2.386934844776988 0.007126354828476906 0.010189428776502609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-8d49e43ece4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtrain_l2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model = FNO2d(modes, modes, 40).to(device)\n",
    "print(count_params(model))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "#                        div_factor=1e4, \n",
    "#                        final_div_factor=1e4,\n",
    "#                        pct_start=0.3,\n",
    "#                        steps_per_epoch=len(train_loader)*5, \n",
    "#                        epochs=100)\n",
    "\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "\n",
    "y_normalizer = UnitGaussianNormalizer(y_train)\n",
    "y_normalizer.cuda(device)\n",
    "\n",
    "with tqdm(total=epochs) as pbar_ep:\n",
    "                            \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t1 = default_timer()\n",
    "        train_l2 = 0\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x).reshape(batch_size, s, s)\n",
    "            out = y_normalizer.decode(out)\n",
    "    #         y = y_normalizer.decode(y)\n",
    "\n",
    "            loss = myloss(out.view(batch_size,-1), y.view(batch_size,-1))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            train_l2 += loss.item()\n",
    "            \n",
    "        ############################\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        desc = f\"epoch: [{epoch+1}/{epochs}]\"\n",
    "        desc += f\" | current lr: {lr:.3e}\"\n",
    "        pbar_ep.set_description(desc)\n",
    "        pbar_ep.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        test_l2 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                out = model(x).reshape(batch_size, s, s)\n",
    "                out = y_normalizer.decode(out)\n",
    "\n",
    "                test_l2 += myloss(out.view(batch_size,-1), y.view(batch_size,-1)).item()\n",
    "\n",
    "        train_l2/= ntrain\n",
    "        test_l2 /= ntest\n",
    "\n",
    "        t2 = default_timer()\n",
    "        print(epoch, t2-t1, train_l2, test_l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
